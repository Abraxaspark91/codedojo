[
  {
    "pid": "python_basic_string_quotes",
    "title": "Python 문자열 표기법",
    "body": "users 데이터프레임에서 이름(name)이 'Alice'인 사람을 찾고 싶습니다. Python 코드 내에서 문자열 값을 표현할 때 사용할 수 있는 따옴표의 종류를 생각하며 filter 함수를 완성해보세요.",
    "schema": "users(name STRING, age INT)",
    "sample_rows": [
      "Alice | 25",
      "Bob | 30"
    ],
    "difficulty": "Lv0 기초",
    "kind": "python",
    "expected": [
      "filter",
      "col"
    ],
    "hint": "Python에서는 작은따옴표(')와 큰따옴표(\") 중 무엇을 써도 상관없습니다. 시작과 끝만 일치시켜주세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_basic_string_literal",
    "title": "SQL 문자열 값 표기 규칙",
    "body": "SQL을 사용하여 name이 'Alice'인 유저를 조회하세요. SQL 문법에서 데이터 값(리터럴)을 감쌀 때 반드시 사용해야 하는 따옴표가 무엇인지 생각해보세요.",
    "schema": "users(name TEXT, age INT)",
    "sample_rows": [
      "Alice | 25",
      "Bob | 30"
    ],
    "difficulty": "Lv0 기초",
    "kind": "sql",
    "expected": [
      "WHERE",
      "name =",
      "'Alice'"
    ],
    "hint": "SQL에서 문자열 값은 반드시 **작은따옴표(')**를 사용해야 합니다. 큰따옴표(\")는 컬럼명 등을 감쌀 때 쓰입니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_import_functions",
    "title": "PySpark 함수 모듈 불러오기",
    "body": "PySpark의 다양한 함수(col, lit, sum 등)를 사용하기 위해 `pyspark.sql.functions` 모듈을 import 하려고 합니다. 코드를 간결하게 쓰기 위해 통상적으로 사용하는 별칭(Alias)을 사용하여 import 문을 작성하세요.",
    "schema": "N/A (코드 설정 문제)",
    "sample_rows": [],
    "difficulty": "Lv0 기초",
    "kind": "python",
    "expected": [
      "import",
      "as F"
    ],
    "hint": "`import ... as ...` 구문을 사용합니다. 보통 `F`라는 약어를 많이 사용합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_filter_null",
    "title": "PySpark 결측치(NULL) 필터링",
    "body": "users 데이터프레임에서 `email` 컬럼이 비어있는(NULL) 사용자만 조회하려고 합니다. Python의 `None`과 비교 연산자(`==`)를 사용하는 것은 올바르지 않습니다. PySpark 전용 메서드를 사용하세요.",
    "schema": "users(name STRING, email STRING)",
    "sample_rows": [
      "Alice | alice@test.com",
      "Bob | NULL",
      "Charlie | charlie@test.com"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "filter",
      "isNull()"
    ],
    "hint": "컬럼 객체 뒤에 `.isNull()` 메서드를 붙여서 확인해야 합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_filter_is_null",
    "title": "SQL 결측치(NULL) 조회 조건",
    "body": "SQL에서 email 정보가 없는(NULL) 사용자를 조회하세요. 일반적인 등호(`=`) 연산자로는 NULL 값을 찾아낼 수 없습니다. 올바른 SQL 조건식을 사용하세요.",
    "schema": "users(name TEXT, email TEXT)",
    "sample_rows": [
      "Alice | alice@test.com",
      "Bob | NULL"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "WHERE",
      "IS NULL"
    ],
    "hint": "`컬럼명 IS NULL` 구문을 사용해야 합니다. (`= NULL`은 동작하지 않습니다.)",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_f_string_basic",
    "title": "변수를 활용한 문자열 조건 생성",
    "body": "외부 변수 `target_age = 30`이 주어졌을 때, 이를 활용하여 \"age >= 30\" 형태의 필터 조건 문자열을 만드세요. Python의 문자열 포맷팅 기능 중 하나를 사용하여 변수 값을 문자열 안에 삽입해야 합니다.",
    "schema": "users(name STRING, age INT)",
    "sample_rows": [
      "Alice | 25",
      "Bob | 35"
    ],
    "difficulty": "Lv0 기초",
    "kind": "python",
    "expected": [
      "filter",
      "f\"age >= {target_age}\""
    ],
    "hint": "따옴표 앞에 `f`를 붙이고, 변수를 중괄호 `{}` 안에 넣는 **f-string** 문법을 사용해보세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_basic_in_operator",
    "title": "다중 값 일치 검색",
    "body": "products 테이블에서 category가 'Fruit'이거나 'Vegetable'인 상품을 조회하세요. `OR` 연산자를 반복해서 사용하는 대신, 목록 중 하나와 일치하는지 확인하는 연산자를 사용하세요.",
    "schema": "products(name TEXT, category TEXT)",
    "sample_rows": [
      "Apple | Fruit",
      "Beef | Meat",
      "Carrot | Vegetable"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "WHERE",
      "IN"
    ],
    "hint": "`컬럼명 IN ('값1', '값2')` 형태의 문법을 사용하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_col_object",
    "title": "PySpark 컬럼 연산 방식",
    "body": "`age` 컬럼의 값에 1을 더한 결과를 `age_plus_1`이라는 새로운 컬럼으로 만드세요. 문자열 \"age\"에 숫자를 더할 수 없으므로, 이를 컬럼 객체로 변환한 뒤 연산해야 합니다.",
    "schema": "users(name STRING, age INT)",
    "sample_rows": [
      "Alice | 20"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "withColumn",
      "F.col",
      "+ 1"
    ],
    "hint": "`F.col(\"컬럼명\")` 함수를 사용하여 문자열을 컬럼 객체로 감싸주세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_alias_column",
    "title": "조회 결과 컬럼명 변경",
    "body": "price 컬럼에 0.9를 곱한 값을 조회하되, 결과 컬럼의 이름을 `discounted_price`로 지정하여 출력하세요.",
    "schema": "products(name TEXT, price DECIMAL)",
    "sample_rows": [
      "Mouse | 10000"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "SELECT",
      "*",
      "AS"
    ],
    "hint": "계산식 뒤에 `AS 새로운이름`을 붙여줍니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_boolean_operators",
    "title": "PySpark 다중 조건 결합",
    "body": "age가 20 이상이고(AND), 30 미만인 사람을 필터링하세요. Python의 기본 키워드(`and`, `or`) 대신 PySpark 및 Pandas에서 사용하는 전용 비트 연산자를 사용해야 하며, 연산 순서에 주의해야 합니다.",
    "schema": "users(name STRING, age INT)",
    "sample_rows": [
      "Alice | 25",
      "Bob | 35"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "filter",
      "&",
      "( )"
    ],
    "hint": "각 조건을 괄호 `()`로 감싸고, `&` (AND) 또는 `|` (OR) 연산자를 사용하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_operator_not_equal",
    "title": "SQL: 특정 조건 제외하기",
    "body": "customers 테이블에서 status가 'inactive'가 **아닌** 고객들만 조회하고 싶습니다. '같다'는 `=`인데, '다르다(Not Equal)'는 어떤 기호를 써야 할까요?",
    "schema": "customers(id INT, status TEXT)",
    "sample_rows": [
      "1 | active",
      "2 | inactive",
      "3 | pending"
    ],
    "difficulty": "Lv0 기초",
    "kind": "sql",
    "expected": [
      "WHERE",
      "<>",
      "!="
    ],
    "hint": "표준 SQL에서는 `<>`를 많이 쓰지만, `!=`도 대부분 지원합니다. 둘 중 하나를 사용해보세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_operator_not_equal",
    "title": "PySpark: 특정 조건 제외하기",
    "body": "users 데이터프레임에서 age가 20이 **아닌** 사람만 남기고 싶습니다. Python의 비교 연산자를 사용하여 필터링하세요.",
    "schema": "users(name STRING, age INT)",
    "sample_rows": [
      "Alice | 20",
      "Bob | 25"
    ],
    "difficulty": "Lv0 기초",
    "kind": "python",
    "expected": [
      "filter",
      "!="
    ],
    "hint": "같다는 `==`이고, 다르다는 `!=` 입니다. `F.col('age') != 20` 형태로 작성합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_rename_column",
    "title": "PySpark: 컬럼 이름 변경",
    "body": "데이터프레임의 `user_name`이라는 컬럼 이름이 너무 길어서 `name`으로 단순하게 바꾸고 싶습니다. 컬럼의 **이름만** 변경하는 전용 메서드를 사용하세요.",
    "schema": "users(user_name STRING, age INT)",
    "sample_rows": [
      "Alice | 20"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "withColumnRenamed"
    ],
    "hint": "`df.withColumnRenamed('기존이름', '새이름')` 메서드를 사용합니다. `alias`는 select 할 때 쓰고, 이 메서드는 DF 자체를 변환할 때 씁니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_count_null_handling",
    "title": "SQL: 데이터 개수 세기와 NULL",
    "body": "orders 테이블의 전체 행 개수가 아니라, `customer_id`가 **비어있지 않은(NULL이 아닌)** 행의 개수만 세고 싶습니다. 괄호 안에 무엇을 넣어야 할까요?",
    "schema": "orders(id INT, customer_id INT)",
    "sample_rows": [
      "1 | 101",
      "2 | NULL",
      "3 | 102"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "COUNT",
      "(column_name)"
    ],
    "hint": "`COUNT(*)`는 NULL 포함 전체를 세고, `COUNT(컬럼명)`은 해당 컬럼의 NULL을 제외하고 셉니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_drop_duplicates_simple",
    "title": "PySpark: 중복 데이터 제거",
    "body": "로그 데이터인 `logs` 데이터프레임에서 완전히 동일한 중복 행들을 제거하고 유니크한 행만 남기려고 합니다. 가장 직관적인 메서드를 사용하세요.",
    "schema": "logs(time STRING, user STRING)",
    "sample_rows": [
      "10:00 | Alice",
      "10:00 | Alice",
      "10:01 | Bob"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "dropDuplicates",
      "distinct"
    ],
    "hint": "`df.dropDuplicates()` 또는 `df.distinct()`를 사용합니다. 실무에서는 특정 컬럼 기준 중복 제거가 가능한 `dropDuplicates()`를 더 자주 씁니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_between_operator",
    "title": "SQL: 범위 검색 단순화",
    "body": "products 테이블에서 가격(price)이 1000 이상이고 2000 이하인 상품을 찾고 싶습니다. `>=`와 `<=`를 `AND`로 연결해도 되지만, 더 읽기 쉬운 전용 연산자가 있습니다.",
    "schema": "products(name TEXT, price INT)",
    "sample_rows": [
      "A | 500",
      "B | 1500",
      "C | 2500"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "WHERE",
      "BETWEEN",
      "AND"
    ],
    "hint": "`price BETWEEN 1000 AND 2000` 구문을 사용하세요. (시작과 끝 값을 포함합니다)",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_sort_descending",
    "title": "PySpark: 내림차순 정렬",
    "body": "scores 데이터프레임을 점수(score)가 높은 순서(내림차순)로 정렬하고 싶습니다. 단순히 `orderBy`만 쓰면 오름차순이 됩니다. 내림차순을 위해 필요한 함수는 무엇일까요?",
    "schema": "scores(name STRING, score INT)",
    "sample_rows": [
      "Alice | 50",
      "Bob | 100"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "orderBy",
      "desc"
    ],
    "hint": "`F.col('score').desc()` 함수를 사용해야 합니다. (`F.desc('score')`도 가능)",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_like_suffix",
    "title": "SQL: 끝나는 문자열 찾기",
    "body": "users 테이블에서 이메일이 '@gmail.com'으로 **끝나는** 모든 사용자를 조회하세요. 문자열 패턴 매칭 연산자를 사용해야 합니다.",
    "schema": "users(name TEXT, email TEXT)",
    "sample_rows": [
      "A | a@naver.com",
      "B | b@gmail.com"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "WHERE",
      "LIKE",
      "%"
    ],
    "hint": "`email LIKE '%@gmail.com'`을 사용하세요. `%`는 '0개 이상의 아무 문자'를 뜻합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_startswith",
    "title": "PySpark: 시작하는 문자열 찾기",
    "body": "products 데이터프레임에서 상품명(name)이 'New'로 **시작하는** 상품만 필터링하세요. SQL의 LIKE보다 더 직관적인 Python 스타일의 문자열 메서드를 사용할 수 있습니다.",
    "schema": "products(name STRING, price INT)",
    "sample_rows": [
      "New Laptop | 100",
      "Old Mouse | 10"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "filter",
      "startswith"
    ],
    "hint": "`F.col('name').startswith('New')` 메서드를 사용하세요. 코드가 훨씬 읽기 좋아집니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_select_math",
    "title": "SQL: 계산 결과 조회",
    "body": "employees 테이블에서 연봉(salary)을 12로 나눈 'monthly_salary'를 조회하고 싶습니다. SELECT 문 안에서 바로 나눗셈 연산을 수행하세요.",
    "schema": "employees(name TEXT, salary INT)",
    "sample_rows": [
      "Alice | 12000"
    ],
    "difficulty": "Lv0 기초",
    "kind": "sql",
    "expected": [
      "SELECT",
      "/",
      "AS"
    ],
    "hint": "`salary / 12 AS monthly_salary` 처럼 작성하면 됩니다. SQL은 SELECT 절에서 사칙연산이 가능합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_select_subset",
    "title": "PySpark: 원하는 컬럼만 골라내기",
    "body": "데이터프레임에 컬럼이 100개쯤 되는데, 그 중에서 `id`와 `email` 두 개의 컬럼만 딱 뽑아서 새로운 데이터프레임을 만들고 싶습니다.",
    "schema": "users(id INT, email STRING, address STRING, phone STRING ...)",
    "sample_rows": [
      "1 | a@test.com | Seoul | 010..."
    ],
    "difficulty": "Lv0 기초",
    "kind": "python",
    "expected": [
      "select"
    ],
    "hint": "`df.select('id', 'email')`을 사용하세요. 분석의 첫 단계는 필요한 데이터만 남기는 것입니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_cast_string_to_int",
    "title": "PySpark: 문자열을 숫자로 변환",
    "body": "`age` 컬럼이 숫자가 아닌 문자열(\"25\")로 저장되어 있어서 산술 연산이 안 됩니다. 이를 숫자(Integer) 타입으로 변환하세요.",
    "schema": "users(name STRING, age STRING)",
    "sample_rows": [
      "Alice | \"25\""
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "withColumn",
      "cast"
    ],
    "hint": "`F.col('age').cast('int')` 또는 `.cast('integer')`를 사용합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_coalesce_basic",
    "title": "SQL: 빈 값(NULL)을 다른 값으로 대체하기",
    "body": "customers 테이블에서 전화번호(phone)를 조회하고 싶습니다. 그런데 전화번호가 NULL인 경우에는 '미입력'이라고 출력하고 싶습니다. IF문을 쓰지 않고도 이를 처리하는 아주 유용한 표준 함수가 있습니다.",
    "schema": "customers(name TEXT, phone TEXT)",
    "sample_rows": [
      "Alice | 010-1234-5678",
      "Bob | NULL"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "SELECT",
      "COALESCE"
    ],
    "hint": "`COALESCE(컬럼명, '대체값')` 함수를 사용하세요. NULL이 아니면 원래 값을, NULL이면 뒤의 값을 반환합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_when_otherwise",
    "title": "PySpark: 조건에 따라 다른 값 넣기 (if-else)",
    "body": "products 데이터프레임에서 `price`가 10000 이상이면 'High', 그렇지 않으면 'Low'라는 값을 가진 `price_level` 컬럼을 새로 만들고 싶습니다. Python의 `if-else` 문법 대신 PySpark 전용 함수를 사용해야 합니다.",
    "schema": "products(name STRING, price INT)",
    "sample_rows": [
      "Mouse | 5000",
      "Keyboard | 15000"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "withColumn",
      "when",
      "otherwise"
    ],
    "hint": "`F.when(조건, 값).otherwise(값)` 형태를 사용합니다. 아주 자주 쓰이는 패턴입니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_literal_column",
    "title": "PySpark: 모든 행에 똑같은 값(상수) 넣기",
    "body": "users 데이터프레임에 `is_active`라는 컬럼을 추가하고, 모든 유저에게 `True` 값을 주고 싶습니다. `withColumn('is_active', True)`라고 쓰면 에러가 납니다. 값을 컬럼 객체로 만들어주는 함수가 필요합니다.",
    "schema": "users(name STRING)",
    "sample_rows": [
      "Alice",
      "Bob"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "withColumn",
      "lit"
    ],
    "hint": "단순한 값(숫자, 문자, 불린)을 컬럼으로 넣을 때는 반드시 `F.lit(값)`으로 감싸야 합니다. (Literal의 줄임말)",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_logic_precedence",
    "title": "SQL: AND와 OR가 섞여 있을 때",
    "body": "products 테이블에서 (카테고리가 'Electronics'이거나 'Computer')이고, 동시에 (가격이 10000 이상)인 상품을 조회하세요. 괄호 없이 그냥 나열하면 의도와 다른 결과가 나옵니다.",
    "schema": "products(name TEXT, category TEXT, price INT)",
    "sample_rows": [
      "Mouse | Electronics | 5000",
      "Monitor | Computer | 20000"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "WHERE",
      "( ... OR ... )",
      "AND"
    ],
    "hint": "AND는 OR보다 우선순위가 높습니다. 따라서 OR 조건을 먼저 묶어주려면 반드시 **괄호 `()`**를 사용해야 합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_print_schema",
    "title": "PySpark: 데이터 구조(타입) 확인하기",
    "body": "데이터를 로드했는데 `age`가 숫자인지 문자인지, `reg_date`가 날짜인지 문자인지 헷갈립니다. 데이터프레임의 컬럼 이름과 데이터 타입을 트리 형태로 예쁘게 출력해서 확인하는 메서드는 무엇일까요?",
    "schema": "N/A (데이터 확인 문제)",
    "sample_rows": [],
    "difficulty": "Lv0 기초",
    "kind": "python",
    "expected": [
      "printSchema()"
    ],
    "hint": "`df.printSchema()`를 실행하면 스키마 정보를 눈으로 확인할 수 있습니다. 디버깅할 때 필수입니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_min_max_basic",
    "title": "SQL: 최솟값과 최댓값 구하기",
    "body": "employees 테이블에서 가장 높은 연봉(salary)과 가장 낮은 연봉을 알고 싶습니다. 두 개의 집계 함수를 사용하여 한 번에 조회하세요.",
    "schema": "employees(name TEXT, salary INT)",
    "sample_rows": [
      "Alice | 5000",
      "Bob | 9000"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "SELECT",
      "MAX",
      "MIN"
    ],
    "hint": "`MAX(salary)`, `MIN(salary)`를 사용합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_show_action",
    "title": "PySpark: 데이터 눈으로 확인하기",
    "body": "열심히 필터링하고 컬럼을 추가했습니다. 이제 결과 데이터의 상위 20줄을 화면에 출력해서 제대로 처리되었는지 확인하고 싶습니다. 가장 많이 쓰는 'Action' 메서드는 무엇일까요?",
    "schema": "result_df(name STRING, age INT)",
    "sample_rows": [],
    "difficulty": "Lv0 기초",
    "kind": "python",
    "expected": [
      "show()"
    ],
    "hint": "`df.show()`를 사용합니다. 괄호 안에 숫자를 넣어 `df.show(5)` 처럼 줄 수를 지정할 수도 있습니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_date_string_format",
    "title": "SQL: 날짜 형식의 문자열 검색",
    "body": "orders 테이블에서 `order_date`가 '2024-01-01' 이후인 주문을 조회하고 싶습니다. 날짜 컬럼과 문자열을 비교할 때, 날짜 포맷(형식)을 어떻게 맞춰야 할까요?",
    "schema": "orders(id INT, order_date DATE)",
    "sample_rows": [
      "1 | 2023-12-31",
      "2 | 2024-01-02"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "WHERE",
      ">",
      "'YYYY-MM-DD'"
    ],
    "hint": "SQL 표준 날짜 포맷은 **'YYYY-MM-DD'**입니다. `order_date > '2024-01-01'` 처럼 작은따옴표로 감싸서 비교하면 자동으로 날짜로 인식합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_fill_null",
    "title": "PySpark: 결측치(NULL) 일괄 채우기",
    "body": "users 데이터프레임에 `age`가 비어있는(NULL) 행들이 있습니다. 비어있는 나이를 모두 0으로 채워넣고 싶습니다. `fillna` (또는 `na.fill`) 함수를 사용해보세요.",
    "schema": "users(name STRING, age INT)",
    "sample_rows": [
      "Alice | NULL",
      "Bob | 25"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "fillna",
      "na.fill"
    ],
    "hint": "`df.fillna(0, subset=['age'])` 형태로 사용합니다. 특정 컬럼을 지정하지 않으면 호환되는 모든 컬럼을 채워버리니 주의하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_distinct_count",
    "title": "SQL: 중복을 제외한 수 세기",
    "body": "access_logs 테이블에는 유저가 접속할 때마다 로그가 쌓입니다. 총 접속 횟수가 아니라, 접속한 **유저가 몇 명인지(고유 방문자 수)** 알고 싶다면 `COUNT` 안에 무엇을 추가해야 할까요?",
    "schema": "access_logs(log_id INT, user_id INT)",
    "sample_rows": [
      "1 | 100",
      "2 | 100",
      "3 | 200"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "COUNT",
      "DISTINCT"
    ],
    "hint": "`COUNT(DISTINCT user_id)`를 사용합니다. 중복을 제거한 뒤 개수를 셉니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_chaining",
    "title": "PySpark: 메서드 체이닝 (점 찍어 연결하기)",
    "body": "PySpark의 큰 장점은 코드를 한 줄씩 끊어 쓰지 않고 연결할 수 있다는 점입니다. (1) `age`가 20 이상인 사람을 필터링하고 (2) `name` 컬럼만 선택하는 과정을 **점(.)**으로 연결해서 한 번에 작성해보세요.",
    "schema": "users(name STRING, age INT, city STRING)",
    "sample_rows": [
      "Alice | 25 | Seoul",
      "Bob | 15 | Busan"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "df.filter(...).select(...)"
    ],
    "hint": "`df.filter(조건).select(컬럼)` 처럼 메서드 뒤에 바로 점을 찍어 다음 메서드를 연결하는 방식을 '체이닝'이라고 합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_concat_strings",
    "title": "SQL: 문자열 합치기",
    "body": "first_name이 'Gildong', last_name이 'Hong'일 때, 이를 합쳐서 'Gildong Hong' (중간에 공백 포함)이라는 `full_name`을 만들고 싶습니다. 문자열 연결 연산자나 함수를 사용하세요.",
    "schema": "users(first_name TEXT, last_name TEXT)",
    "sample_rows": [
      "Gildong | Hong"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "CONCAT",
      "||"
    ],
    "hint": "데이터베이스마다 다르지만 표준적으로 `CONCAT(first_name, ' ', last_name)`을 쓰거나, `||` 기호를 사용합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_select_distinct_category",
    "title": "중복 제거와 카디널리티 확인",
    "body": "products 테이블에서 현재 판매 중인 상품들의 카테고리(category) 목록을 조회하세요. 동일한 카테고리가 여러 번 출력되지 않아야 하며, 결과는 알파벳 순으로 정렬하세요. 실무에서는 컬럼의 고유 값 개수(Cardinality)를 파악할 때 자주 사용됩니다.",
    "schema": "products(product_id INT, category TEXT, price DECIMAL)",
    "sample_rows": [
      "1 | Electronics | 120000",
      "2 | Electronics | 35000",
      "3 | Books | 15000",
      "4 | Beauty | 25000"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "SELECT DISTINCT",
      "ORDER BY"
    ],
    "hint": "`SELECT DISTINCT category`를 사용하여 중복을 제거한 후 정렬하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_filter_literal",
    "title": "상수 컬럼 추가와 리터럴 처리",
    "body": "users 데이터프레임에 'country'라는 새로운 컬럼을 추가하고, 모든 행에 'Korea'라는 값을 입력하세요. PySpark에서는 문자열 값을 직접 컬럼으로 다룰 때 함수 사용이 필요합니다.",
    "schema": "users(user_id INT, name STRING)",
    "sample_rows": [
      "1 | Alice",
      "2 | Bob"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "withColumn",
      "F.lit"
    ],
    "hint": "단순 문자열을 할당하면 에러가 납니다. `df.withColumn('country', F.lit('Korea'))`를 사용하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_where_null_check",
    "title": "NULL 값과 빈 문자열의 차이",
    "body": "customers 테이블에서 이메일(email) 정보가 없는 고객을 조회하세요. 데이터베이스에 따라 '정보 없음'이 NULL로 저장되기도 하고, 빈 문자열('')로 저장되기도 하므로 두 경우를 모두 고려해야 합니다.",
    "schema": "customers(id INT, name TEXT, email TEXT)",
    "sample_rows": [
      "1 | Alice | alice@example.com",
      "2 | Bob | NULL",
      "3 | Charlie | ",
      "4 | David | david@example.com"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "WHERE",
      "IS NULL",
      "OR",
      "=''"
    ],
    "hint": "`WHERE email IS NULL OR email = ''` 조건을 사용하세요. `email = NULL`은 동작하지 않습니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_cast_type",
    "title": "데이터 타입 변환과 스키마 불일치",
    "body": "sales 데이터의 price 컬럼이 실수로 문자열(STRING) 타입으로 적재되었습니다. 이를 정수형(INTEGER)으로 변환한 뒤, 가격이 10000원 이상인 데이터만 필터링하세요.",
    "schema": "sales(product_id INT, price STRING)",
    "sample_rows": [
      "101 | '5000'",
      "102 | '15000'",
      "103 | '900'"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "withColumn",
      "cast",
      "filter"
    ],
    "hint": "`df.withColumn('price', F.col('price').cast('int'))`로 타입을 변경한 후 필터링을 수행하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_order_limit",
    "title": "상위 N개 추출: 정렬과 제한",
    "body": "orders 테이블에서 가장 최근에 주문된 5개의 주문 기록을 조회하세요. 전체 데이터를 가져온 후 애플리케이션에서 자르는 것보다, DB단에서 필요한 데이터만 가져오는 것이 네트워크 비용 절감에 유리합니다.",
    "schema": "orders(order_id INT, amount DECIMAL, order_date TIMESTAMP)",
    "sample_rows": [
      "1 | 50000 | 2024-03-01 10:00:00",
      "2 | 10000 | 2024-03-01 10:05:00",
      "3 | 30000 | 2024-03-01 09:00:00"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "ORDER BY DESC",
      "LIMIT"
    ],
    "hint": "`ORDER BY order_date DESC LIMIT 5` 구문을 사용하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_select_alias",
    "title": "컬럼 선택과 이름 변경(Alias)",
    "body": "복잡한 컬럼명(product_manufacturing_cost)을 분석하기 쉽게 `cost`로 이름을 변경하여 선택하고, `product_id`와 함께 출력하세요. 긴 컬럼명은 코드를 읽기 어렵게 만듭니다.",
    "schema": "products(product_id INT, product_manufacturing_cost DOUBLE)",
    "sample_rows": [
      "1 | 15.50",
      "2 | 20.00"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "select",
      "alias"
    ],
    "hint": "`df.select(F.col('product_id'), F.col('product_manufacturing_cost').alias('cost'))`를 사용하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_like_wildcard",
    "title": "특정 패턴 검색: 인덱스와 Like",
    "body": "users 테이블에서 이름이 'J'로 시작하는 모든 사용자를 조회하세요. 문자열 패턴 검색 시 와일드카드(%)의 위치에 따라 인덱스 활용 여부가 달라질 수 있습니다.",
    "schema": "users(user_id INT, name TEXT)",
    "sample_rows": [
      "1 | James",
      "2 | Alice",
      "3 | John",
      "4 | Robert"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "WHERE",
      "LIKE"
    ],
    "hint": "`WHERE name LIKE 'J%'`를 사용하세요. `%J%`와 달리 접두사 검색은 인덱스를 탈 가능성이 높습니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_drop_columns",
    "title": "불필요한 컬럼 삭제: 메모리 최적화",
    "body": "분석에 필요 없는 민감 정보인 `password`와 `ssn` 컬럼을 데이터프레임에서 제거하세요. 불필요한 데이터를 계속 들고 다니는 것은 메모리 낭비의 주원인입니다.",
    "schema": "users(user_id INT, name STRING, password STRING, ssn STRING)",
    "sample_rows": [
      "1 | Alice | 1234 | 111-222",
      "2 | Bob | abcd | 333-444"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "drop"
    ],
    "hint": "`df.drop('password', 'ssn')`과 같이 여러 컬럼을 한 번에 삭제할 수 있습니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_basic_math",
    "title": "파생 변수 생성: 산술 연산",
    "body": "products 테이블에서 가격(price)에 10% 부가세를 더한 `final_price`를 계산하여 조회하세요. 기존 컬럼을 조합하여 새로운 지표를 만드는 것은 데이터 분석의 기초입니다.",
    "schema": "products(product_id INT, price DECIMAL)",
    "sample_rows": [
      "1 | 10000",
      "2 | 20000"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "SELECT",
      "*",
      "AS"
    ],
    "hint": "`SELECT product_id, price, price * 1.1 AS final_price FROM products`와 같이 작성하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_isin_filter",
    "title": "다중 값 필터링: OR 조건의 단순화",
    "body": "products 데이터에서 카테고리가 'Electronics', 'Books', 'Garden' 중 하나인 상품만 필터링하세요. OR 조건을 여러 번 쓰는 것보다 포함 여부를 확인하는 것이 가독성에 좋습니다.",
    "schema": "products(product_id INT, category STRING)",
    "sample_rows": [
      "1 | Electronics",
      "2 | Beauty",
      "3 | Books",
      "4 | Toys"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "filter",
      "isin"
    ],
    "hint": "`df.filter(F.col('category').isin(['Electronics', 'Books', 'Garden']))`을 사용하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_boolean_logic",
    "title": "복합 논리 연산: AND와 괄호의 중요성",
    "body": "inventory 테이블에서 재고(stock)가 10개 미만이면서, 동시에 단종되지 않은(is_discontinued = 'N') 상품을 조회하세요. 긴급 발주가 필요한 목록입니다.",
    "schema": "inventory(product_id INT, stock INT, is_discontinued CHAR(1))",
    "sample_rows": [
      "1 | 5 | N",
      "2 | 0 | Y",
      "3 | 20 | N",
      "4 | 8 | Y"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "WHERE",
      "AND",
      "<"
    ],
    "hint": "`WHERE stock < 10 AND is_discontinued = 'N'` 조건을 사용하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_orderby_multiple",
    "title": "다중 정렬 기준: 우선순위 적용",
    "body": "employees 데이터를 부서(department)별로 오름차순 정렬하되, 같은 부서 내에서는 연봉(salary)이 높은 순서(내림차순)로 정렬하세요.",
    "schema": "employees(name STRING, department STRING, salary INT)",
    "sample_rows": [
      "Alice | Sales | 5000",
      "Bob | Sales | 6000",
      "Charlie | HR | 4000"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "orderBy",
      "asc",
      "desc"
    ],
    "hint": "`df.orderBy(F.col('department').asc(), F.col('salary').desc())`와 같이 정렬 기준을 순서대로 나열하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_select_filter",
    "title": "신규 고객 조회와 인덱스 활용",
    "body": "customers 테이블에서 가입일(signup_date)이 2024-01-01 이후인 고객을 조회하세요. signup_date 컬럼에 인덱스가 걸려있다고 가정할 때, 함수 사용을 자제하고 원본 컬럼을 그대로 사용하여 조회 성능을 최적화해야 합니다.",
    "schema": "customers(id INT, name TEXT, signup_date DATE)",
    "sample_rows": [
      "1 | Alice | 2024-02-10",
      "2 | Bob | 2023-12-30",
      "3 | Casey | 2024-03-01",
      "4 | David | NULL",
      "5 | Eve | 2024-01-01"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "SELECT",
      "WHERE",
      ">=",
      "ORDER BY"
    ],
    "hint": "컬럼을 가공(예: YEAR(signup_date))하면 인덱스를 타지 않을 수 있습니다. `signup_date >= '2024-01-01'`과 같이 원본 컬럼을 그대로 비교하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_aggregation_monthly",
    "title": "월별 매출 보고서: 날짜 절삭과 포맷팅",
    "body": "sales 테이블에서 월별 총 매출액을 계산하세요. 분석 툴(BI)에서 인식하기 쉽도록 문자열이 아닌 날짜 타입으로 월의 첫날(예: 2024-01-01)을 기준으로 그룹화하고, 출력 시에만 포맷을 변경하는 것이 좋습니다.",
    "schema": "sales(order_date DATE, amount DECIMAL)",
    "sample_rows": [
      "2024-01-05 | 120000",
      "2024-01-18 | 98000",
      "2024-02-02 | 150000",
      "NULL | 10000",
      "2024-02-28 | 5000"
    ],
    "difficulty": "Lv2 초급",
    "kind": "sql",
    "expected": [
      "GROUP BY",
      "SUM",
      "DATE_TRUNC",
      "TO_CHAR / DATE_FORMAT",
      "ORDER BY"
    ],
    "hint": "`DATE_TRUNC('month', order_date)`를 사용하여 월 단위로 절삭한 후 그룹화하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_join_customer_orders",
    "title": "고객별 주문 합계: 1:N 조인과 데이터 뻥튀기 주의",
    "body": "customers와 orders 테이블을 INNER JOIN하여 고객별 총 주문 금액을 구하세요. 실무에서는 조인 키가 중복될 경우 결과 행이 의도치 않게 늘어나는(Fan-out) 현상을 주의해야 합니다.",
    "schema": "customers(cust_id INT, name TEXT)\norders(order_id INT, cust_id INT, amount DECIMAL)",
    "sample_rows": [
      "customers: 1 | Alice",
      "customers: 2 | Bob",
      "orders: 10 | 1 | 50000",
      "orders: 11 | NULL | 30000",
      "orders: 12 | 2 | 45000",
      "orders: 13 | 1 | 20000"
    ],
    "difficulty": "Lv3 중급",
    "kind": "sql",
    "expected": [
      "JOIN",
      "ON",
      "GROUP BY",
      "SUM",
      "ORDER BY"
    ],
    "hint": "조인 조건은 `ON c.cust_id = o.cust_id`입니다. 집계 함수 `SUM(amount)`는 조인 결과 집합에 대해 수행됨을 유의하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_window_rank",
    "title": "카테고리별 인기 상품: 동점자 처리 전략",
    "body": "products 테이블에서 카테고리별 가장 비싼 상품을 조회하세요. 비즈니스 요건상 가격이 동일하다면 공동 1등으로 모두 보여줘야 합니다. 이런 경우 적절한 윈도우 함수를 선택해야 합니다.",
    "schema": "products(id INT, category TEXT, price DECIMAL)",
    "sample_rows": [
      "1 | electronics | 120000",
      "2 | electronics | 120000",
      "3 | electronics | 90000",
      "4 | books | 18000",
      "5 | books | 15000"
    ],
    "difficulty": "Lv4 고급",
    "kind": "sql",
    "expected": [
      "RANK() OVER",
      "PARTITION BY",
      "ORDER BY DESC",
      "Subquery / CTE"
    ],
    "hint": "`ROW_NUMBER()`는 강제로 순위를 매기지만, `RANK()`나 `DENSE_RANK()`는 동점을 처리합니다. 공동 1등을 모두 포함하려면 `RANK()`를 사용하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_running_total",
    "title": "누적 매출 집계: 윈도우 함수의 정렬 비용",
    "body": "sales 테이블에서 월별 매출과 함께 '누적 매출'을 계산하세요. 윈도우 함수 사용 시 정렬(ORDER BY)은 비용이 많이 드는 작업이므로, 인덱스가 있다면 이를 활용하는 것이 좋으나 여기서는 쿼리로 해결합니다.",
    "schema": "sales(month TEXT, amount DECIMAL)",
    "sample_rows": [
      "2024-01 | 200000",
      "2024-03 | 180000",
      "2024-02 | 230000",
      "2024-04 | 150000"
    ],
    "difficulty": "Lv5 심화",
    "kind": "sql",
    "expected": [
      "SUM() OVER",
      "ORDER BY",
      "ROWS BETWEEN"
    ],
    "hint": "`SUM(amount) OVER (ORDER BY month)` 구문을 사용하세요. 문자열 날짜 포맷이 'YYYY-MM'으로 통일되어 있다면 별도 캐스팅 없이 정렬 가능합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_basic_filter",
    "title": "활성 유저 필터링: Boolean 타입과 Predicate Pushdown",
    "body": "users DataFrame에서 활성 유저(active=True)이면서 마지막 로그인이 2024년 이후인 데이터를 필터링하세요. 파케이(Parquet) 등의 포맷을 사용할 때 필터 순서는 데이터 스캔량을 줄이는 데 영향을 줄 수 있습니다.",
    "schema": "users(id INT, active BOOLEAN, last_login TIMESTAMP)",
    "sample_rows": [
      "1 | true | 2024-02-03 10:00:00",
      "2 | false | NULL",
      "3 | true | 2023-12-10 15:30:00",
      "4 | true | 2024-01-05 09:20:00"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "filter / where",
      "col('active') == True",
      "col('last_login') > ..."
    ],
    "hint": "`df.filter((F.col('active') == True) & (F.col('last_login') >= F.lit('2024-01-01')))` 형태로 작성하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_join",
    "title": "주문 데이터 결합: 셔플링(Shuffling)과 성능",
    "body": "customers와 orders DataFrame을 INNER JOIN 하세요. 두 테이블 모두 크기가 클 경우, 조인 키를 기준으로 대규모 데이터 이동(Shuffle)이 발생하여 성능 저하의 원인이 됩니다.",
    "schema": "customers(cust_id INT, name STRING)\norders(order_id INT, cust_id INT, amount DOUBLE)",
    "sample_rows": [
      "Cust: 1 | Alice",
      "Cust: 2 | Bob",
      "Ord: 10 | 1 | 50000.0",
      "Ord: 11 | 3 | 30000.0",
      "Ord: 12 | 2 | 10000.0"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "join",
      "on='cust_id'",
      "how='inner'"
    ],
    "hint": "`orders.join(customers, on='cust_id', how='inner')`를 사용합니다. 키 컬럼의 이름이 같다면 `on` 파라미터를 단일 문자열로 전달하는 것이 깔끔합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_window",
    "title": "센서 데이터 이동 평균: 윈도우 정의와 파티셔닝",
    "body": "events 데이터에서 시간 순으로 정렬하여 직전 2개 데이터와 현재 데이터의 평균(Moving Average)을 구하세요. 단일 파티션으로 윈도우를 몰아넣으면(OOM 위험), 병렬 처리가 불가능하므로 적절한 파티션 키가 필요할 수 있습니다.",
    "schema": "events(device_id INT, time TIMESTAMP, value DOUBLE)",
    "sample_rows": [
      "1 | 10:00 | 10.0",
      "1 | 10:05 | 14.0",
      "1 | 10:10 | 12.0",
      "2 | 10:00 | 20.0"
    ],
    "difficulty": "Lv3 중급",
    "kind": "python",
    "expected": [
      "Window.partitionBy",
      "orderBy",
      "rowsBetween",
      "F.avg"
    ],
    "hint": "`Window.partitionBy('device_id').orderBy('time').rowsBetween(-2, 0)`을 정의하여 디바이스별로 계산되도록 하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_pivot",
    "title": "주간 상태 리포트: 피벗(Pivot)과 카디널리티",
    "body": "logs 데이터에서 각 주차(Week)별로 상태 코드(status)의 발생 횟수를 피벗하여 집계하세요. 피벗 대상 컬럼(status)의 고유 값(Cardinality)이 너무 많으면 컬럼 폭발이 일어날 수 있으니 주의해야 합니다.",
    "schema": "logs(timestamp TIMESTAMP, status STRING)",
    "sample_rows": [
      "2024-01-02 | success",
      "2024-01-03 | fail",
      "2024-01-08 | success",
      "2024-01-09 | pending"
    ],
    "difficulty": "Lv4 고급",
    "kind": "python",
    "expected": [
      "date_trunc",
      "groupBy",
      "pivot",
      "count",
      "fillna"
    ],
    "hint": "`df.groupBy(F.date_trunc('week', 'timestamp')).pivot('status').count()`를 사용하고, 결측치는 `.fillna(0)`으로 처리하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_sessionize",
    "title": "세션 ID 생성: Lag 함수를 이용한 복합 로직",
    "body": "유저별로 페이지 방문 기록이 있습니다. 이전 방문과 30분 이상 차이가 나면 새로운 세션으로 간주하여 고유 세션 ID를 부여하세요. 이는 Clickstream 데이터 분석의 핵심 로직입니다.",
    "schema": "pageviews(user_id INT, ts TIMESTAMP)",
    "sample_rows": [
      "1 | 10:00",
      "1 | 10:10",
      "1 | 11:00 (New Session)",
      "1 | 11:05"
    ],
    "difficulty": "Lv5 심화",
    "kind": "python",
    "expected": [
      "Window",
      "F.lag",
      "F.when",
      "F.sum().over"
    ],
    "hint": "1. `F.lag('ts')`로 이전 시간 확보 \n2. 시간 차이 > 30분이면 1, 아니면 0인 플래그 생성 \n3. 해당 플래그를 누적 합(`F.sum`)하여 세션 ID 생성.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_broadcast_join_large_small",
    "title": "브로드캐스트 조인 최적화: 셔플 제거",
    "body": "매우 큰 주문 테이블(df_large)과 작은 코드 테이블(df_small, 10MB 미만)을 조인할 때, 작은 테이블을 각 노드로 복사(Broadcast)하여 네트워크 비용(Shuffle)을 획기적으로 줄이세요.",
    "schema": "df_large(id INT, code_id INT, amount INT)\ndf_small(code_id INT, description STRING)",
    "sample_rows": [
      "Large: 1M rows...",
      "Small: 100 rows (1 | 'Discount', 2 | 'Regular')"
    ],
    "difficulty": "Lv3 중급",
    "kind": "python",
    "expected": [
      "F.broadcast",
      "join"
    ],
    "hint": "`df_large.join(F.broadcast(df_small), 'code_id')` 구문을 사용하여 스파크가 맵 사이드 조인(Map-side Join)을 수행하도록 유도하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_explode_array_to_rows",
    "title": "JSON 배열 처리: Explode와 데이터 정규화",
    "body": "한 유저가 구매한 상품들이 배열(Array) 형태로 저장되어 있습니다. 이를 분석하기 쉽게 상품당 한 행(Row)을 가지도록 펼쳐주세요(Normalize). 배열 크기가 클 경우 데이터 양이 급증할 수 있습니다.",
    "schema": "orders(user_id INT, items ARRAY<STRING>)",
    "sample_rows": [
      "1 | ['Apple', 'Banana']",
      "2 | ['Cherry']",
      "3 | []",
      "4 | NULL"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "F.explode",
      "withColumn / select"
    ],
    "hint": "`df.select('user_id', F.explode('items').alias('item'))`을 사용하세요. `explode_outer`를 쓰면 빈 배열도 보존할 수 있습니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_udf_string_length",
    "title": "사용자 정의 함수(UDF) 주의점: 직렬화 비용",
    "body": "name 컬럼의 길이를 구하세요. Python UDF는 JVM과 Python 프로세스 간 데이터 이동(직렬화/역직렬화) 비용이 크므로, 가능한 Spark 내장 함수를 사용하는 것이 성능에 유리합니다.",
    "schema": "users(name STRING)",
    "sample_rows": [
      "Alice",
      "Bob",
      "Christopher"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "F.length",
      "(Optional) F.udf"
    ],
    "hint": "단순 길이는 `F.length('name')` 내장 함수가 가장 빠릅니다. 굳이 UDF를 써야 한다면 `@F.udf(returnType=IntegerType())` 데코레이터를 사용하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_repartition_coalesce",
    "title": "파티션 제어: Coalesce vs Repartition",
    "body": "작업 후 파티션 개수가 100개인데 파일 크기가 너무 작습니다(Small File Issue). 셔플링을 최소화하면서 파티션 개수를 1개로 줄여 단일 파일로 저장하고 싶습니다.",
    "schema": "df(id INT, data STRING)",
    "sample_rows": [
      "Partition 1: 10 rows",
      "Partition 2: 5 rows",
      "..."
    ],
    "difficulty": "Lv3 중급",
    "kind": "python",
    "expected": [
      "coalesce(1)",
      "write"
    ],
    "hint": "`df.repartition(1)`은 전체 셔플을 유발합니다. 단순히 파티션을 합치는 것이라면 `df.coalesce(1)`이 훨씬 효율적입니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_cache_usage",
    "title": "중간 결과 재사용: Cache와 Persist 전략",
    "body": "복잡한 전처리를 거친 DataFrame을 사용하여 두 가지 다른 분석(액션)을 수행합니다. 전처리 과정이 두 번 실행되지 않도록 메모리에 캐싱하세요. 메모리가 부족할 경우 디스크 스필(Spill)이 발생할 수 있습니다.",
    "schema": "heavy_df(id INT, computed_value DOUBLE)",
    "sample_rows": [
      "1 | 0.123 (computed costly)",
      "2 | 0.456 (computed costly)"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "cache / persist",
      "count",
      "show"
    ],
    "hint": "`df.cache()` 호출 후 첫 번째 액션(`count()` 등)이 일어날 때 메모리에 적재됩니다. 작업이 끝나면 `df.unpersist()`로 해제하는 것이 좋습니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_null_cleaning_sales",
    "title": "더러운 데이터 정제: 유효성 검사",
    "body": "매출 금액이 음수이거나 NULL인 경우는 시스템 오류로 간주합니다. 유효한 매출(양수)과 2024년 데이터만 조회하세요. 현업 데이터는 항상 예상 범위를 벗어날 수 있음을 가정해야 합니다.",
    "schema": "sales(product_name TEXT, order_date DATE, amount DECIMAL)",
    "sample_rows": [
      "Laptop | 2024-01-05 | 150000",
      "Mouse | 2023-12-28 | NULL",
      "Keyboard | 2024-02-10 | -5000"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "WHERE",
      "AND",
      ">",
      "ORDER BY"
    ],
    "hint": "`WHERE amount > 0` 조건은 암시적으로 NULL을 배제합니다. 날짜 필터는 `order_date >= '2024-01-01'`을 사용하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_agg_approx_count",
    "title": "대용량 고유 유저 수: 근사 집계(HyperLogLog)",
    "body": "수십억 건의 로그에서 고유 방문자 수(Distinct Users)를 구해야 합니다. 정확한 `distinct count`는 메모리를 많이 사용하고 느립니다. 오차 허용 범위 내에서 빠르게 결과를 내는 근사 함수를 사용하세요.",
    "schema": "logs(user_id STRING, page_id STRING)",
    "sample_rows": [
      "uuid-1 | home",
      "uuid-2 | detail",
      "uuid-1 | cart"
    ],
    "difficulty": "Lv4 고급",
    "kind": "python",
    "expected": [
      "F.approx_count_distinct"
    ],
    "hint": "`F.countDistinct('user_id')` 대신 `F.approx_count_distinct('user_id', rsd=0.05)`를 사용하면 훨씬 빠른 속도로 근사치를 얻을 수 있습니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_cte_readability",
    "title": "복잡한 쿼리 구조화: CTE(Common Table Expression) 활용",
    "body": "카테고리별 매출 합계를 구하고, 그 중 평균 매출보다 높은 카테고리만 조회하세요. 서브쿼리를 중첩(Nested Subquery)하는 것보다 CTE를 사용하면 가독성과 유지보수성이 좋아집니다.",
    "schema": "sales(category TEXT, amount DECIMAL)",
    "sample_rows": [
      "A | 100",
      "B | 200",
      "A | 150",
      "C | 50"
    ],
    "difficulty": "Lv3 중급",
    "kind": "sql",
    "expected": [
      "WITH",
      "AS",
      "SELECT",
      "JOIN / FILTER"
    ],
    "hint": "`WITH CategorySales AS (SELECT ...)` 구문으로 먼저 요약 테이블을 정의한 후 메인 쿼리에서 활용하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_struct_field",
    "title": "중첩 데이터(Struct) 접근: 점 표기법",
    "body": "address 컬럼이 구조체(Struct: city, zipcode)로 되어 있습니다. 이 중 'city' 정보만 추출하여 별도 컬럼으로 만드세요. Parquet/Avro 데이터 소스에서 흔한 패턴입니다.",
    "schema": "users(name STRING, address STRUCT<city:STRING, zipcode:INT>)",
    "sample_rows": [
      "Alice | {city: 'Seoul', zipcode: 04524}",
      "Bob | {city: 'Busan', zipcode: 40210}"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "col('col.field')",
      "select"
    ],
    "hint": "`F.col('address.city')`와 같이 점(dot) 표기법을 사용하여 구조체 내부 필드에 접근할 수 있습니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_salting_skew",
    "title": "데이터 편향(Skew) 해결: Salting 기법",
    "body": "특정 키(key='A')에 데이터가 90% 몰려있어 조인 시 해당 태스크만 멈춰있습니다(Straggler). 조인 키에 임의의 난수(Salt)를 추가하여 데이터를 강제로 분산시킨 후 조인하는 전략을 구상해 보세요.",
    "schema": "df_skew(key STRING, value INT)",
    "sample_rows": [
      "A | 1",
      "A | 2",
      "... (A repeats 1M times)",
      "B | 1"
    ],
    "difficulty": "Lv5 심화",
    "kind": "python",
    "expected": [
      "withColumn",
      "rand",
      "concat",
      "join"
    ],
    "hint": "1. `F.concat(col('key'), F.lit('_'), F.rand())`로 키를 쪼갭니다. \n2. 상대 테이블은 해당 개수만큼 복제(explode)하여 조인 조건을 맞춥니다.",
    "problem_type": "코딩"
  }
]