[
  {
    "pid": "python_basic_string_quotes",
    "title": "Python 문자열 표기법",
    "body": "users 데이터프레임에서 이름(name)이 'Alice'인 사람을 찾고 싶습니다. Python 코드 내에서 문자열 값을 표현할 때 사용할 수 있는 따옴표의 종류를 생각하며 filter 함수를 완성해보세요.",
    "schema": "users(name STRING, age INT)",
    "sample_rows": [
      "Alice | 25",
      "Bob | 30"
    ],
    "difficulty": "Lv0 기초",
    "kind": "python",
    "expected": [
      "filter",
      "col"
    ],
    "hint": "Python에서는 작은따옴표(')와 큰따옴표(\") 중 무엇을 써도 상관없습니다. 시작과 끝만 일치시켜주세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_basic_string_literal",
    "title": "SQL 문자열 값 표기 규칙",
    "body": "SQL을 사용하여 name이 'Alice'인 유저를 조회하세요. SQL 문법에서 데이터 값(리터럴)을 감쌀 때 반드시 사용해야 하는 따옴표가 무엇인지 생각해보세요.",
    "schema": "users(name TEXT, age INT)",
    "sample_rows": [
      "Alice | 25",
      "Bob | 30"
    ],
    "difficulty": "Lv0 기초",
    "kind": "sql",
    "expected": [
      "WHERE",
      "name =",
      "'Alice'"
    ],
    "hint": "SQL에서 문자열 값은 반드시 **작은따옴표(')**를 사용해야 합니다. 큰따옴표(\")는 컬럼명 등을 감쌀 때 쓰입니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_import_functions",
    "title": "PySpark 함수 모듈 불러오기",
    "body": "PySpark의 다양한 함수(col, lit, sum 등)를 사용하기 위해 `pyspark.sql.functions` 모듈을 import 하려고 합니다. 코드를 간결하게 쓰기 위해 통상적으로 사용하는 별칭(Alias)을 사용하여 import 문을 작성하세요.",
    "schema": "N/A (코드 설정 문제)",
    "sample_rows": [],
    "difficulty": "Lv0 기초",
    "kind": "python",
    "expected": [
      "import",
      "as F"
    ],
    "hint": "`import ... as ...` 구문을 사용합니다. 보통 `F`라는 약어를 많이 사용합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_filter_null",
    "title": "PySpark 결측치(NULL) 필터링",
    "body": "users 데이터프레임에서 `email` 컬럼이 비어있는(NULL) 사용자만 조회하려고 합니다. Python의 `None`과 비교 연산자(`==`)를 사용하는 것은 올바르지 않습니다. PySpark 전용 메서드를 사용하세요.",
    "schema": "users(name STRING, email STRING)",
    "sample_rows": [
      "Alice | alice@test.com",
      "Bob | NULL",
      "Charlie | charlie@test.com"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "filter",
      "isNull()"
    ],
    "hint": "컬럼 객체 뒤에 `.isNull()` 메서드를 붙여서 확인해야 합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_filter_is_null",
    "title": "SQL 결측치(NULL) 조회 조건",
    "body": "SQL에서 email 정보가 없는(NULL) 사용자를 조회하세요. 일반적인 등호(`=`) 연산자로는 NULL 값을 찾아낼 수 없습니다. 올바른 SQL 조건식을 사용하세요.",
    "schema": "users(name TEXT, email TEXT)",
    "sample_rows": [
      "Alice | alice@test.com",
      "Bob | NULL"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "WHERE",
      "IS NULL"
    ],
    "hint": "`컬럼명 IS NULL` 구문을 사용해야 합니다. (`= NULL`은 동작하지 않습니다.)",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_f_string_basic",
    "title": "변수를 활용한 문자열 조건 생성",
    "body": "외부 변수 `target_age = 30`이 주어졌을 때, 이를 활용하여 \"age >= 30\" 형태의 필터 조건 문자열을 만드세요. Python의 문자열 포맷팅 기능 중 하나를 사용하여 변수 값을 문자열 안에 삽입해야 합니다.",
    "schema": "users(name STRING, age INT)",
    "sample_rows": [
      "Alice | 25",
      "Bob | 35"
    ],
    "difficulty": "Lv0 기초",
    "kind": "python",
    "expected": [
      "filter",
      "f\"age >= {target_age}\""
    ],
    "hint": "따옴표 앞에 `f`를 붙이고, 변수를 중괄호 `{}` 안에 넣는 **f-string** 문법을 사용해보세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_basic_in_operator",
    "title": "다중 값 일치 검색",
    "body": "products 테이블에서 category가 'Fruit'이거나 'Vegetable'인 상품을 조회하세요. `OR` 연산자를 반복해서 사용하는 대신, 목록 중 하나와 일치하는지 확인하는 연산자를 사용하세요.",
    "schema": "products(name TEXT, category TEXT)",
    "sample_rows": [
      "Apple | Fruit",
      "Beef | Meat",
      "Carrot | Vegetable"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "WHERE",
      "IN"
    ],
    "hint": "`컬럼명 IN ('값1', '값2')` 형태의 문법을 사용하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_col_object",
    "title": "PySpark 컬럼 연산 방식",
    "body": "`age` 컬럼의 값에 1을 더한 결과를 `age_plus_1`이라는 새로운 컬럼으로 만드세요. 문자열 \"age\"에 숫자를 더할 수 없으므로, 이를 컬럼 객체로 변환한 뒤 연산해야 합니다.",
    "schema": "users(name STRING, age INT)",
    "sample_rows": [
      "Alice | 20"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "withColumn",
      "F.col",
      "+ 1"
    ],
    "hint": "`F.col(\"컬럼명\")` 함수를 사용하여 문자열을 컬럼 객체로 감싸주세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_alias_column",
    "title": "조회 결과 컬럼명 변경",
    "body": "price 컬럼에 0.9를 곱한 값을 조회하되, 결과 컬럼의 이름을 `discounted_price`로 지정하여 출력하세요.",
    "schema": "products(name TEXT, price DECIMAL)",
    "sample_rows": [
      "Mouse | 10000"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "SELECT",
      "*",
      "AS"
    ],
    "hint": "계산식 뒤에 `AS 새로운이름`을 붙여줍니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_boolean_operators",
    "title": "PySpark 다중 조건 결합",
    "body": "age가 20 이상이고(AND), 30 미만인 사람을 필터링하세요. Python의 기본 키워드(`and`, `or`) 대신 PySpark 및 Pandas에서 사용하는 전용 비트 연산자를 사용해야 하며, 연산 순서에 주의해야 합니다.",
    "schema": "users(name STRING, age INT)",
    "sample_rows": [
      "Alice | 25",
      "Bob | 35"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "filter",
      "&",
      "( )"
    ],
    "hint": "각 조건을 괄호 `()`로 감싸고, `&` (AND) 또는 `|` (OR) 연산자를 사용하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_operator_not_equal",
    "title": "SQL: 특정 조건 제외하기",
    "body": "customers 테이블에서 status가 'inactive'가 **아닌** 고객들만 조회하고 싶습니다. '같다'는 `=`인데, '다르다(Not Equal)'는 어떤 기호를 써야 할까요?",
    "schema": "customers(id INT, status TEXT)",
    "sample_rows": [
      "1 | active",
      "2 | inactive",
      "3 | pending"
    ],
    "difficulty": "Lv0 기초",
    "kind": "sql",
    "expected": [
      "WHERE",
      "<>",
      "!="
    ],
    "hint": "표준 SQL에서는 `<>`를 많이 쓰지만, `!=`도 대부분 지원합니다. 둘 중 하나를 사용해보세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_operator_not_equal",
    "title": "PySpark: 특정 조건 제외하기",
    "body": "users 데이터프레임에서 age가 20이 **아닌** 사람만 남기고 싶습니다. Python의 비교 연산자를 사용하여 필터링하세요.",
    "schema": "users(name STRING, age INT)",
    "sample_rows": [
      "Alice | 20",
      "Bob | 25"
    ],
    "difficulty": "Lv0 기초",
    "kind": "python",
    "expected": [
      "filter",
      "!="
    ],
    "hint": "같다는 `==`이고, 다르다는 `!=` 입니다. `F.col('age') != 20` 형태로 작성합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_rename_column",
    "title": "PySpark: 컬럼 이름 변경",
    "body": "데이터프레임의 `user_name`이라는 컬럼 이름이 너무 길어서 `name`으로 단순하게 바꾸고 싶습니다. 컬럼의 **이름만** 변경하는 전용 메서드를 사용하세요.",
    "schema": "users(user_name STRING, age INT)",
    "sample_rows": [
      "Alice | 20"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "withColumnRenamed"
    ],
    "hint": "`df.withColumnRenamed('기존이름', '새이름')` 메서드를 사용합니다. `alias`는 select 할 때 쓰고, 이 메서드는 DF 자체를 변환할 때 씁니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_count_null_handling",
    "title": "SQL: 데이터 개수 세기와 NULL",
    "body": "orders 테이블의 전체 행 개수가 아니라, `customer_id`가 **비어있지 않은(NULL이 아닌)** 행의 개수만 세고 싶습니다. 괄호 안에 무엇을 넣어야 할까요?",
    "schema": "orders(id INT, customer_id INT)",
    "sample_rows": [
      "1 | 101",
      "2 | NULL",
      "3 | 102"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "COUNT",
      "(column_name)"
    ],
    "hint": "`COUNT(*)`는 NULL 포함 전체를 세고, `COUNT(컬럼명)`은 해당 컬럼의 NULL을 제외하고 셉니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_drop_duplicates_simple",
    "title": "PySpark: 중복 데이터 제거",
    "body": "로그 데이터인 `logs` 데이터프레임에서 완전히 동일한 중복 행들을 제거하고 유니크한 행만 남기려고 합니다. 가장 직관적인 메서드를 사용하세요.",
    "schema": "logs(time STRING, user STRING)",
    "sample_rows": [
      "10:00 | Alice",
      "10:00 | Alice",
      "10:01 | Bob"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "dropDuplicates",
      "distinct"
    ],
    "hint": "`df.dropDuplicates()` 또는 `df.distinct()`를 사용합니다. 실무에서는 특정 컬럼 기준 중복 제거가 가능한 `dropDuplicates()`를 더 자주 씁니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_between_operator",
    "title": "SQL: 범위 검색 단순화",
    "body": "products 테이블에서 가격(price)이 1000 이상이고 2000 이하인 상품을 찾고 싶습니다. `>=`와 `<=`를 `AND`로 연결해도 되지만, 더 읽기 쉬운 전용 연산자가 있습니다.",
    "schema": "products(name TEXT, price INT)",
    "sample_rows": [
      "A | 500",
      "B | 1500",
      "C | 2500"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "WHERE",
      "BETWEEN",
      "AND"
    ],
    "hint": "`price BETWEEN 1000 AND 2000` 구문을 사용하세요. (시작과 끝 값을 포함합니다)",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_sort_descending",
    "title": "PySpark: 내림차순 정렬",
    "body": "scores 데이터프레임을 점수(score)가 높은 순서(내림차순)로 정렬하고 싶습니다. 단순히 `orderBy`만 쓰면 오름차순이 됩니다. 내림차순을 위해 필요한 함수는 무엇일까요?",
    "schema": "scores(name STRING, score INT)",
    "sample_rows": [
      "Alice | 50",
      "Bob | 100"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "orderBy",
      "desc"
    ],
    "hint": "`F.col('score').desc()` 함수를 사용해야 합니다. (`F.desc('score')`도 가능)",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_like_suffix",
    "title": "SQL: 끝나는 문자열 찾기",
    "body": "users 테이블에서 이메일이 '@gmail.com'으로 **끝나는** 모든 사용자를 조회하세요. 문자열 패턴 매칭 연산자를 사용해야 합니다.",
    "schema": "users(name TEXT, email TEXT)",
    "sample_rows": [
      "A | a@naver.com",
      "B | b@gmail.com"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "WHERE",
      "LIKE",
      "%"
    ],
    "hint": "`email LIKE '%@gmail.com'`을 사용하세요. `%`는 '0개 이상의 아무 문자'를 뜻합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_startswith",
    "title": "PySpark: 시작하는 문자열 찾기",
    "body": "products 데이터프레임에서 상품명(name)이 'New'로 **시작하는** 상품만 필터링하세요. SQL의 LIKE보다 더 직관적인 Python 스타일의 문자열 메서드를 사용할 수 있습니다.",
    "schema": "products(name STRING, price INT)",
    "sample_rows": [
      "New Laptop | 100",
      "Old Mouse | 10"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "filter",
      "startswith"
    ],
    "hint": "`F.col('name').startswith('New')` 메서드를 사용하세요. 코드가 훨씬 읽기 좋아집니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_select_math",
    "title": "SQL: 계산 결과 조회",
    "body": "employees 테이블에서 연봉(salary)을 12로 나눈 'monthly_salary'를 조회하고 싶습니다. SELECT 문 안에서 바로 나눗셈 연산을 수행하세요.",
    "schema": "employees(name TEXT, salary INT)",
    "sample_rows": [
      "Alice | 12000"
    ],
    "difficulty": "Lv0 기초",
    "kind": "sql",
    "expected": [
      "SELECT",
      "/",
      "AS"
    ],
    "hint": "`salary / 12 AS monthly_salary` 처럼 작성하면 됩니다. SQL은 SELECT 절에서 사칙연산이 가능합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_select_subset",
    "title": "PySpark: 원하는 컬럼만 골라내기",
    "body": "데이터프레임에 컬럼이 100개쯤 되는데, 그 중에서 `id`와 `email` 두 개의 컬럼만 딱 뽑아서 새로운 데이터프레임을 만들고 싶습니다.",
    "schema": "users(id INT, email STRING, address STRING, phone STRING ...)",
    "sample_rows": [
      "1 | a@test.com | Seoul | 010..."
    ],
    "difficulty": "Lv0 기초",
    "kind": "python",
    "expected": [
      "select"
    ],
    "hint": "`df.select('id', 'email')`을 사용하세요. 분석의 첫 단계는 필요한 데이터만 남기는 것입니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_cast_string_to_int",
    "title": "PySpark: 문자열을 숫자로 변환",
    "body": "`age` 컬럼이 숫자가 아닌 문자열(\"25\")로 저장되어 있어서 산술 연산이 안 됩니다. 이를 숫자(Integer) 타입으로 변환하세요.",
    "schema": "users(name STRING, age STRING)",
    "sample_rows": [
      "Alice | \"25\""
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "withColumn",
      "cast"
    ],
    "hint": "`F.col('age').cast('int')` 또는 `.cast('integer')`를 사용합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_coalesce_basic",
    "title": "SQL: 빈 값(NULL)을 다른 값으로 대체하기",
    "body": "customers 테이블에서 전화번호(phone)를 조회하고 싶습니다. 그런데 전화번호가 NULL인 경우에는 '미입력'이라고 출력하고 싶습니다. IF문을 쓰지 않고도 이를 처리하는 아주 유용한 표준 함수가 있습니다.",
    "schema": "customers(name TEXT, phone TEXT)",
    "sample_rows": [
      "Alice | 010-1234-5678",
      "Bob | NULL"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "SELECT",
      "COALESCE"
    ],
    "hint": "`COALESCE(컬럼명, '대체값')` 함수를 사용하세요. NULL이 아니면 원래 값을, NULL이면 뒤의 값을 반환합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_when_otherwise",
    "title": "PySpark: 조건에 따라 다른 값 넣기 (if-else)",
    "body": "products 데이터프레임에서 `price`가 10000 이상이면 'High', 그렇지 않으면 'Low'라는 값을 가진 `price_level` 컬럼을 새로 만들고 싶습니다. Python의 `if-else` 문법 대신 PySpark 전용 함수를 사용해야 합니다.",
    "schema": "products(name STRING, price INT)",
    "sample_rows": [
      "Mouse | 5000",
      "Keyboard | 15000"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "withColumn",
      "when",
      "otherwise"
    ],
    "hint": "`F.when(조건, 값).otherwise(값)` 형태를 사용합니다. 아주 자주 쓰이는 패턴입니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_literal_column",
    "title": "PySpark: 모든 행에 똑같은 값(상수) 넣기",
    "body": "users 데이터프레임에 `is_active`라는 컬럼을 추가하고, 모든 유저에게 `True` 값을 주고 싶습니다. `withColumn('is_active', True)`라고 쓰면 에러가 납니다. 값을 컬럼 객체로 만들어주는 함수가 필요합니다.",
    "schema": "users(name STRING)",
    "sample_rows": [
      "Alice",
      "Bob"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "withColumn",
      "lit"
    ],
    "hint": "단순한 값(숫자, 문자, 불린)을 컬럼으로 넣을 때는 반드시 `F.lit(값)`으로 감싸야 합니다. (Literal의 줄임말)",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_logic_precedence",
    "title": "SQL: AND와 OR가 섞여 있을 때",
    "body": "products 테이블에서 (카테고리가 'Electronics'이거나 'Computer')이고, 동시에 (가격이 10000 이상)인 상품을 조회하세요. 괄호 없이 그냥 나열하면 의도와 다른 결과가 나옵니다.",
    "schema": "products(name TEXT, category TEXT, price INT)",
    "sample_rows": [
      "Mouse | Electronics | 5000",
      "Monitor | Computer | 20000"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "WHERE",
      "( ... OR ... )",
      "AND"
    ],
    "hint": "AND는 OR보다 우선순위가 높습니다. 따라서 OR 조건을 먼저 묶어주려면 반드시 **괄호 `()`**를 사용해야 합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_print_schema",
    "title": "PySpark: 데이터 구조(타입) 확인하기",
    "body": "데이터를 로드했는데 `age`가 숫자인지 문자인지, `reg_date`가 날짜인지 문자인지 헷갈립니다. 데이터프레임의 컬럼 이름과 데이터 타입을 트리 형태로 예쁘게 출력해서 확인하는 메서드는 무엇일까요?",
    "schema": "N/A (데이터 확인 문제)",
    "sample_rows": [],
    "difficulty": "Lv0 기초",
    "kind": "python",
    "expected": [
      "printSchema()"
    ],
    "hint": "`df.printSchema()`를 실행하면 스키마 정보를 눈으로 확인할 수 있습니다. 디버깅할 때 필수입니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_min_max_basic",
    "title": "SQL: 최솟값과 최댓값 구하기",
    "body": "employees 테이블에서 가장 높은 연봉(salary)과 가장 낮은 연봉을 알고 싶습니다. 두 개의 집계 함수를 사용하여 한 번에 조회하세요.",
    "schema": "employees(name TEXT, salary INT)",
    "sample_rows": [
      "Alice | 5000",
      "Bob | 9000"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "SELECT",
      "MAX",
      "MIN"
    ],
    "hint": "`MAX(salary)`, `MIN(salary)`를 사용합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_show_action",
    "title": "PySpark: 데이터 눈으로 확인하기",
    "body": "열심히 필터링하고 컬럼을 추가했습니다. 이제 결과 데이터의 상위 20줄을 화면에 출력해서 제대로 처리되었는지 확인하고 싶습니다. 가장 많이 쓰는 'Action' 메서드는 무엇일까요?",
    "schema": "result_df(name STRING, age INT)",
    "sample_rows": [],
    "difficulty": "Lv0 기초",
    "kind": "python",
    "expected": [
      "show()"
    ],
    "hint": "`df.show()`를 사용합니다. 괄호 안에 숫자를 넣어 `df.show(5)` 처럼 줄 수를 지정할 수도 있습니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_date_string_format",
    "title": "SQL: 날짜 형식의 문자열 검색",
    "body": "orders 테이블에서 `order_date`가 '2024-01-01' 이후인 주문을 조회하고 싶습니다. 날짜 컬럼과 문자열을 비교할 때, 날짜 포맷(형식)을 어떻게 맞춰야 할까요?",
    "schema": "orders(id INT, order_date DATE)",
    "sample_rows": [
      "1 | 2023-12-31",
      "2 | 2024-01-02"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "WHERE",
      ">",
      "'YYYY-MM-DD'"
    ],
    "hint": "SQL 표준 날짜 포맷은 **'YYYY-MM-DD'**입니다. `order_date > '2024-01-01'` 처럼 작은따옴표로 감싸서 비교하면 자동으로 날짜로 인식합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_fill_null",
    "title": "PySpark: 결측치(NULL) 일괄 채우기",
    "body": "users 데이터프레임에 `age`가 비어있는(NULL) 행들이 있습니다. 비어있는 나이를 모두 0으로 채워넣고 싶습니다. `fillna` (또는 `na.fill`) 함수를 사용해보세요.",
    "schema": "users(name STRING, age INT)",
    "sample_rows": [
      "Alice | NULL",
      "Bob | 25"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "fillna",
      "na.fill"
    ],
    "hint": "`df.fillna(0, subset=['age'])` 형태로 사용합니다. 특정 컬럼을 지정하지 않으면 호환되는 모든 컬럼을 채워버리니 주의하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_distinct_count",
    "title": "SQL: 중복을 제외한 수 세기",
    "body": "access_logs 테이블에는 유저가 접속할 때마다 로그가 쌓입니다. 총 접속 횟수가 아니라, 접속한 **유저가 몇 명인지(고유 방문자 수)** 알고 싶다면 `COUNT` 안에 무엇을 추가해야 할까요?",
    "schema": "access_logs(log_id INT, user_id INT)",
    "sample_rows": [
      "1 | 100",
      "2 | 100",
      "3 | 200"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "COUNT",
      "DISTINCT"
    ],
    "hint": "`COUNT(DISTINCT user_id)`를 사용합니다. 중복을 제거한 뒤 개수를 셉니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_chaining",
    "title": "PySpark: 메서드 체이닝 (점 찍어 연결하기)",
    "body": "PySpark의 큰 장점은 코드를 한 줄씩 끊어 쓰지 않고 연결할 수 있다는 점입니다. (1) `age`가 20 이상인 사람을 필터링하고 (2) `name` 컬럼만 선택하는 과정을 **점(.)**으로 연결해서 한 번에 작성해보세요.",
    "schema": "users(name STRING, age INT, city STRING)",
    "sample_rows": [
      "Alice | 25 | Seoul",
      "Bob | 15 | Busan"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "df.filter(...).select(...)"
    ],
    "hint": "`df.filter(조건).select(컬럼)` 처럼 메서드 뒤에 바로 점을 찍어 다음 메서드를 연결하는 방식을 '체이닝'이라고 합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_concat_strings",
    "title": "SQL: 문자열 합치기",
    "body": "first_name이 'Gildong', last_name이 'Hong'일 때, 이를 합쳐서 'Gildong Hong' (중간에 공백 포함)이라는 `full_name`을 만들고 싶습니다. 문자열 연결 연산자나 함수를 사용하세요.",
    "schema": "users(first_name TEXT, last_name TEXT)",
    "sample_rows": [
      "Gildong | Hong"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "CONCAT",
      "||"
    ],
    "hint": "데이터베이스마다 다르지만 표준적으로 `CONCAT(first_name, ' ', last_name)`을 쓰거나, `||` 기호를 사용합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_select_distinct_category",
    "title": "중복 제거와 카디널리티 확인",
    "body": "products 테이블에서 현재 판매 중인 상품들의 카테고리(category) 목록을 조회하세요. 동일한 카테고리가 여러 번 출력되지 않아야 하며, 결과는 알파벳 순으로 정렬하세요. 실무에서는 컬럼의 고유 값 개수(Cardinality)를 파악할 때 자주 사용됩니다.",
    "schema": "products(product_id INT, category TEXT, price DECIMAL)",
    "sample_rows": [
      "1 | Electronics | 120000",
      "2 | Electronics | 35000",
      "3 | Books | 15000",
      "4 | Beauty | 25000"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "SELECT DISTINCT",
      "ORDER BY"
    ],
    "hint": "`SELECT DISTINCT category`를 사용하여 중복을 제거한 후 정렬하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_filter_literal",
    "title": "상수 컬럼 추가와 리터럴 처리",
    "body": "users 데이터프레임에 'country'라는 새로운 컬럼을 추가하고, 모든 행에 'Korea'라는 값을 입력하세요. PySpark에서는 문자열 값을 직접 컬럼으로 다룰 때 함수 사용이 필요합니다.",
    "schema": "users(user_id INT, name STRING)",
    "sample_rows": [
      "1 | Alice",
      "2 | Bob"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "withColumn",
      "F.lit"
    ],
    "hint": "단순 문자열을 할당하면 에러가 납니다. `df.withColumn('country', F.lit('Korea'))`를 사용하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_where_null_check",
    "title": "NULL 값과 빈 문자열의 차이",
    "body": "customers 테이블에서 이메일(email) 정보가 없는 고객을 조회하세요. 데이터베이스에 따라 '정보 없음'이 NULL로 저장되기도 하고, 빈 문자열('')로 저장되기도 하므로 두 경우를 모두 고려해야 합니다.",
    "schema": "customers(id INT, name TEXT, email TEXT)",
    "sample_rows": [
      "1 | Alice | alice@example.com",
      "2 | Bob | NULL",
      "3 | Charlie | ",
      "4 | David | david@example.com"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "WHERE",
      "IS NULL",
      "OR",
      "=''"
    ],
    "hint": "`WHERE email IS NULL OR email = ''` 조건을 사용하세요. `email = NULL`은 동작하지 않습니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_cast_type",
    "title": "데이터 타입 변환과 스키마 불일치",
    "body": "sales 데이터의 price 컬럼이 실수로 문자열(STRING) 타입으로 적재되었습니다. 이를 정수형(INTEGER)으로 변환한 뒤, 가격이 10000원 이상인 데이터만 필터링하세요.",
    "schema": "sales(product_id INT, price STRING)",
    "sample_rows": [
      "101 | '5000'",
      "102 | '15000'",
      "103 | '900'"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "withColumn",
      "cast",
      "filter"
    ],
    "hint": "`df.withColumn('price', F.col('price').cast('int'))`로 타입을 변경한 후 필터링을 수행하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_order_limit",
    "title": "상위 N개 추출: 정렬과 제한",
    "body": "orders 테이블에서 가장 최근에 주문된 5개의 주문 기록을 조회하세요. 전체 데이터를 가져온 후 애플리케이션에서 자르는 것보다, DB단에서 필요한 데이터만 가져오는 것이 네트워크 비용 절감에 유리합니다.",
    "schema": "orders(order_id INT, amount DECIMAL, order_date TIMESTAMP)",
    "sample_rows": [
      "1 | 50000 | 2024-03-01 10:00:00",
      "2 | 10000 | 2024-03-01 10:05:00",
      "3 | 30000 | 2024-03-01 09:00:00"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "ORDER BY DESC",
      "LIMIT"
    ],
    "hint": "`ORDER BY order_date DESC LIMIT 5` 구문을 사용하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_select_alias",
    "title": "컬럼 선택과 이름 변경(Alias)",
    "body": "복잡한 컬럼명(product_manufacturing_cost)을 분석하기 쉽게 `cost`로 이름을 변경하여 선택하고, `product_id`와 함께 출력하세요. 긴 컬럼명은 코드를 읽기 어렵게 만듭니다.",
    "schema": "products(product_id INT, product_manufacturing_cost DOUBLE)",
    "sample_rows": [
      "1 | 15.50",
      "2 | 20.00"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "select",
      "alias"
    ],
    "hint": "`df.select(F.col('product_id'), F.col('product_manufacturing_cost').alias('cost'))`를 사용하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_like_wildcard",
    "title": "특정 패턴 검색: 인덱스와 Like",
    "body": "users 테이블에서 이름이 'J'로 시작하는 모든 사용자를 조회하세요. 문자열 패턴 검색 시 와일드카드(%)의 위치에 따라 인덱스 활용 여부가 달라질 수 있습니다.",
    "schema": "users(user_id INT, name TEXT)",
    "sample_rows": [
      "1 | James",
      "2 | Alice",
      "3 | John",
      "4 | Robert"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "WHERE",
      "LIKE"
    ],
    "hint": "`WHERE name LIKE 'J%'`를 사용하세요. `%J%`와 달리 접두사 검색은 인덱스를 탈 가능성이 높습니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_drop_columns",
    "title": "불필요한 컬럼 삭제: 메모리 최적화",
    "body": "분석에 필요 없는 민감 정보인 `password`와 `ssn` 컬럼을 데이터프레임에서 제거하세요. 불필요한 데이터를 계속 들고 다니는 것은 메모리 낭비의 주원인입니다.",
    "schema": "users(user_id INT, name STRING, password STRING, ssn STRING)",
    "sample_rows": [
      "1 | Alice | 1234 | 111-222",
      "2 | Bob | abcd | 333-444"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "drop"
    ],
    "hint": "`df.drop('password', 'ssn')`과 같이 여러 컬럼을 한 번에 삭제할 수 있습니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_basic_math",
    "title": "파생 변수 생성: 산술 연산",
    "body": "products 테이블에서 가격(price)에 10% 부가세를 더한 `final_price`를 계산하여 조회하세요. 기존 컬럼을 조합하여 새로운 지표를 만드는 것은 데이터 분석의 기초입니다.",
    "schema": "products(product_id INT, price DECIMAL)",
    "sample_rows": [
      "1 | 10000",
      "2 | 20000"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "SELECT",
      "*",
      "AS"
    ],
    "hint": "`SELECT product_id, price, price * 1.1 AS final_price FROM products`와 같이 작성하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_isin_filter",
    "title": "다중 값 필터링: OR 조건의 단순화",
    "body": "products 데이터에서 카테고리가 'Electronics', 'Books', 'Garden' 중 하나인 상품만 필터링하세요. OR 조건을 여러 번 쓰는 것보다 포함 여부를 확인하는 것이 가독성에 좋습니다.",
    "schema": "products(product_id INT, category STRING)",
    "sample_rows": [
      "1 | Electronics",
      "2 | Beauty",
      "3 | Books",
      "4 | Toys"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "filter",
      "isin"
    ],
    "hint": "`df.filter(F.col('category').isin(['Electronics', 'Books', 'Garden']))`을 사용하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_boolean_logic",
    "title": "복합 논리 연산: AND와 괄호의 중요성",
    "body": "inventory 테이블에서 재고(stock)가 10개 미만이면서, 동시에 단종되지 않은(is_discontinued = 'N') 상품을 조회하세요. 긴급 발주가 필요한 목록입니다.",
    "schema": "inventory(product_id INT, stock INT, is_discontinued CHAR(1))",
    "sample_rows": [
      "1 | 5 | N",
      "2 | 0 | Y",
      "3 | 20 | N",
      "4 | 8 | Y"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "WHERE",
      "AND",
      "<"
    ],
    "hint": "`WHERE stock < 10 AND is_discontinued = 'N'` 조건을 사용하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_orderby_multiple",
    "title": "다중 정렬 기준: 우선순위 적용",
    "body": "employees 데이터를 부서(department)별로 오름차순 정렬하되, 같은 부서 내에서는 연봉(salary)이 높은 순서(내림차순)로 정렬하세요.",
    "schema": "employees(name STRING, department STRING, salary INT)",
    "sample_rows": [
      "Alice | Sales | 5000",
      "Bob | Sales | 6000",
      "Charlie | HR | 4000"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "orderBy",
      "asc",
      "desc"
    ],
    "hint": "`df.orderBy(F.col('department').asc(), F.col('salary').desc())`와 같이 정렬 기준을 순서대로 나열하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_select_filter",
    "title": "신규 고객 조회와 인덱스 활용",
    "body": "customers 테이블에서 가입일(signup_date)이 2024-01-01 이후인 고객을 조회하세요. signup_date 컬럼에 인덱스가 걸려있다고 가정할 때, 함수 사용을 자제하고 원본 컬럼을 그대로 사용하여 조회 성능을 최적화해야 합니다.",
    "schema": "customers(id INT, name TEXT, signup_date DATE)",
    "sample_rows": [
      "1 | Alice | 2024-02-10",
      "2 | Bob | 2023-12-30",
      "3 | Casey | 2024-03-01",
      "4 | David | NULL",
      "5 | Eve | 2024-01-01"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "SELECT",
      "WHERE",
      ">=",
      "ORDER BY"
    ],
    "hint": "컬럼을 가공(예: YEAR(signup_date))하면 인덱스를 타지 않을 수 있습니다. `signup_date >= '2024-01-01'`과 같이 원본 컬럼을 그대로 비교하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_aggregation_monthly",
    "title": "월별 매출 보고서: 날짜 절삭과 포맷팅",
    "body": "sales 테이블에서 월별 총 매출액을 계산하세요. 분석 툴(BI)에서 인식하기 쉽도록 문자열이 아닌 날짜 타입으로 월의 첫날(예: 2024-01-01)을 기준으로 그룹화하고, 출력 시에만 포맷을 변경하는 것이 좋습니다.",
    "schema": "sales(order_date DATE, amount DECIMAL)",
    "sample_rows": [
      "2024-01-05 | 120000",
      "2024-01-18 | 98000",
      "2024-02-02 | 150000",
      "NULL | 10000",
      "2024-02-28 | 5000"
    ],
    "difficulty": "Lv2 초급",
    "kind": "sql",
    "expected": [
      "GROUP BY",
      "SUM",
      "DATE_TRUNC",
      "TO_CHAR / DATE_FORMAT",
      "ORDER BY"
    ],
    "hint": "`DATE_TRUNC('month', order_date)`를 사용하여 월 단위로 절삭한 후 그룹화하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_join_customer_orders",
    "title": "고객별 주문 합계: 1:N 조인과 데이터 뻥튀기 주의",
    "body": "customers와 orders 테이블을 INNER JOIN하여 고객별 총 주문 금액을 구하세요. 실무에서는 조인 키가 중복될 경우 결과 행이 의도치 않게 늘어나는(Fan-out) 현상을 주의해야 합니다.",
    "schema": "customers(cust_id INT, name TEXT)\norders(order_id INT, cust_id INT, amount DECIMAL)",
    "sample_rows": [
      "customers: 1 | Alice",
      "customers: 2 | Bob",
      "orders: 10 | 1 | 50000",
      "orders: 11 | NULL | 30000",
      "orders: 12 | 2 | 45000",
      "orders: 13 | 1 | 20000"
    ],
    "difficulty": "Lv3 중급",
    "kind": "sql",
    "expected": [
      "JOIN",
      "ON",
      "GROUP BY",
      "SUM",
      "ORDER BY"
    ],
    "hint": "조인 조건은 `ON c.cust_id = o.cust_id`입니다. 집계 함수 `SUM(amount)`는 조인 결과 집합에 대해 수행됨을 유의하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_window_rank",
    "title": "카테고리별 인기 상품: 동점자 처리 전략",
    "body": "products 테이블에서 카테고리별 가장 비싼 상품을 조회하세요. 비즈니스 요건상 가격이 동일하다면 공동 1등으로 모두 보여줘야 합니다. 이런 경우 적절한 윈도우 함수를 선택해야 합니다.",
    "schema": "products(id INT, category TEXT, price DECIMAL)",
    "sample_rows": [
      "1 | electronics | 120000",
      "2 | electronics | 120000",
      "3 | electronics | 90000",
      "4 | books | 18000",
      "5 | books | 15000"
    ],
    "difficulty": "Lv4 고급",
    "kind": "sql",
    "expected": [
      "RANK() OVER",
      "PARTITION BY",
      "ORDER BY DESC",
      "Subquery / CTE"
    ],
    "hint": "`ROW_NUMBER()`는 강제로 순위를 매기지만, `RANK()`나 `DENSE_RANK()`는 동점을 처리합니다. 공동 1등을 모두 포함하려면 `RANK()`를 사용하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_running_total",
    "title": "누적 매출 집계: 윈도우 함수의 정렬 비용",
    "body": "sales 테이블에서 월별 매출과 함께 '누적 매출'을 계산하세요. 윈도우 함수 사용 시 정렬(ORDER BY)은 비용이 많이 드는 작업이므로, 인덱스가 있다면 이를 활용하는 것이 좋으나 여기서는 쿼리로 해결합니다.",
    "schema": "sales(month TEXT, amount DECIMAL)",
    "sample_rows": [
      "2024-01 | 200000",
      "2024-03 | 180000",
      "2024-02 | 230000",
      "2024-04 | 150000"
    ],
    "difficulty": "Lv5 심화",
    "kind": "sql",
    "expected": [
      "SUM() OVER",
      "ORDER BY",
      "ROWS BETWEEN"
    ],
    "hint": "`SUM(amount) OVER (ORDER BY month)` 구문을 사용하세요. 문자열 날짜 포맷이 'YYYY-MM'으로 통일되어 있다면 별도 캐스팅 없이 정렬 가능합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_basic_filter",
    "title": "활성 유저 필터링: Boolean 타입과 Predicate Pushdown",
    "body": "users DataFrame에서 활성 유저(active=True)이면서 마지막 로그인이 2024년 이후인 데이터를 필터링하세요. 파케이(Parquet) 등의 포맷을 사용할 때 필터 순서는 데이터 스캔량을 줄이는 데 영향을 줄 수 있습니다.",
    "schema": "users(id INT, active BOOLEAN, last_login TIMESTAMP)",
    "sample_rows": [
      "1 | true | 2024-02-03 10:00:00",
      "2 | false | NULL",
      "3 | true | 2023-12-10 15:30:00",
      "4 | true | 2024-01-05 09:20:00"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "filter / where",
      "col('active') == True",
      "col('last_login') > ..."
    ],
    "hint": "`df.filter((F.col('active') == True) & (F.col('last_login') >= F.lit('2024-01-01')))` 형태로 작성하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_join",
    "title": "주문 데이터 결합: 셔플링(Shuffling)과 성능",
    "body": "customers와 orders DataFrame을 INNER JOIN 하세요. 두 테이블 모두 크기가 클 경우, 조인 키를 기준으로 대규모 데이터 이동(Shuffle)이 발생하여 성능 저하의 원인이 됩니다.",
    "schema": "customers(cust_id INT, name STRING)\norders(order_id INT, cust_id INT, amount DOUBLE)",
    "sample_rows": [
      "Cust: 1 | Alice",
      "Cust: 2 | Bob",
      "Ord: 10 | 1 | 50000.0",
      "Ord: 11 | 3 | 30000.0",
      "Ord: 12 | 2 | 10000.0"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "join",
      "on='cust_id'",
      "how='inner'"
    ],
    "hint": "`orders.join(customers, on='cust_id', how='inner')`를 사용합니다. 키 컬럼의 이름이 같다면 `on` 파라미터를 단일 문자열로 전달하는 것이 깔끔합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_window",
    "title": "센서 데이터 이동 평균: 윈도우 정의와 파티셔닝",
    "body": "events 데이터에서 시간 순으로 정렬하여 직전 2개 데이터와 현재 데이터의 평균(Moving Average)을 구하세요. 단일 파티션으로 윈도우를 몰아넣으면(OOM 위험), 병렬 처리가 불가능하므로 적절한 파티션 키가 필요할 수 있습니다.",
    "schema": "events(device_id INT, time TIMESTAMP, value DOUBLE)",
    "sample_rows": [
      "1 | 10:00 | 10.0",
      "1 | 10:05 | 14.0",
      "1 | 10:10 | 12.0",
      "2 | 10:00 | 20.0"
    ],
    "difficulty": "Lv3 중급",
    "kind": "python",
    "expected": [
      "Window.partitionBy",
      "orderBy",
      "rowsBetween",
      "F.avg"
    ],
    "hint": "`Window.partitionBy('device_id').orderBy('time').rowsBetween(-2, 0)`을 정의하여 디바이스별로 계산되도록 하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_pivot",
    "title": "주간 상태 리포트: 피벗(Pivot)과 카디널리티",
    "body": "logs 데이터에서 각 주차(Week)별로 상태 코드(status)의 발생 횟수를 피벗하여 집계하세요. 피벗 대상 컬럼(status)의 고유 값(Cardinality)이 너무 많으면 컬럼 폭발이 일어날 수 있으니 주의해야 합니다.",
    "schema": "logs(timestamp TIMESTAMP, status STRING)",
    "sample_rows": [
      "2024-01-02 | success",
      "2024-01-03 | fail",
      "2024-01-08 | success",
      "2024-01-09 | pending"
    ],
    "difficulty": "Lv4 고급",
    "kind": "python",
    "expected": [
      "date_trunc",
      "groupBy",
      "pivot",
      "count",
      "fillna"
    ],
    "hint": "`df.groupBy(F.date_trunc('week', 'timestamp')).pivot('status').count()`를 사용하고, 결측치는 `.fillna(0)`으로 처리하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_sessionize",
    "title": "세션 ID 생성: Lag 함수를 이용한 복합 로직",
    "body": "유저별로 페이지 방문 기록이 있습니다. 이전 방문과 30분 이상 차이가 나면 새로운 세션으로 간주하여 고유 세션 ID를 부여하세요. 이는 Clickstream 데이터 분석의 핵심 로직입니다.",
    "schema": "pageviews(user_id INT, ts TIMESTAMP)",
    "sample_rows": [
      "1 | 10:00",
      "1 | 10:10",
      "1 | 11:00 (New Session)",
      "1 | 11:05"
    ],
    "difficulty": "Lv5 심화",
    "kind": "python",
    "expected": [
      "Window",
      "F.lag",
      "F.when",
      "F.sum().over"
    ],
    "hint": "1. `F.lag('ts')`로 이전 시간 확보 \n2. 시간 차이 > 30분이면 1, 아니면 0인 플래그 생성 \n3. 해당 플래그를 누적 합(`F.sum`)하여 세션 ID 생성.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_broadcast_join_large_small",
    "title": "브로드캐스트 조인 최적화: 셔플 제거",
    "body": "매우 큰 주문 테이블(df_large)과 작은 코드 테이블(df_small, 10MB 미만)을 조인할 때, 작은 테이블을 각 노드로 복사(Broadcast)하여 네트워크 비용(Shuffle)을 획기적으로 줄이세요.",
    "schema": "df_large(id INT, code_id INT, amount INT)\ndf_small(code_id INT, description STRING)",
    "sample_rows": [
      "Large: 1M rows...",
      "Small: 100 rows (1 | 'Discount', 2 | 'Regular')"
    ],
    "difficulty": "Lv3 중급",
    "kind": "python",
    "expected": [
      "F.broadcast",
      "join"
    ],
    "hint": "`df_large.join(F.broadcast(df_small), 'code_id')` 구문을 사용하여 스파크가 맵 사이드 조인(Map-side Join)을 수행하도록 유도하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_explode_array_to_rows",
    "title": "JSON 배열 처리: Explode와 데이터 정규화",
    "body": "한 유저가 구매한 상품들이 배열(Array) 형태로 저장되어 있습니다. 이를 분석하기 쉽게 상품당 한 행(Row)을 가지도록 펼쳐주세요(Normalize). 배열 크기가 클 경우 데이터 양이 급증할 수 있습니다.",
    "schema": "orders(user_id INT, items ARRAY<STRING>)",
    "sample_rows": [
      "1 | ['Apple', 'Banana']",
      "2 | ['Cherry']",
      "3 | []",
      "4 | NULL"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "F.explode",
      "withColumn / select"
    ],
    "hint": "`df.select('user_id', F.explode('items').alias('item'))`을 사용하세요. `explode_outer`를 쓰면 빈 배열도 보존할 수 있습니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_udf_string_length",
    "title": "사용자 정의 함수(UDF) 주의점: 직렬화 비용",
    "body": "name 컬럼의 길이를 구하세요. Python UDF는 JVM과 Python 프로세스 간 데이터 이동(직렬화/역직렬화) 비용이 크므로, 가능한 Spark 내장 함수를 사용하는 것이 성능에 유리합니다.",
    "schema": "users(name STRING)",
    "sample_rows": [
      "Alice",
      "Bob",
      "Christopher"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "F.length",
      "(Optional) F.udf"
    ],
    "hint": "단순 길이는 `F.length('name')` 내장 함수가 가장 빠릅니다. 굳이 UDF를 써야 한다면 `@F.udf(returnType=IntegerType())` 데코레이터를 사용하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_repartition_coalesce",
    "title": "파티션 제어: Coalesce vs Repartition",
    "body": "작업 후 파티션 개수가 100개인데 파일 크기가 너무 작습니다(Small File Issue). 셔플링을 최소화하면서 파티션 개수를 1개로 줄여 단일 파일로 저장하고 싶습니다.",
    "schema": "df(id INT, data STRING)",
    "sample_rows": [
      "Partition 1: 10 rows",
      "Partition 2: 5 rows",
      "..."
    ],
    "difficulty": "Lv3 중급",
    "kind": "python",
    "expected": [
      "coalesce(1)",
      "write"
    ],
    "hint": "`df.repartition(1)`은 전체 셔플을 유발합니다. 단순히 파티션을 합치는 것이라면 `df.coalesce(1)`이 훨씬 효율적입니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_cache_usage",
    "title": "중간 결과 재사용: Cache와 Persist 전략",
    "body": "복잡한 전처리를 거친 DataFrame을 사용하여 두 가지 다른 분석(액션)을 수행합니다. 전처리 과정이 두 번 실행되지 않도록 메모리에 캐싱하세요. 메모리가 부족할 경우 디스크 스필(Spill)이 발생할 수 있습니다.",
    "schema": "heavy_df(id INT, computed_value DOUBLE)",
    "sample_rows": [
      "1 | 0.123 (computed costly)",
      "2 | 0.456 (computed costly)"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "cache / persist",
      "count",
      "show"
    ],
    "hint": "`df.cache()` 호출 후 첫 번째 액션(`count()` 등)이 일어날 때 메모리에 적재됩니다. 작업이 끝나면 `df.unpersist()`로 해제하는 것이 좋습니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_null_cleaning_sales",
    "title": "더러운 데이터 정제: 유효성 검사",
    "body": "매출 금액이 음수이거나 NULL인 경우는 시스템 오류로 간주합니다. 유효한 매출(양수)과 2024년 데이터만 조회하세요. 현업 데이터는 항상 예상 범위를 벗어날 수 있음을 가정해야 합니다.",
    "schema": "sales(product_name TEXT, order_date DATE, amount DECIMAL)",
    "sample_rows": [
      "Laptop | 2024-01-05 | 150000",
      "Mouse | 2023-12-28 | NULL",
      "Keyboard | 2024-02-10 | -5000"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "WHERE",
      "AND",
      ">",
      "ORDER BY"
    ],
    "hint": "`WHERE amount > 0` 조건은 암시적으로 NULL을 배제합니다. 날짜 필터는 `order_date >= '2024-01-01'`을 사용하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_agg_approx_count",
    "title": "대용량 고유 유저 수: 근사 집계(HyperLogLog)",
    "body": "수십억 건의 로그에서 고유 방문자 수(Distinct Users)를 구해야 합니다. 정확한 `distinct count`는 메모리를 많이 사용하고 느립니다. 오차 허용 범위 내에서 빠르게 결과를 내는 근사 함수를 사용하세요.",
    "schema": "logs(user_id STRING, page_id STRING)",
    "sample_rows": [
      "uuid-1 | home",
      "uuid-2 | detail",
      "uuid-1 | cart"
    ],
    "difficulty": "Lv4 고급",
    "kind": "python",
    "expected": [
      "F.approx_count_distinct"
    ],
    "hint": "`F.countDistinct('user_id')` 대신 `F.approx_count_distinct('user_id', rsd=0.05)`를 사용하면 훨씬 빠른 속도로 근사치를 얻을 수 있습니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_cte_readability",
    "title": "복잡한 쿼리 구조화: CTE(Common Table Expression) 활용",
    "body": "카테고리별 매출 합계를 구하고, 그 중 평균 매출보다 높은 카테고리만 조회하세요. 서브쿼리를 중첩(Nested Subquery)하는 것보다 CTE를 사용하면 가독성과 유지보수성이 좋아집니다.",
    "schema": "sales(category TEXT, amount DECIMAL)",
    "sample_rows": [
      "A | 100",
      "B | 200",
      "A | 150",
      "C | 50"
    ],
    "difficulty": "Lv3 중급",
    "kind": "sql",
    "expected": [
      "WITH",
      "AS",
      "SELECT",
      "JOIN / FILTER"
    ],
    "hint": "`WITH CategorySales AS (SELECT ...)` 구문으로 먼저 요약 테이블을 정의한 후 메인 쿼리에서 활용하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_struct_field",
    "title": "중첩 데이터(Struct) 접근: 점 표기법",
    "body": "address 컬럼이 구조체(Struct: city, zipcode)로 되어 있습니다. 이 중 'city' 정보만 추출하여 별도 컬럼으로 만드세요. Parquet/Avro 데이터 소스에서 흔한 패턴입니다.",
    "schema": "users(name STRING, address STRUCT<city:STRING, zipcode:INT>)",
    "sample_rows": [
      "Alice | {city: 'Seoul', zipcode: 04524}",
      "Bob | {city: 'Busan', zipcode: 40210}"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "col('col.field')",
      "select"
    ],
    "hint": "`F.col('address.city')`와 같이 점(dot) 표기법을 사용하여 구조체 내부 필드에 접근할 수 있습니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_salting_skew",
    "title": "데이터 편향(Skew) 해결: Salting 기법",
    "body": "특정 키(key='A')에 데이터가 90% 몰려있어 조인 시 해당 태스크만 멈춰있습니다(Straggler). 조인 키에 임의의 난수(Salt)를 추가하여 데이터를 강제로 분산시킨 후 조인하는 전략을 구상해 보세요.",
    "schema": "df_skew(key STRING, value INT)",
    "sample_rows": [
      "A | 1",
      "A | 2",
      "... (A repeats 1M times)",
      "B | 1"
    ],
    "difficulty": "Lv5 심화",
    "kind": "python",
    "expected": [
      "withColumn",
      "rand",
      "concat",
      "join"
    ],
    "hint": "1. `F.concat(col('key'), F.lit('_'), F.rand())`로 키를 쪼갭니다. \n2. 상대 테이블은 해당 개수만큼 복제(explode)하여 조인 조건을 맞춥니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "python_list_comp_basic",
    "title": "리스트 컴프리헨션: 짝수만 제곱하기",
    "body": "숫자 리스트 `numbers`가 주어졌을 때, 짝수인 숫자만 골라서 그 값을 제곱한 새로운 리스트를 만드세요. `for` 루프를 쓰지 말고 리스트 컴프리헨션을 사용해야 합니다.",
    "schema": "numbers (List[int])",
    "sample_rows": [
      "[1, 2, 3, 4, 5] -> [4, 16]",
      "[10, 11, 12] -> [100, 144]"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "[x**2 for x in numbers if ...]"
    ],
    "hint": "`[식 for 변수 in 리스트 if 조건]` 형태를 기억하세요. `x % 2 == 0`이 짝수 조건입니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "python_list_comp_string",
    "title": "리스트 컴프리헨션: 글자 수 필터링 및 변환",
    "body": "문자열 리스트 `words`에서 길이가 5 이상인 단어만 골라, 모두 대문자로 변환된 리스트를 생성하세요.",
    "schema": "words (List[str])",
    "sample_rows": [
      "['apple', 'cat', 'banana'] -> ['APPLE', 'BANANA']"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "[w.upper() for w in words if ...]"
    ],
    "hint": "문자열 길이 확인은 `len(w)`, 대문자 변환은 `.upper()`를 사용합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "python_def_sum_rows",
    "title": "함수(def) & 행 다루기: 2차원 리스트 행별 합계",
    "body": "2차원 리스트(행렬)를 입력받아, 각 행(row)의 합계를 담은 리스트를 반환하는 함수 `row_sums(matrix)`를 작성하세요.",
    "schema": "matrix (List[List[int]])",
    "sample_rows": [
      "[[1, 2], [3, 4]] -> [3, 7]",
      "[[1, 1, 1], [2, 2, 2]] -> [3, 6]"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "def row_sums(matrix):",
      "return [sum(row) for row in matrix]"
    ],
    "hint": "`for row in matrix`로 각 행을 꺼낸 뒤, `sum(row)`를 이용하면 쉽게 구할 수 있습니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "python_array_transpose",
    "title": "배열 다루기: 행과 열 바꾸기 (Transpose)",
    "body": "2차원 리스트 `matrix`의 행과 열을 뒤바꾼 새로운 행렬을 만드세요. (예: 2x3 행렬 -> 3x2 행렬). 이중 루프를 사용해도 좋고, 리스트 컴프리헨션을 사용해도 좋습니다.",
    "schema": "matrix (List[List[int]])",
    "sample_rows": [
      "[[1, 2, 3], [4, 5, 6]] -> [[1, 4], [2, 5], [3, 6]]"
    ],
    "difficulty": "Lv3 중급",
    "kind": "python",
    "expected": [
      "[[row[i] for row in matrix] for i in range(len(matrix[0]))]"
    ],
    "hint": "외부 루프는 열의 개수(`range(len(matrix[0]))`)만큼 돌고, 내부에서 각 행의 `i`번째 요소를 가져옵니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "python_algo_factorial",
    "title": "알고리즘(Loop): 팩토리얼 계산",
    "body": "정수 `n`을 입력받아 `n!` (팩토리얼) 값을 반환하는 함수 `factorial(n)`을 작성하세요. 재귀함수가 아닌 `for` 또는 `while` 루프를 사용해 구현하세요.",
    "schema": "n (int)",
    "sample_rows": [
      "5 -> 120 (1*2*3*4*5)",
      "3 -> 6"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "result = 1",
      "for i in range(1, n+1):",
      "result *= i"
    ],
    "hint": "초기값을 1로 설정(`result = 1`)하고, 1부터 n까지 반복하며 계속 곱해줍니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "python_algo_count_dict",
    "title": "알고리즘 & 딕셔너리: 단어 빈도수 세기",
    "body": "단어들이 담긴 리스트 `words`를 입력받아, 각 단어가 몇 번 등장했는지 세어 `{단어: 횟수}` 형태의 딕셔너리로 반환하는 함수 `count_words(words)`를 작성하세요.",
    "schema": "words (List[str])",
    "sample_rows": [
      "['apple', 'banana', 'apple'] -> {'apple': 2, 'banana': 1}"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "counts = {}",
      "if word in counts:",
      "counts[word] += 1",
      "else: counts[word] = 1"
    ],
    "hint": "빈 딕셔너리 `{}`를 만들고, 리스트를 루프 돌면서 키가 있으면 +1, 없으면 1로 초기화합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "python_list_slice_rotate",
    "title": "배열 슬라이싱: 리스트 회전시키기",
    "body": "리스트 `nums`와 정수 `k`가 주어질 때, 리스트의 요소를 오른쪽으로 `k`번 회전시킨 결과를 반환하는 함수 `rotate_list(nums, k)`를 작성하세요. 슬라이싱 기능을 활용하면 루프 없이 한 줄로 가능합니다.",
    "schema": "nums (List[int]), k (int)",
    "sample_rows": [
      "nums=[1, 2, 3, 4, 5], k=2 -> [4, 5, 1, 2, 3]"
    ],
    "difficulty": "Lv3 중급",
    "kind": "python",
    "expected": [
      "def rotate_list(nums, k):",
      "k = k % len(nums)",
      "return nums[-k:] + nums[:-k]"
    ],
    "hint": "`nums[-k:]`는 뒤에서 k개, `nums[:-k]`는 앞의 나머지 부분입니다. 이 둘을 더하면 회전된 결과가 됩니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "python_algo_prime",
    "title": "알고리즘(Loop): 소수(Prime) 판별 함수",
    "body": "정수 `n`을 입력받아 소수이면 `True`, 아니면 `False`를 반환하는 함수 `is_prime(n)`을 작성하세요. 루프를 사용하여 2부터 `n-1`까지 나누어 떨어지는 수가 있는지 확인해야 합니다.",
    "schema": "n (int)",
    "sample_rows": [
      "7 -> True",
      "10 -> False"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "if n < 2: return False",
      "for i in range(2, int(n**0.5) + 1):",
      "if n % i == 0: return False"
    ],
    "hint": "반복문 도중 `n % i == 0`인 경우가 한 번이라도 발견되면 즉시 `return False`하고 종료합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "python_list_flatten",
    "title": "리스트 컴프리헨션: 2차원 리스트 평탄화(Flatten)",
    "body": "2차원 리스트(리스트 안의 리스트)를 1차원 리스트로 쭉 펼치는 함수 `flatten(matrix)`를 리스트 컴프리헨션을 사용해 작성하세요.",
    "schema": "matrix (List[List[int]])",
    "sample_rows": [
      "[[1, 2], [3, 4], [5]] -> [1, 2, 3, 4, 5]"
    ],
    "difficulty": "Lv3 중급",
    "kind": "python",
    "expected": [
      "[val for row in matrix for val in row]"
    ],
    "hint": "이중 `for`문 순서대로 적으면 됩니다. `for row in matrix`가 먼저 오고, 그 뒤에 `for val in row`가 옵니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "python_algo_max_index",
    "title": "배열 다루기: 최댓값과 그 위치 찾기",
    "body": "숫자 리스트 `nums`에서 가장 큰 값과 그 값의 인덱스(위치)를 튜플 `(max_val, index)` 형태로 반환하는 함수 `find_max_info(nums)`를 작성하세요. 내장함수 `max()`나 `index()`를 쓰지 않고 루프로 구현해보세요.",
    "schema": "nums (List[int])",
    "sample_rows": [
      "[10, 50, 30] -> (50, 1)"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "max_val = nums[0]",
      "idx = 0",
      "enumerate"
    ],
    "hint": "`for i, val in enumerate(nums):`를 사용하여 인덱스와 값을 동시에 얻고, 현재 최댓값보다 크면 갱신합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_python_list_comp_rename",
    "title": "List Comprehension: 모든 컬럼명 소문자로 변경하기",
    "body": "데이터프레임의 컬럼명이 대소문자가 섞여 있어('UserID', 'CreateDATE') 다루기 불편합니다. 파이썬의 리스트 컴프리헨션을 사용하여 모든 컬럼명을 소문자로 바꾼 뒤, `toDF` 메서드를 사용해 한 번에 적용하세요.",
    "schema": "df(UserID STRING, CreateDATE DATE, Amount INT)",
    "sample_rows": [
      "user1 | 2024-01-01 | 100"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "[c.lower() for c in df.columns]",
      "toDF(*...)"
    ],
    "hint": "`df.columns`는 파이썬 리스트를 반환합니다. 이를 리스트 컴프리헨션으로 가공하고, `df.toDF(*new_cols)`와 같이 언패킹(`*`)하여 전달하는 패턴은 국룰입니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_python_unpacking_select",
    "title": "Unpacking(*): 리스트에 담긴 컬럼들만 선택하기",
    "body": "분석에 필요한 컬럼 이름들이 파이썬 리스트 `target_cols = ['name', 'age']`에 담겨 있습니다. 이 리스트를 `select` 함수에 직접 전달하면 에러가 납니다. 리스트의 요소들을 풀어서(Unpacking) 전달하는 파이썬 문법을 사용하세요.",
    "schema": "df(name STRING, age INT, address STRING, phone STRING)",
    "sample_rows": [
      "Alice | 25 | Seoul | 010..."
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "select(*target_cols)"
    ],
    "hint": "함수 인자로 리스트를 바로 넣는 대신, `*리스트이름`을 쓰면 리스트 껍질을 벗겨서 알맹이만 `arg1, arg2, ...` 형태로 전달합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_python_udf_def_logic",
    "title": "Def & UDF: 복잡한 점수 계산 로직 적용",
    "body": "단순한 사칙연산이 아니라, 비즈니스 로직이 포함된 점수 계산이 필요합니다. 파이썬 함수 `calculate_score(purchase, visits)`를 정의하고(구매액/방문수 비율 계산, 0으로 나누기 방지 등), 이를 UDF로 변환하여 적용하세요.",
    "schema": "df(user_id INT, purchase INT, visits INT)",
    "sample_rows": [
      "1 | 10000 | 5 -> 2000.0",
      "2 | 0 | 0 -> 0.0"
    ],
    "difficulty": "Lv3 중급",
    "kind": "python",
    "expected": [
      "def calculate_score(p, v):",
      "if v == 0: return 0.0",
      "udf(calculate_score, DoubleType())"
    ],
    "hint": "Spark SQL만으로 복잡한 `if-else`나 예외처리를 하긴 어렵습니다. 이럴 때 파이썬 함수(`def`)를 짜고 `udf`로 감싸서 사용합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_python_lambda_udf",
    "title": "Lambda & UDF: 일회용 함수로 문자열 가공",
    "body": "email 컬럼에서 '@' 앞의 아이디만 추출하고 싶습니다. 별도로 `def`를 선언하기엔 너무 간단한 로직입니다. 파이썬의 익명 함수 `lambda`를 사용하여 한 줄짜리 UDF를 만들고 적용하세요.",
    "schema": "df(email STRING)",
    "sample_rows": [
      "alice@company.com -> alice"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "udf(lambda x: x.split('@')[0])"
    ],
    "hint": "`lambda 입력변수: 반환값` 형식을 사용합니다. `split` 같은 파이썬 문자열 메서드를 Spark 안에서 쓸 수 있게 해줍니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_python_dict_loop_rename",
    "title": "Dict Loop: 매핑 사전을 이용한 컬럼명 일괄 변경",
    "body": "시스템 컬럼명과 한글 컬럼명이 매핑된 딕셔너리 `col_map = {'nm': '이름', 'ag': '나이'}`가 있습니다. `for` 루프와 `withColumnRenamed`를 사용하여 딕셔너리에 정의된 대로 컬럼 이름을 모두 변경하세요.",
    "schema": "df(nm STRING, ag INT)",
    "sample_rows": [
      "Alice | 25"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "for old, new in col_map.items():",
      "df = df.withColumnRenamed(old, new)"
    ],
    "hint": "딕셔너리의 `.items()`를 순회하며 `df` 변수를 계속 덮어쓰는(update) 방식은 PySpark 코드 작성 시 흔한 패턴입니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_python_f_string_sql",
    "title": "f-string: 동적 SQL 쿼리 생성",
    "body": "테이블 이름 `table_name = 'sales_2024'`와 기준 금액 `min_amount = 1000`이 변수로 주어집니다. 파이썬의 f-string을 사용하여 `SELECT * FROM sales_2024 WHERE amount >= 1000` 형태의 쿼리 문자열을 생성하고 `spark.sql()`로 실행하세요.",
    "schema": "N/A (Spark SQL 실행)",
    "sample_rows": [],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "query = f\"SELECT * FROM {table_name} ...\"",
      "spark.sql(query)"
    ],
    "hint": "SQL 쿼리도 결국 문자열입니다. 파이썬 변수를 쿼리 중간에 꽂아 넣을 때 f-string이 가장 가독성이 좋습니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_python_list_comp_filter",
    "title": "List Comprehension: 특정 타입의 컬럼만 골라내기",
    "body": "데이터프레임 `df.dtypes`를 확인하면 `('col_name', 'string')` 형태의 튜플 리스트가 나옵니다. 리스트 컴프리헨션을 사용하여 데이터 타입이 'int'인 컬럼의 이름만 리스트로 추출하세요.",
    "schema": "df(name STRING, age INT, score INT)",
    "sample_rows": [
      "dtypes -> [('name', 'string'), ('age', 'int'), ('score', 'int')]"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "[c for c, t in df.dtypes if t == 'int']"
    ],
    "hint": "`for c, t in df.dtypes` 처럼 튜플을 언패킹하며 루프를 돌고, `if` 조건으로 필터링하는 전형적인 파이썬 패턴입니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_python_try_except",
    "title": "Try-Except: 파일 로드 시 예외 처리",
    "body": "데이터 파일 경로 `path`가 주어집니다. 파일을 읽어오되(`spark.read.csv`), 파일이 없거나 에러가 발생하면 \"파일을 찾을 수 없습니다\"를 출력하고 빈 데이터프레임을 생성하는 안전한 코드를 작성하세요.",
    "schema": "N/A",
    "sample_rows": [],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "try:",
      "spark.read.csv(path)",
      "except AnalysisException:",
      "print(...)"
    ],
    "hint": "데이터 파이프라인은 언제든 깨질 수 있습니다. `try-except` 블록으로 에러 상황을 우아하게(Graceful) 처리하는 것은 엔지니어의 기본 소양입니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_python_set_operations",
    "title": "Set 연산: 두 데이터프레임의 공통 컬럼 찾기",
    "body": "두 개의 데이터프레임 `df1`과 `df2`가 있습니다. 파이썬의 집합(Set) 자료형과 교집합 연산자(`&`)를 사용하여 두 데이터프레임에 공통으로 존재하는 컬럼명 리스트를 구하세요.",
    "schema": "df1(id, name, age), df2(id, address, phone)",
    "sample_rows": [
      "공통 컬럼 -> ['id']"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "set(df1.columns) & set(df2.columns)",
      "list(...)"
    ],
    "hint": "리스트를 `set()`으로 변환하면 교집합(`&`), 차집합(`-`), 합집합(`|`)을 매우 쉽게 구할 수 있습니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_python_assert",
    "title": "Assert: 데이터 검증(Sanity Check)",
    "body": "데이터 가공이 끝난 후, 결과 데이터프레임 `result_df`의 행 개수가 0이면 안 된다는 조건을 걸고 싶습니다. 파이썬의 `assert` 문을 사용하여 행 개수가 0일 때 에러를 발생시키는 코드를 작성하세요.",
    "schema": "result_df(...)",
    "sample_rows": [],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "count = result_df.count()",
      "assert count > 0, \"메시지\""
    ],
    "hint": "`assert 조건, 에러메시지` 형태입니다. 조건이 False면 프로그램이 즉시 중단되며 에러를 뿜습니다. 데이터 품질 체크에 필수적입니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_python_strip_columns",
    "title": "List Comp & String: 컬럼명 공백 제거",
    "body": "CSV를 읽었더니 컬럼명 앞뒤에 공백이 섞여 있습니다(예: ' user_id ', ' name'). 파이썬의 문자열 메서드 `.strip()`과 리스트 컴프리헨션을 사용하여 깔끔한 컬럼명 리스트를 만들고, `toDF`로 적용하세요.",
    "schema": "df(' user_id ', ' name')",
    "sample_rows": [
      "1 | Alice"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "[c.strip() for c in df.columns]",
      "toDF(*...)"
    ],
    "hint": "문자열 `s.strip()`은 양쪽 공백을 제거합니다. `[c.strip() for c in df.columns]` 패턴은 데이터 로드 직후 국룰입니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_python_zip_rename",
    "title": "Zip Loop: 두 리스트를 엮어서 컬럼명 변경",
    "body": "현재 컬럼명 리스트 `old_cols = ['a', 'b']`와 바꿀 한글명 리스트 `new_cols = ['이름', '나이']`가 있습니다. 파이썬의 `zip()`을 사용하여 두 리스트를 쌍으로 묶은 뒤, 루프를 돌며 컬럼명을 변경하세요.",
    "schema": "df(a, b)",
    "sample_rows": [
      "Alice | 25"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "for old, new in zip(old_cols, new_cols):",
      "df = df.withColumnRenamed(old, new)"
    ],
    "hint": "`zip(list_a, list_b)`는 두 리스트의 요소를 순서대로 튜플 `(a[0], b[0]), ...`로 묶어줍니다. 매핑 작업에 필수입니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_python_kwargs_options",
    "title": "Dict Unpacking(**): 읽기 옵션 한 번에 넣기",
    "body": "CSV를 읽을 때 옵션이 많습니다(header=True, inferSchema=True, sep=','). 이 옵션들을 `read_opts`라는 딕셔너리에 담아두고, `spark.read.options()` 함수에 **언패킹(kwargs)**하여 전달하세요.",
    "schema": "N/A",
    "sample_rows": [],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "read_opts = {'header': 'true', 'sep': ','}",
      "spark.read.options(**read_opts).csv(...)"
    ],
    "hint": "함수에 `**딕셔너리`를 넣으면, 딕셔너리의 키=값 쌍이 함수의 인자(keyword arguments)로 풀려서 들어갑니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_python_dict_get_default",
    "title": "Dict Get: 매핑 실패 시 기본값 사용 (UDF)",
    "body": "코드 변환용 딕셔너리 `code_map = {'KR': 'Korea', 'US': 'USA'}`가 있습니다. 국가 코드를 입력받아 이름을 반환하되, 딕셔너리에 없는 코드는 'Unknown'을 반환하는 함수를 `def`로 짜고 UDF로 적용하세요.",
    "schema": "df(code STRING)",
    "sample_rows": [
      "KR -> Korea",
      "JP -> Unknown"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "def map_country(code):",
      "return code_map.get(code, 'Unknown')",
      "udf(...)"
    ],
    "hint": "`dict[key]`는 키가 없으면 에러가 나지만, `dict.get(key, default)`는 안전하게 기본값을 반환합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_python_enumerate_columns",
    "title": "Enumerate: 인덱스와 함께 루프 돌기",
    "body": "컬럼명이 중복되거나 의미 없을 때, 순서대로 번호를 붙이고 싶습니다. `df.columns`를 루프 돌되, `enumerate()`를 사용하여 'col_0', 'col_1', 'col_2'로 이름을 변경하세요.",
    "schema": "df(x, y, z)",
    "sample_rows": [
      "..."
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "for idx, col_name in enumerate(df.columns):",
      "df = df.withColumnRenamed(col_name, f'col_{idx}')"
    ],
    "hint": "`enumerate(리스트)`는 `(0, 첫번째값), (1, 두번째값)...` 순서로 인덱스와 값을 동시에 뱉어줍니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_python_string_slicing",
    "title": "String Slicing: 날짜 문자열 쪼개기 (Lambda)",
    "body": "`YYYYMMDD` 형태의 문자열(예: '20240101')에서 연도(앞 4자리)만 추출하고 싶습니다. 복잡한 날짜 함수 대신, 파이썬의 문자열 슬라이싱 기능을 `lambda` UDF로 만들어 해결하세요.",
    "schema": "df(date_str STRING)",
    "sample_rows": [
      "20240101 -> 2024"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "udf(lambda x: x[:4])"
    ],
    "hint": "문자열 `s`에서 `s[:4]`는 처음부터 4번째 인덱스 전까지 자릅니다. 파이썬의 가장 강력한 기능 중 하나입니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_python_truthy_check",
    "title": "Truthy/Falsy: 리스트가 비었는지 확인하기",
    "body": "데이터 처리 중 에러가 발생하여 `error_list`에 에러 메시지가 담겼습니다. 이 리스트가 **비어있지 않을 때만** 경고 로그를 출력하려고 합니다. 파이썬스럽게 조건문을 작성하세요.",
    "schema": "N/A (로직 문제)",
    "sample_rows": [],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "if error_list:",
      "print('Error!')"
    ],
    "hint": "`if len(error_list) > 0:` 대신 `if error_list:`를 씁니다. 빈 리스트 `[]`는 False, 내용이 있으면 True로 취급됩니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_python_replace_string",
    "title": "String Replace: 파일 경로 수정",
    "body": "S3 경로가 담긴 리스트 `paths`가 있는데, 프로토콜이 `s3n://`으로 되어 있습니다. 이를 모두 `s3a://`로 바꾸는 리스트 컴프리헨션을 작성하세요.",
    "schema": "paths (List[str])",
    "sample_rows": [
      "['s3n://bucket/a', 's3n://bucket/b']"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "[p.replace('s3n://', 's3a://') for p in paths]"
    ],
    "hint": "문자열 메서드 `.replace('찾을값', '바꿀값')`을 사용합니다. 단순 치환에 가장 많이 쓰입니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_python_in_operator",
    "title": "IN Operator: 금지어 포함 여부 확인",
    "body": "금지어 리스트 `bad_words = ['error', 'fail']`가 있습니다. 로그 메시지(문자열) 내에 금지어 중 하나라도 포함되어 있는지 확인하는 함수를 작성하세요. (Spark 함수가 아닌 파이썬 `in` 연산자 활용)",
    "schema": "N/A (UDF 로직)",
    "sample_rows": [],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "def check_bad(msg):",
      "for w in bad_words:",
      "if w in msg: return True",
      "return False"
    ],
    "hint": "파이썬에서 `'a' in 'abc'`는 True입니다. 루프와 `in`을 결합하여 부분 문자열 검색을 구현합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_python_split_unpack",
    "title": "Split & Indexing: 이메일 도메인 추출",
    "body": "이메일 주소 'user@gmail.com'을 `@` 기준으로 쪼갠 후, 뒤쪽 도메인('gmail.com')만 가져오고 싶습니다. `split` 함수와 인덱싱을 `lambda` UDF로 구현하세요.",
    "schema": "df(email STRING)",
    "sample_rows": [
      "a@b.com -> b.com"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "udf(lambda x: x.split('@')[1])"
    ],
    "hint": "`s.split('@')`은 리스트 `['user', 'gmail.com']`을 반환합니다. `[1]` 또는 `[-1]`로 뒤쪽 요소를 가져옵니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_python_type_check",
    "title": "Isinstance: 데이터 타입 방어 로직",
    "body": "함수 `process_data(value)`를 만들 때, 입력된 `value`가 정수(int)일 때만 처리하고, 문자열이나 다른 타입이면 None을 반환하도록 방어 코드를 작성하세요.",
    "schema": "N/A",
    "sample_rows": [],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "if not isinstance(value, int):",
      "return None"
    ],
    "hint": "`type(value) == int`보다 `isinstance(value, int)`가 권장됩니다. 데이터 파이프라인에서 잘못된 타입이 들어오는 것을 막을 때 씁니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_python_zfill",
    "title": "String Padding: 월(Month) 0 채우기",
    "body": "숫자형으로 된 월(month) 데이터(1, 2, ..., 12)를 문자열 '01', '02', ..., '12' 형태로 변환해야 합니다. 파이썬 문자열 메서드 `.zfill()`을 사용하여 2자리로 맞추는 Lambda UDF를 작성하세요.",
    "schema": "df(month INT)",
    "sample_rows": [
      "1 -> '01', 12 -> '12'"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "udf(lambda x: str(x).zfill(2))"
    ],
    "hint": "숫자를 먼저 `str(x)`로 문자로 바꾼 뒤 `.zfill(2)`를 호출하면 앞에 0을 채워줍니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_python_join_list",
    "title": "String Join: 리스트를 문자열로 합치기",
    "body": "가공 결과가 리스트 `['Seoul', 'Busan', 'Jeju']`로 나왔습니다. 이를 콤마(,)로 구분된 하나의 문자열 \"Seoul,Busan,Jeju\"로 합쳐서 DB에 저장하려고 합니다. 파이썬 문자열 메서드를 사용하세요.",
    "schema": "cities (List[str])",
    "sample_rows": [],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "','.join(cities)"
    ],
    "hint": "`'구분자'.join(리스트)` 형태입니다. 리스트를 문자열로 직렬화할 때 가장 많이 씁니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_python_dict_comp",
    "title": "Dict Comprehension: 키-값 뒤집기",
    "body": "코드 매핑 딕셔너리 `mapping = {'A': 1, 'B': 2}`가 있는데, 반대로 숫자로 문자를 찾고 싶어 `{1: 'A', 2: 'B'}` 형태로 뒤집으려 합니다. 딕셔너리 컴프리헨션을 사용하세요.",
    "schema": "N/A",
    "sample_rows": [],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "{v: k for k, v in mapping.items()}"
    ],
    "hint": "리스트 컴프리헨션과 비슷하지만 중괄호 `{}`를 쓰고 `key: value` 형태로 작성합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_python_none_check_udf",
    "title": "None Check: UDF 내부의 결측치 방어",
    "body": "UDF로 문자열 길이를 잴 때, 값이 `None`이면 에러가 납니다. 입력 `x`가 `None`이면 0을, 아니면 길이를 반환하는 안전한 Lambda 식을 작성하세요.",
    "schema": "df(text STRING)",
    "sample_rows": [
      "NULL -> 0"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "udf(lambda x: len(x) if x else 0)"
    ],
    "hint": "파이썬의 삼항 연산자 `값 if 조건 else 다른값`을 활용합니다. `if x:`는 x가 None이거나 빈 문자열이 아님을 체크합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "python_basic_slicing_udf",
    "title": "문자열 슬라이싱: 앞자리만 자르기",
    "body": "주민등록번호나 ID의 앞 6자리만 필요합니다. 복잡한 함수 정의 없이, 파이썬의 슬라이싱 문법 `[:6]`을 사용하는 `lambda` 함수를 UDF로 만들어 적용하세요.",
    "schema": "df(personal_id STRING)",
    "sample_rows": [
      "900101-1234567 -> 900101"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "udf(lambda x: x[:6])"
    ],
    "hint": "파이썬에서 `문자열[:n]`은 처음부터 n번째 글자 앞까지 자릅니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "python_basic_split_last",
    "title": "문자열 쪼개기: 파일 확장자 추출",
    "body": "파일 경로에서 확장자(예: txt, csv)만 가져오고 싶습니다. 점(`.`)을 기준으로 쪼갠(`split`) 후, 리스트의 **맨 마지막 요소**를 가져오는 파이썬 인덱싱 `-1`을 활용하세요.",
    "schema": "df(filepath STRING)",
    "sample_rows": [
      "/data/users.csv -> csv",
      "report.pdf -> pdf"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "udf(lambda x: x.split('.')[-1])"
    ],
    "hint": "`x.split('.')`의 결과는 리스트입니다. `[-1]`은 리스트의 마지막 요소를 의미합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "python_basic_ternary_op",
    "title": "삼항 연산자: 한 줄짜리 If-Else",
    "body": "점수(`score`)가 60 이상이면 'Pass', 아니면 'Fail'을 반환하고 싶습니다. `def`와 `if-else` 블록을 길게 쓰지 말고, 파이썬의 **한 줄 if문(삼항 연산자)**을 사용하여 `lambda` UDF를 만드세요.",
    "schema": "df(score INT)",
    "sample_rows": [
      "80 -> Pass",
      "50 -> Fail"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "udf(lambda x: 'Pass' if x >= 60 else 'Fail')"
    ],
    "hint": "`값1 if 조건 else 값2` 형식을 사용합니다. 파이썬에서 가장 자주 쓰이는 숏컷 중 하나입니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "python_basic_replace",
    "title": "문자열 치환: 특수문자 제거",
    "body": "전화번호에 포함된 하이픈(`-`)을 모두 없애고 숫자만 남기고 싶습니다. 파이썬 문자열의 `.replace()` 메서드를 사용하는 UDF를 작성하세요.",
    "schema": "df(phone STRING)",
    "sample_rows": [
      "010-1234-5678 -> 01012345678"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "udf(lambda x: x.replace('-', ''))"
    ],
    "hint": "`문자열.replace('찾을값', '바꿀값')`을 사용합니다. 바꿀 값에 빈 문자열 `''`을 넣으면 삭제 효과가 납니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "python_basic_f_string_udf",
    "title": "F-String: 단위 붙이기",
    "body": "몸무게 데이터가 숫자로 되어있는데, 출력용으로 뒤에 'kg' 단위를 붙이고 싶습니다. 파이썬의 **f-string**을 사용하여 `lambda` UDF를 만드세요.",
    "schema": "df(weight DOUBLE)",
    "sample_rows": [
      "70.5 -> 70.5kg"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "udf(lambda x: f'{x}kg')"
    ],
    "hint": "`f'{변수}kg'` 형태로 작성하면 변수 값이 문자열 안에 예쁘게 삽입됩니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "python_basic_upper_lower",
    "title": "대소문자 변환: 무조건 소문자로",
    "body": "사용자 입력값이 대문자, 소문자 섞여 있어 검색이 어렵습니다. 파이썬 문자열 메서드 `.lower()`를 사용하여 입력값을 모두 소문자로 통일하는 UDF를 작성하세요.",
    "schema": "df(input_text STRING)",
    "sample_rows": [
      "PySpark -> pyspark",
      "PYTHON -> python"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "udf(lambda x: x.lower())"
    ],
    "hint": "`x.upper()`는 대문자, `x.lower()`는 소문자로 변환합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "python_basic_round",
    "title": "숫자 반올림: 소수점 정리",
    "body": "계산된 평점이 소수점 10자리까지 나와서 지저분합니다. 파이썬 내장 함수 `round()`를 사용하여 소수점 둘째 자리까지만 남기는 UDF를 작성하세요.",
    "schema": "df(rating DOUBLE)",
    "sample_rows": [
      "4.56789 -> 4.57"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "udf(lambda x: round(x, 2))"
    ],
    "hint": "`round(숫자, 자릿수)` 함수를 사용합니다. `round(x, 2)`는 둘째 자리까지 반올림합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "python_basic_in_operator",
    "title": "포함 여부 확인: in 연산자",
    "body": "주소(`address`) 문자열 안에 'Seoul'이라는 단어가 포함되어 있는지 확인하여 True/False를 반환하고 싶습니다. 파이썬의 `in` 키워드를 사용하는 UDF를 작성하세요.",
    "schema": "df(address STRING)",
    "sample_rows": [
      "Gangnam-gu, Seoul -> True",
      "Busan, Korea -> False"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "udf(lambda x: 'Seoul' in x)"
    ],
    "hint": "`'찾을단어' in 문자열변수` 형태로 작성하면 결과는 `True` 또는 `False`가 됩니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "python_basic_type_cast",
    "title": "형변환: 문자를 숫자로 (int)",
    "body": "숫자로 된 문자열 \"007\"을 정수 `7`로 바꾸고 싶습니다. 파이썬 내장 함수 `int()`를 사용하여 형변환을 수행하는 UDF를 작성하세요.",
    "schema": "df(code_str STRING)",
    "sample_rows": [
      "\"007\" -> 7",
      "\"100\" -> 100"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "udf(lambda x: int(x))"
    ],
    "hint": "`int(x)`는 문자열이나 실수를 정수로 변환해줍니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "python_basic_none_check",
    "title": "None 체크: 안전한 길이 구하기",
    "body": "문자열의 길이를 구하는 `len()` 함수를 쓰고 싶은데, 데이터에 `None`(NULL)이 섞여 있어 에러가 걱정됩니다. `x`가 `None`이면 0을, 아니면 `len(x)`를 반환하는 안전한 로직을 작성하세요.",
    "schema": "df(name STRING)",
    "sample_rows": [
      "Alice -> 5",
      "NULL -> 0"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "udf(lambda x: len(x) if x is not None else 0)"
    ],
    "hint": "`if x is not None`은 파이썬에서 가장 정석적인 NULL 체크 방식입니다. (또는 간단히 `if x:`를 쓰기도 합니다)",
    "problem_type": "코딩"
  }
]