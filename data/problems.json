[
  {
    "pid": "sql_select_filter",
    "title": "신규 고객의 정확한 조회: 가입일 기준 필터링과 데이터 무결성 확인",
    "body": "customers 테이블에서 가입일이 2024-01-01 이후인 고객을 조회하세요. 하지만, signup_date가 NULL인 행은 제외하고, 결과는 최근 가입일 순으로 정렬하세요. 실제 실무에서는 NULL 값이 잘못 처리되면 분석 오류로 이어지므로 주의해야 합니다.",
    "schema": "customers(id INT, name TEXT, signup_date DATE)",
    "sample_rows": [
      "1 | Alice | 2024-02-10",
      "2 | Bob | 2023-12-30",
      "3 | Casey | 2024-03-01",
      "4 | David | NULL"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "SELECT",
      "WHERE",
      "IS NOT NULL",
      "ORDER BY"
    ],
    "hint": "NULL 값을 처리하기 위해 signup_date IS NOT NULL 조건을 반드시 추가하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_aggregation_monthly",
    "title": "월별 매출 합계: 날짜 형식 오류 및 누락 데이터 대응",
    "body": "sales 테이블에서 월별 총 매출액을 계산하세요. 그러나 order_date가 NULL인 행은 제외하고, month는 'YYYY-MM' 포맷으로 출력하세요. 실무에서는 잘못된 날짜 형식이나 NULL로 인해 집계 오류가 발생하므로 주의해야 합니다.",
    "schema": "sales(order_date DATE, amount DECIMAL)",
    "sample_rows": [
      "2024-01-05 | 120000",
      "2024-01-18 | 98000",
      "2024-02-02 | 150000",
      "NULL | 10000"
    ],
    "difficulty": "Lv2 초급",
    "kind": "sql",
    "expected": [
      "GROUP BY",
      "SUM",
      "DATE_TRUNC",
      "IS NOT NULL",
      "ORDER BY"
    ],
    "hint": "WHERE order_date IS NOT NULL로 잘못된 데이터 제거하고, DATE_TRUNC('month', order_date)로 월 추출하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_join_customer_orders",
    "title": "고객별 주문 금액 합계: 조인 시 NULL 값과 외부 테이블의 처리",
    "body": "customers(cust_id, name)와 orders(order_id, cust_id, amount) 테이블을 INNER JOIN하여 고객 이름과 총 주문 금액을 조회하세요. 그러나 orders.cust_id가 NULL인 행은 조인 대상에서 제외하고, 결과는 금액 내림차순으로 정렬하세요.",
    "schema": "customers(cust_id INT, name TEXT)\norders(order_id INT, cust_id INT, amount DECIMAL)",
    "sample_rows": [
      "customers: 1 | Alice",
      "customers: 2 | Bob",
      "orders: 10 | 1 | 50000",
      "orders: 11 | NULL | 30000",
      "orders: 12 | 2 | 45000"
    ],
    "difficulty": "Lv3 중급",
    "kind": "sql",
    "expected": [
      "JOIN",
      "ON",
      "IS NOT NULL",
      "GROUP BY",
      "SUM",
      "ORDER BY"
    ],
    "hint": "JOIN 조건에 cust_id IS NOT NULL을 추가하거나, WHERE 절에서 필터링하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_window_rank",
    "title": "카테고리별 최고 판매 상품: 동점 처리 및 복수 데이터 경우의 대응",
    "body": "products(id, category, price) 테이블에서 카테고리별로 가장 비싼 상품을 찾아주세요. 단, 가격이 같은 제품이 여러 개 있을 수 있으므로, 이 경우 **모든 동점 제품**을 포함하여 반환하세요 (RANK 사용). 결과는 id 오름차순으로 정렬하세요.",
    "schema": "products(id INT, category TEXT, price DECIMAL)",
    "sample_rows": [
      "1 | electronics | 120000",
      "2 | electronics | 120000",
      "3 | books | 18000"
    ],
    "difficulty": "Lv4 고급",
    "kind": "sql",
    "expected": [
      "RANK() OVER(PARTITION BY category ORDER BY price DESC)",
      "WHERE rank = 1",
      "ORDER BY id"
    ],
    "hint": "ROW_NUMBER는 동점 시 하나만 반환하지만, RANK()는 모든 동점을 포함하므로 적절합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_running_total",
    "title": "월별 누적 매출: 날짜 포맷 불일치 및 데이터 순서 오류 방지",
    "body": "sales(month TEXT, amount DECIMAL) 테이블에서 월 기준으로 매출과 누적 매출을 계산하세요. 그러나 month 값이 '2024-03', '2024-01'처럼 문자열 순서로 정렬되지 않을 수 있으므로, 이를 정확히 정렬하고 누적합을 구하세요.",
    "schema": "sales(month TEXT, amount DECIMAL)",
    "sample_rows": [
      "2024-03 | 180000",
      "2024-01 | 200000",
      "2024-02 | 230000"
    ],
    "difficulty": "Lv5 심화",
    "kind": "sql",
    "expected": [
      "CAST(month AS DATE)",
      "SUM() OVER(ORDER BY CAST(month AS DATE))",
      "ORDER BY CAST(month AS DATE)"
    ],
    "hint": "문자열은 알파벳순으로 정렬되므로, DATE로 변환해 정확한 순서를 보장하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_basic_filter",
    "title": "활성 사용자 필터링: NULL 및 불일치된 타입 처리",
    "body": "users DataFrame에서 active가 True이고 last_login이 2024-01-01 이후인 사용자를 필터링하세요. 그러나 last_login가 NULL인 행은 제외하고, active 컬럼은 BOOLEAN 타입임을 확인하세요 (실제로 문자열 'true'일 수 있으므로 주의).",
    "schema": "users(id INT, active BOOLEAN, last_login TIMESTAMP)",
    "sample_rows": [
      "1 | true | 2024-02-03",
      "2 | false | NULL",
      "3 | TRUE | 2024-01-12"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "filter",
      "isNotNull()",
      "col('active').eq(lit(True))"
    ],
    "hint": "NULL 처리를 위해 .isNull().not() 또는 isNotNull()을 사용하고, Boolean 비교는 lit(True)로 하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_join",
    "title": "고객 주문 통계: 조인 시 외부 테이블의 NULL 값 처리 및 성능 고려",
    "body": "customers와 orders DataFrame을 INNER JOIN하여 고객 이름과 총 주문 금액, 주문 건수를 계산하세요. 그러나 customers.cust_id가 orders에 없는 경우는 포함하지 말고, 결과는 주문 금액 내림차순으로 정렬하세요.",
    "schema": "customers(cust_id INT, name STRING)\norders(order_id INT, cust_id INT, amount DOUBLE)",
    "sample_rows": [
      "customers: 1 | Alice",
      "customers: 2 | Bob",
      "orders: 10 | 1 | 50000.0",
      "orders: 11 | 99 | 30000.0",
      "orders: 12 | 2 | 10000.0"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "join",
      "groupBy",
      "agg",
      "sum('amount')",
      "count('*')",
      "orderBy(desc('total_amount'))"
    ],
    "hint": "INNER JOIN은 자동으로 없는 키는 제외하므로, 조인 전에 cust_id가 NULL인지 확인하는 것이 안전합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_window",
    "title": "이동 평균 계산: 시간 정렬 오류 및 데이터 누락 처리",
    "body": "events(time TIMESTAMP, value DOUBLE)에서 최근 3개 행의 이동 평균을 계산하세요. 하지만 time 컬럼에 NULL이나 중복 값이 있을 수 있으므로, 이를 제거하고 정확한 순서로 계산하세요.",
    "schema": "events(time TIMESTAMP, value DOUBLE)",
    "sample_rows": [
      "2024-03-01 10:00 | 10.0",
      "2024-03-01 10:05 | 14.0",
      "2024-03-01 10:05 | 12.0",
      "2024-03-01 10:15 | 16.0"
    ],
    "difficulty": "Lv3 중급",
    "kind": "python",
    "expected": [
      "dropDuplicates(['time'])",
      "orderBy('time')",
      "Window.orderBy('time').rowsBetween(-2, 0)",
      "avg('value')"
    ],
    "hint": "중복 시간은 제거하고, 정확한 순서를 보장하기 위해 time 기준으로 정렬하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_pivot",
    "title": "주간 상태별 집계 피벗: WEEK 날짜 오차 및 NULL 값 대체",
    "body": "logs(timestamp TIMESTAMP, status STRING)에서 주차별(status 별) 건수를 피벗 형태로 계산하세요. 다만, timestamp가 NULL이면 제외하고, 주차는 date_trunc('week', timestamp) 기준으로 정의하세요. 결과에서 결측치는 0으로 채우세요.",
    "schema": "logs(timestamp TIMESTAMP, status STRING)",
    "sample_rows": [
      "2024-01-02 | success",
      "2024-01-03 | fail",
      "NULL | success", 
      "2024-01-08 | success"
    ],
    "difficulty": "Lv4 고급",
    "kind": "python",
    "expected": [
      "withColumn('week', date_trunc('week', 'timestamp'))",
      "filter(isNotNull('timestamp'))",
      "groupBy('week', 'status').count()",
      "pivot('status')",
      "fillna(0)"
    ],
    "hint": "date_trunc 이후 groupBy 후 pivot 전에 NULL 제거가 중요합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_sessionize",
    "title": "세션 분리: 시간 간격 오류 및 정밀한 체크",
    "body": "pageviews(user_id, ts)에서 사용자별로 30분 이상의 간격이 있을 경우 새로운 세션이 시작됩니다. 세션 번호를 생성하고, 각 세션당 페이지뷰 수를 집계하세요. 단, ts가 NULL인 행은 제외하고, 시간 정렬을 반드시 수행하세요.",
    "schema": "pageviews(user_id INT, ts TIMESTAMP)",
    "sample_rows": [
      "1 | 2024-01-01 10:00",
      "1 | 2024-01-01 10:10", 
      "1 | 2024-01-01 11:00", 
      "1 | NULL"
    ],
    "difficulty": "Lv5 심화",
    "kind": "python",
    "expected": [
      "orderBy('user_id', 'ts')",
      "dropna(subset=['ts'])",
      "Window.partitionBy('user_id').orderBy('ts')",
      "lag('ts')",
      "when(datediff(ts, lag_ts) > 0.5/24, lit(1)).otherwise(lit(0))",
      "sum().over() as session_id"
    ],
    "hint": "lag()로 이전 시각을 가져온 후, 시간 차이를 계산하고 30분 이상이면 플래그 생성.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_window_rank",
    "title": "전체 데이터 순위: 동점 처리 및 정렬 방식 선택",
    "body": "DataFrame에서 score를 내림차순으로 순위를 매기되, 동점일 경우 **동일한 순위**로 지정하세요 (RANK() 사용). 결과는 rank 컬럼과 함께 출력하세요.",
    "schema": "score INT",
    "sample_rows": [
      "95",
      "87",
      "92",
      "92"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "Window.orderBy(desc('score'))",
      "rank().over(window)",
      "withColumn"
    ],
    "hint": "row_number()는 순서를 강제하고, rank()는 동점에 대해 같은 순위를 부여합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_window_rank_dept",
    "title": "부서별 급여 순위: 여러 명 동점 처리 및 데이터 검증",
    "body": "department 기준으로 salary 내림차순으로 rank를 매기고, 각 부서의 최고 급여자만 추출하세요. 그러나 동점자가 있을 수 있으므로, **모든 동점자**를 포함해야 합니다 (RANK = 1). 결과는 department와 salary 오름차순 정렬.",
    "schema": "department STRING, salary INT",
    "sample_rows": [
      "Sales | 65000",
      "Engineering | 80000",
      "Sales | 70000",
      "Engineering | 80000"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "Window.partitionBy('department').orderBy(desc('salary'))",
      "rank().over(window)",
      "filter(col('rank') == 1)",
      "orderBy('department', 'salary')"
    ],
    "hint": "RANK() 사용 시 동점자 모두 포함되며, row_number()는 하나만 반환합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_select_columns",
    "title": "특정 컬럼 선택: 불필요한 컬럼의 존재 및 실무 상황 반영",
    "body": "name과 score 컬럼만 선택하여 출력하세요. 그러나 name이 NULL일 경우 'Unknown'으로 대체하고, score는 정수형(int)으로 변환하세요.",
    "schema": "name STRING, age INT, score INT",
    "sample_rows": [
      "Alice | 25 | 90",
      "NULL | 30 | 80"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "select",
      "coalesce('name', lit('Unknown'))",
      "col('score').cast('int')"
    ],
    "hint": "NULL 처리는 coalesce 또는 when-otherwise, 타입 변환은 cast를 사용하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_select_filter_columns",
    "title": "조건에 맞는 직원의 이름과 급여 선택: 컬럼명 대소문자 불일치 처리",
    "body": "dept가 'Sales'인 직원의 name과 salary만 선택하세요. 그러나 dept 컬럼이 대문자, 소문자, 혼합 등 다양한 형태로 입력될 수 있으므로, 모두 일관되게 비교하세요.",
    "schema": "name STRING, dept STRING, salary INT",
    "sample_rows": [
      "Alice | Sales | 60000",
      "Bob | sales | 55000",
      "Charlie | SALES | 62000"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "filter(lower(col('dept')) == 'sales')",
      "select"
    ],
    "hint": "lower()나 upper()로 정규화 후 비교하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_upper_case",
    "title": "이름 대문자 변환: NULL 값 포함 및 실수 처리",
    "body": "name 컬럼을 대문자로 변환한 name_upper 컬럼을 추가하세요. 그러나 name이 NULL인 경우는 'UNKNOWN'으로 대체하세요.",
    "schema": "name STRING, salary INT",
    "sample_rows": [
      "alice | 60000",
      "bob | 55000",
      "NULL | 70000"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "withColumn('name_upper', coalesce(upper(col('name')), lit('UNKNOWN')))"
    ],
    "hint": "coalesce는 NULL을 처리하고, upper()로 대문자 변환.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_concat_and_lower",
    "title": "이름 결합 및 소문자 처리: 중간 공백 포함 및 NULL 처리",
    "body": "first_name과 last_name을 공백으로 연결한 full_name 컬럼을 만들고, 소문자로 변환하세요. 그러나 first_name이나 last_name이 NULL일 경우는 'unknown'으로 대체하고, 중복 공백은 제거해야 합니다.",
    "schema": "first_name STRING, last_name STRING, salary INT",
    "sample_rows": [
      "John | Doe | 70000",
      "Jane | Smith | 68000",
      "NULL | Doe | 50000"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "concat_ws(' ', coalesce(first_name, lit('unknown')), coalesce(last_name, lit('unknown')))",
      "lower()"
    ],
    "hint": "concat_ws는 NULL을 자동으로 무시하고 공백 제어 가능.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_sql_register_view",
    "title": "DataFrame을 SQL 뷰로 등록: 쿼리 오류 방지 및 실시간 분석 시나리오",
    "body": "DataFrame을 sales라는 임시 뷰로 등록하고, region별 총 amount를 계산하세요. 그러나 만약 view에 이미 같은 이름이 존재하면 덮어쓰세요. 결과는 amount 내림차순으로 정렬.",
    "schema": "region STRING, amount DECIMAL",
    "sample_rows": [
      "North | 10000",
      "South | 15000"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "createOrReplaceTempView('sales')",
      "spark.sql('SELECT region, SUM(amount) FROM sales GROUP BY region ORDER BY amount DESC')"
    ],
    "hint": "createOrReplaceTempView는 기존 뷰를 덮어쓰므로 안전합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_sql_window_rank_dept",
    "title": "SQL에서 Window 함수로 부서별 최고 급여자: 서브쿼리의 성능 및 정확성",
    "body": "employees를 임시 뷰로 등록하고, 각 부서의 최고 급여자를 RANK() OVER(PARTITION BY)를 사용해 조회하세요. 결과는 department와 salary 오름차순으로 출력하고, 동점자는 모두 포함.",
    "schema": "department STRING, salary DECIMAL",
    "sample_rows": [
      "Engineering | 80000",
      "Sales | 75000",
      "Engineering | 80000"
    ],
    "difficulty": "Lv3 중급",
    "kind": "python",
    "expected": [
      "createOrReplaceTempView('employees')",
      "spark.sql(\"SELECT department, salary FROM (SELECT department, salary, RANK() OVER(PARTITION BY department ORDER BY salary DESC) as rnk FROM employees) WHERE rnk = 1 ORDER BY department, salary\")"
    ],
    "hint": "서브쿼리 내에서 RANK 함수를 사용하고, 외부에서 rank=1 조건 필터링.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_cast_price_int",
    "title": "가격 컬럼 정수형 변환: 반올림 오류 및 실수 처리",
    "body": "price 컬럼을 정수형(int)으로 변환하세요. 단, 원래 값이 소수일 경우 **반올림**하여 정수로 변환하고, NULL은 0으로 채우세요.",
    "schema": "product STRING, price DOUBLE",
    "sample_rows": [
      "Laptop | 1299.99",
      "Mouse | 25.5",
      "NULL | 0"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "withColumn('price_int', round(coalesce(col('price'), lit(0))).cast('int'))"
    ],
    "hint": "round() + coalesce + cast를 조합하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_fillna_null_handling",
    "title": "NULL 값 대체: 복수 컬럼 동시 처리 및 실무 사례 반영",
    "body": "salary가 NULL인 행은 0으로, department가 NULL인 행은 'Unknown'으로 채우세요. 그러나 이 작업은 **모든 열에 대해 일관되게** 수행되어야 하며, 비즈니스 규칙을 준수해야 합니다.",
    "schema": "name STRING, salary DOUBLE, department STRING",
    "sample_rows": [
      "Alice | null | Sales",
      "Bob | 60000 | null"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "fillna({'salary': 0.0, 'department': 'Unknown'})"
    ],
    "hint": "fillna는 딕셔너리 형식으로 여러 컬럼 처리 가능.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_distinct_remove_duplicates",
    "title": "완전 중복 행 제거: 중복 행의 의미 파악 및 실무 적용",
    "body": "DataFrame에서 완전히 동일한 행을 제거하세요. 그러나 정확히 같은 값이더라도, 비즈니스적으로 다른 의미를 가질 수 있으므로, 반드시 이 작업 전에 데이터의 출처와 중복 원인을 확인해야 합니다.",
    "schema": "name STRING, age INT, department STRING",
    "sample_rows": [
      "Alice | 25 | Sales",
      "Alice | 25 | Sales",
      "Bob | 30 | Engineering"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "distinct()"
    ],
    "hint": ".distinct()는 완전 일치 행 제거에 효과적입니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_drop_duplicates_email",
    "title": "이메일 기준 중복 제거 및 개수 비교: 비즈니스 규칙 반영",
    "body": "email 컬럼 기준으로 중복된 행을 제거하고, 제거 전후의 행 수를 출력하세요. 그러나 같은 이메일인데 다른 정보(예: 이름)가 있는 경우는 데이터 일관성 문제이므로, 주의해야 합니다.",
    "schema": "name STRING, email STRING, score INT",
    "sample_rows": [
      "Alice | alice@email.com | 90",
      "Bob | bob@email.com | 85",
      "Alice | alice@email.com | 92"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "dropDuplicates(['email'])",
      "count()"
    ],
    "hint": "중복 제거 후 count()로 개수 비교. 비즈니스 규칙에 따라 이메일 중복은 허용되나, 다른 필드는 일치해야 함.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_orderby_price_asc",
    "title": "가격 오름차순 정렬: NULL 값 처리 포함",
    "body": "DataFrame을 price 컬럼 기준으로 오름차순으로 정렬하세요. 그러나 price가 NULL인 행은 가장 뒤에 배치해야 합니다 (실무에서 자주 요구됨).",
    "schema": "product STRING, price DECIMAL",
    "sample_rows": [
      "Laptop | 1299.99",
      "Mouse | 25.5",
      "Keyboard | NULL"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "orderBy(col('price').asc_nulls_last())"
    ],
    "hint": ".asc_nulls_last()로 NULL을 가장 뒤에 배치할 수 있습니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_groupby_avg_salary_desc",
    "title": "부서별 평균 급여: NULL 처리 및 실수 소수점 반올림",
    "body": "department 기준으로 그룹화하여 평균 salary를 계산하고, 결과는 평균 급여 내림차순으로 정렬하세요. 단, salary가 NULL인 행은 제외하고, 평균값은 소수 둘째 자리까지 반올림하여 출력하세요.",
    "schema": "department STRING, salary DECIMAL",
    "sample_rows": [
      "Engineering | 80000",
      "Sales | 65000",
      "Marketing | NULL"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "groupBy('department')",
      "agg(round(avg('salary'), 2).alias('avg_salary'))",
      "orderBy(desc('avg_salary'))"
    ],
    "hint": "null 처리는 groupBy 전에 filter(isNotNull('salary')) 또는 avg 함수 자체가 자동으로 무시함. round()로 반올림.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_union_combine_dataframes",
    "title": "두 DataFrame의 결합: 스키마 일치 여부 확인 및 실무 주의사항",
    "body": "df1과 df2를 union으로 결합하세요. 그러나 두 DataFrame의 컬럼 이름이 다를 수 있으므로, 결과가 예상한 대로 되는지 반드시 검증해야 합니다. 실무에서는 스키마 불일치로 인해 오류 발생 가능.",
    "schema": "name STRING, value INT",
    "sample_rows": [
      "A | 1",
      "B | 2"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "union"
    ],
    "hint": "union은 컬럼 순서와 타입이 정확히 일치해야 함. 스키마 불일치 시 오류 발생.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_unionbyname_mismatched_columns",
    "title": "컬럼 순서가 다른 DataFrame 결합: unionByName의 장점 및 주의사항",
    "body": "df1과 df2의 컬럼 순서가 다를 때, unionByName로 결합하고 중복 행을 제거하세요. 그러나 컬럼명이 동일하지 않으면 오류가 발생하므로, 반드시 스키마 일치 여부를 확인해야 합니다.",
    "schema": "name STRING, age INT (df1)\nage INT, name STRING (df2)",
    "sample_rows": [
      "Alice | 25",
      "30 | Bob"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "unionByName",
      ".distinct()"
    ],
    "hint": "unionByName은 컬럼명 기반 결합이므로, 컬럼 이름이 같아야 함. 스키마 불일치 시 예외 발생.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_when_otherwise_score_grade",
    "title": "점수에 따른 합격 여부: NULL 값 처리 및 조건 체계적 관리",
    "body": "score가 60 이상이면 'Pass', 미만이면 'Fail'인 result 컬럼을 추가하세요. 그러나 score가 NULL일 경우는 'Unknown'으로 처리하고, 결과는 정렬하여 출력하세요.",
    "schema": "name STRING, score INT",
    "sample_rows": [
      "Alice | 85",
      "Bob | 50",
      "Charlie | NULL"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "when(col('score').isNotNull(), when(col('score') >= 60, 'Pass').otherwise('Fail')).otherwise('Unknown')"
    ],
    "hint": "NULL 체크를 먼저 해야 함. 조건을 중첩으로 처리해야 오류 없음.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_when_otherwise_price_level",
    "title": "가격 기준 등급 분류: 조건 순서 및 NULL 처리 중요성 강조",
    "body": "price를 기준으로 10000 미만은 'Low', 50000 미만은 'Medium', 50000 이상은 'High'로 분류하세요. 그러나 price가 NULL인 경우는 'Unknown'으로 처리하고, 조건 순서에 주의하세요.",
    "schema": "product STRING, price DECIMAL",
    "sample_rows": [
      "Laptop | 8000",
      "Desktop | 45000",
      "Server | 120000",
      "Printer | NULL"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "when(col('price').isNull(), lit('Unknown'))",
      ".when(col('price') < 10000, 'Low')",
      ".when(col('price') < 50000, 'Medium')",
      ".otherwise('High')"
    ],
    "hint": "조건 순서는 중요. 큰 값부터 체크하지 않으면 오류 발생 가능.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_agg_count_avg_multiple",
    "title": "전체 상품 수와 평균 가격: 하나의 agg로 처리 및 스키마 검증",
    "body": "DataFrame에서 전체 상품 개수(count)와 평균 가격(avg_price)을 한 번의 agg()로 계산하세요. 그러나 price가 NULL인 경우는 제외하고, 결과 컬럼명은 각각 'total_count', 'avg_price'로 명확히 지정하세요.",
    "schema": "product STRING, price DECIMAL",
    "sample_rows": [
      "Laptop | 1299.99",
      "Mouse | 25.5",
      "Keyboard | NULL"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "agg(count('*').alias('total_count'), avg('price').alias('avg_price'))"
    ],
    "hint": "count('*')는 NULL 제외. count(col)도 가능하지만, '*'가 더 직관적.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_agg_groupby_dept_stats",
    "title": "부서별 통계: 최소/최대/평균 급여 계산 및 NULL 처리 반영",
    "body": "department 기준으로 그룹화하여 직원 수, 최저 급여, 최고 급여, 평균 급여를 모두 계산하세요. 그러나 salary가 NULL인 행은 통계에서 제외하고, 결과는 department 오름차순 정렬.",
    "schema": "name STRING, department STRING, salary DECIMAL",
    "sample_rows": [
      "Alice | Engineering | 80000",
      "Bob | Sales | 65000",
      "Charlie | Marketing | NULL"
    ],
    "difficulty": "Lv3 중급",
    "kind": "python",
    "expected": [
      "groupBy('department')",
      "agg(count('*').alias('emp_count'), min('salary').alias('min_salary'), max('salary').alias('max_salary'), avg('salary').alias('avg_salary'))",
      "orderBy('department')"
    ],
    "hint": "NULL은 집계 함수에서 자동으로 무시되지만, 명확히 설명하는 것이 좋음.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_date_extract_year_month",
    "title": "주문 날짜 연도와 월 추출: NULL 처리 및 실수형 데이터 문제",
    "body": "order_date 컬럼에서 year과 month를 추출하여 새로운 컬럼으로 추가하세요. 그러나 order_date가 NULL인 경우는 0으로 대체하고, 결과는 정확한 정수 타입으로 유지하세요.",
    "schema": "name STRING, order_date DATE",
    "sample_rows": [
      "Alice | 2024-03-15",
      "Bob | NULL"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "withColumn('year', coalesce(year(col('order_date')), lit(0)))",
      "withColumn('month', coalesce(month(col('order_date')), lit(0)))"
    ],
    "hint": "year() 함수는 NULL을 반환하므로, coalesce로 대체.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_date_days_ago_filter",
    "title": "현재 날짜와 차이 계산 후 필터링: 반올림 오류 및 실수 처리",
    "body": "order_date와 현재 날짜의 차이를 days_ago 컬럼으로 추가하고, 30일 이상 지난 데이터만 필터링하세요. 그러나 datediff는 정수 반환하므로 소수점은 무시하며, NULL 값은 제외.",
    "schema": "product STRING, order_date DATE",
    "sample_rows": [
      "Laptop | 2024-01-15",
      "Mouse | 2024-03-10",
      "Keyboard | NULL"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "withColumn('days_ago', datediff(current_date(), col('order_date')))",
      "filter(col('days_ago') >= 30).drop('days_ago')"
    ],
    "hint": "NULL은 자동 제외되며, days_ago는 정수형으로 사용 가능.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_coalesce_phone_display",
    "title": "전화번호 NULL 처리: N/A 대체 및 실무 비즈니스 규칙 반영",
    "body": "phone 컬럼이 NULL인 경우 'N/A'로 대체한 phone_display 컬럼을 추가하세요. 그러나 phone 값이 빈 문자열('')일 수도 있으므로, 이 또한 'N/A'로 처리하세요.",
    "schema": "name STRING, phone STRING",
    "sample_rows": [
      "Alice | null",
      "Bob | 123-4567",
      "Charlie | ''"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "withColumn('phone_display', coalesce(when(col('phone') == '', lit('N/A')), col('phone'), lit('N/A')))"
    ],
    "hint": "isnull() 또는 eq(lit(''))로 빈 문자열도 처리.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_fillna_salary_null",
    "title": "NULL 값 채우기: 복수 컬럼 동시 처리 및 실무 규칙 강조",
    "body": "salary가 NULL인 행은 0으로, department가 NULL인 행은 'Unknown'으로 fillna로 채우세요. 그러나 이 작업은 데이터 품질 개선 전 단계이므로, 이후에 반드시 검증해야 합니다.",
    "schema": "name STRING, salary DOUBLE, department STRING",
    "sample_rows": [
      "Alice | null | Sales",
      "Bob | 60000 | null"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "fillna({'salary': 0.0, 'department': 'Unknown'})"
    ],
    "hint": "fillna는 여러 컬럼을 딕셔너리로 한 번에 처리 가능.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_cache_filter_count",
    "title": "필터링 후 cache: 성능 향상과 메모리 사용 주의사항",
    "body": "DataFrame을 score가 80 이상인 행으로 필터링한 후, cache()를 적용하고 count()와 show()를 실행하세요. 그러나 cache는 메모리에 저장되므로, 너무 큰 데이터에는 적합하지 않으며, 이후에 반드시 uncache()를 고려해야 합니다.",
    "schema": "name STRING, score INT",
    "sample_rows": [
      "Alice | 95",
      "Bob | 70"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "filter(col('score') >= 80)",
      ".cache()",
      "count()",
      "show()"
    ],
    "hint": "cache()는 반복 작업에 유리하지만, 메모리 관리 필요. 실무에서는 무분별 사용 금지.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_cache_avg_dept_use_twice",
    "title": "부서별 평균 급여 cache 후 두 번 사용: 반복 작업 최적화 및 주의점",
    "body": "department 기준으로 평균 급여를 계산한 결과를 cache()로 저장하고, show()와 특정 조건(평균 60000 이상) 필터링을 두 번 실행하세요. 그러나 이 작업은 데이터 크기가 커질수록 메모리 사용 증가하므로 주의해야 합니다.",
    "schema": "name STRING, department STRING, salary DECIMAL",
    "sample_rows": [
      "Alice | Engineering | 80000",
      "Bob | Sales | 55000"
    ],
    "difficulty": "Lv3 중급",
    "kind": "python",
    "expected": [
      "groupBy('department').agg(avg('salary').alias('avg_salary')).cache()",
      "show()",
      "filter(col('avg_salary') >= 60000).show()"
    ],
    "hint": "cache()는 반복 조회에 효과적. 그러나 메모리 관리가 필요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_collect_high_score_students",
    "title": "80점 이상 학생 수집: collect()의 리소스 제한 및 실무 주의사항",
    "body": "score가 80 이상인 학생들을 collect()로 가져와, 이름만 담은 리스트를 만드세요. 그러나 DataFrame이 너무 클 경우, 메모리 오류 발생 가능하므로, 반드시 샘플 데이터나 limit을 사용해 테스트하세요.",
    "schema": "name STRING, score INT",
    "sample_rows": [
      "Alice | 95",
      "Bob | 75"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "filter(col('score') >= 80)",
      ".collect()",
      "[row.name for row in rows]"
    ],
    "hint": "collect()는 드라이버 메모리에 모든 데이터를 가져오므로, 크기 제한 있음. 실무에서는 limit 사용 권장.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_repartition_5_partitions",
    "title": "DataFrame을 5개 파티션으로 재분할: 파티션 수 검증 및 성능 영향 고려",
    "body": "DataFrame을 5개의 파티션으로 repartition하고, 파티션 수를 출력하세요. 그러나 repartition은 데이터 리셔플링이므로, 성능 저하가 발생할 수 있으며, 적절한 파티션 수는 클러스터 크기에 따라 달라집니다.",
    "schema": "name STRING, value INT",
    "sample_rows": [
      "A | 1",
      "B | 2"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "repartition(5)",
      ".getNumPartitions()"
    ],
    "hint": "repartition은 데이터를 다시 분할하므로 성능 저하 가능. 너무 작은 숫자는 병렬 처리 효율 감소.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_repartition_coalesce",
    "title": "파티션 수 줄이기: repartition 후 coalesce로 최적화",
    "body": "DataFrame을 10개 파티션으로 repartition한 후, coalesce(1)로 1개 파티션으로 합쳐서 파티션 수를 확인하세요. 그러나 coalesce는 리셔플링 없이 조합되므로 효율적입니다.",
    "schema": "name STRING, value INT",
    "sample_rows": [
      "A | 1",
      "B | 2"
    ],
    "difficulty": "Lv3 중급",
    "kind": "python",
    "expected": [
      "repartition(10)",
      "coalesce(1)",
      ".getNumPartitions()"
    ],
    "hint": "repartition은 리셔플링, coalesce는 리셔플링 없이 파티션 조합 가능. 성능 고려 필수.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_broadcast_join_large_small",
    "title": "큰 DataFrame과 작은 DataFrame의 broadcast join: 메모리 사용 및 장점 설명",
    "body": "df_large와 df_small을 customer_id 기준으로 broadcast join하세요. 그러나 small DataFrame은 일반적으로 10MB 이하로 제한되어야 하며, 그렇지 않으면 브로드캐스트 오류 발생 가능.",
    "schema": "customer_id INT, amount DECIMAL (large)\nregion STRING, category STRING (small)",
    "sample_rows": [
      "101 | 500",
      "102 | 300"
    ],
    "difficulty": "Lv3 중급",
    "kind": "python",
    "expected": [
      "broadcast(df_small)",
      "join(broadcast(df_small), on='customer_id')"
    ],
    "hint": "broadcast는 small 테이블에만 사용. 크기 제한을 반드시 확인하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_broadcast_join_count_by_category",
    "title": "브로드캐스트 조인 후 카테고리별 주문 수: 실무 성능 최적화 사례",
    "body": "orders(큰 테이블)와 categories(작은 테이블)를 broadcast join하고, 각 카테고리별 주문 개수를 계산하세요. 그러나 categories의 크기는 10MB 이하로 제한되어야 하며, 그렇지 않으면 브로드캐스트 실패 가능.",
    "schema": "order_id INT, customer_id INT, category_id INT (orders)\ncategory_id INT, category_name STRING (categories)",
    "sample_rows": [
      "1 | 101 | 5",
      "2 | 102 | 3"
    ],
    "difficulty": "Lv4 고급",
    "kind": "python",
    "expected": [
      "broadcast(categories)",
      "join(broadcast(categories), on='category_id')",
      "groupBy('category_name').count()"
    ],
    "hint": "broadcast 조인은 작은 테이블에만 사용. 크기 초과 시 오류 발생.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_explode_array_to_rows",
    "title": "배열 컬럼을 행으로 펼치기: NULL 및 빈 배열 처리 강조",
    "body": "items 배열 컬럼을 explode하여 각 요소를 별도의 행으로 만드세요. 그러나 items가 NULL이거나 비어 있는 경우는 제외하고, 결과는 정렬된 상태로 출력하세요.",
    "schema": "name STRING, items ARRAY<STRING>",
    "sample_rows": [
      "Alice | [Apple, Banana]",
      "Bob | []",
      "Charlie | null"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "filter(col('items').isNotNull() & (size(col('items')) > 0))",
      "withColumn('item', explode(col('items')))",
      "orderBy('name', 'item')"
    ],
    "hint": "explode는 NULL이나 빈 배열에 대해 null을 반환하므로, 필터링 필요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_explode_count_by_product",
    "title": "고객 구매 상품 배열 explode 후 카운트: 중복 및 NULL 처리 포함",
    "body": "products 배열 컬럼을 explode하여 각 상품별 구매 횟수를 세세요. 그러나 products가 NULL이나 빈 배열인 경우는 제외하고, 결과는 count 내림차순 정렬.",
    "schema": "customer_id INT, products ARRAY<STRING>",
    "sample_rows": [
      "1 | [Laptop, Mouse]",
      "2 | []",
      "3 | null"
    ],
    "difficulty": "Lv3 중급",
    "kind": "python",
    "expected": [
      "filter(col('products').isNotNull() & (size(col('products')) > 0))",
      "withColumn('product', explode(col('products'))) ",
      "groupBy('product').count().orderBy(desc('count'))"
    ],
    "hint": "explode 전에 NULL/빈 배열 필터링이 중요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_pivot_dept_avg_salary",
    "title": "부서별 연도별 평균 급여 피벗: 비즈니스 요구사항 반영 및 결측값 처리",
    "body": "department 기준으로 year을 피벗하여 각 부서의 연도별 평균 급여를 표시하세요. 그러나 결과에서 결측치는 0으로 채우고, department와 year 오름차순 정렬.",
    "schema": "department STRING, year INT, salary DECIMAL",
    "sample_rows": [
      "Engineering | 2023 | 75000",
      "Sales | 2024 | 68000"
    ],
    "difficulty": "Lv3 중급",
    "kind": "python",
    "expected": [
      "groupBy('department').pivot('year').agg(avg('salary')).fillna(0)",
      "orderBy('department', 'year')"
    ],
    "hint": "pivot 후 fillna(0)로 결측치 대체.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_pivot_region_category_sum",
    "title": "지역-카테고리별 판매량 피벗: null을 0으로 채우기 및 실무 적용",
    "body": "region과 category 기준으로 quantity의 합계를 피벗 테이블로 만들고, 결측치는 0으로 처리하세요. 그러나 결과는 region과 category 오름차순 정렬.",
    "schema": "region STRING, category STRING, quantity INT",
    "sample_rows": [
      "North | Electronics | 10",
      "South | Furniture | 5"
    ],
    "difficulty": "Lv4 고급",
    "kind": "python",
    "expected": [
      "groupBy('region', 'category').sum('quantity')",
      "pivot('category')",
      "fillna(0)",
      "orderBy('region', 'category')"
    ],
    "hint": "피벗 전에 groupBy로 집계하고, 이후 fillna(0) 적용.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_udf_string_length",
    "title": "문자열 길이 UDF: NULL 처리 및 성능 고려 사항 강조",
    "body": "name 컬럼의 문자열 길이를 계산하여 name_length 컬럼으로 추가하세요. 그러나 name이 NULL인 경우는 0으로 반환하고, 함수는 효율적으로 작성되어야 합니다.",
    "schema": "name STRING, score INT",
    "sample_rows": [
      "Alice | 90",
      "Bob | 85",
      "NULL | 70"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "udf(lambda s: len(s) if s is not None else 0, IntegerType())",
      "withColumn('name_length', udf_name_length(col('name')))"
    ],
    "hint": "UDF는 Python 함수이므로 NULL 처리 필수. 성능 저하 가능.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_udf_score_to_grade",
    "title": "점수 → 등급 변환 UDF: 조건 체계 및 실무 규칙 반영",
    "body": "score 컬럼을 기준으로 90 이상은 'A', 80 이상은 'B', 그 외는 'C'로 분류하는 UDF를 만들어 grade 컬럼에 적용하세요. 그러나 score가 NULL인 경우는 'Unknown'으로 처리.",
    "schema": "name STRING, score INT",
    "sample_rows": [
      "Alice | 95",
      "Bob | 82",
      "Charlie | NULL"
    ],
    "difficulty": "Lv3 중급",
    "kind": "python",
    "expected": [
      "udf(lambda s: 'A' if s >= 90 else ('B' if s >= 80 else 'C') if s is not None else 'Unknown', StringType())"
    ],
    "hint": "조건문을 포함한 UDF는 Python 코드로 작성. NULL 처리 필수.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_groupby_avg_salary_desc",
    "title": "부서별 평균 급여 내림차순 정렬하기",
    "body": "department 기준으로 그룹화하여 평균 salary를 계산하고, 결과를 평균 급여 내림차순으로 정렬하세요.",
    "schema": "department STRING, salary DECIMAL",
    "sample_rows": [
      "Engineering | 80000",
      "Sales | 65000"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "groupBy",
      "agg",
      "F.avg",
      "orderBy",
      "F.desc"
    ],
    "hint": "F.avg('salary').alias('avg_salary')로 평균 계산 후 F.desc()로 정렬하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_union_combine_dataframes",
    "title": "두 DataFrame을 union으로 결합하기",
    "body": "df1과 df2를 union으로 결합하여 하나의 DataFrame으로 만드세요.",
    "schema": "name STRING, value INT",
    "sample_rows": [
      "A | 1",
      "B | 2"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "union"
    ],
    "hint": "df1.union(df2)로 결합하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_unionbyname_mismatched_columns",
    "title": "컬럼 순서가 다른 DataFrame을 unionByName으로 결합하기",
    "body": "df1과 df2의 컬럼 순서가 다를 때, unionByName로 결합하고 중복 행을 제거하세요.",
    "schema": "name STRING, age INT (df1)\nage INT, name STRING (df2)",
    "sample_rows": [
      "Alice | 25",
      "30 | Bob"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "unionByName",
      ".distinct()"
    ],
    "hint": "unionByName()로 컬럼명 기반 결합 후 distinct()로 중복 제거하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_agg_count_avg_multiple",
    "title": "전체 상품 수와 평균 가격 한 번에 조회하기",
    "body": "DataFrame에서 전체 상품 개수(count)와 평균 가격(avg_price)을 한 번의 agg()로 계산하세요.",
    "schema": "product STRING, price DECIMAL",
    "sample_rows": [
      "Laptop | 1299.99",
      "Mouse | 25.5"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "agg",
      "F.count",
      "F.avg"
    ],
    "hint": "agg(F.count('*').alias('count'), F.avg('price').alias('avg_price'))로 수행하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_agg_groupby_dept_stats",
    "title": "부서별 직원 수, 최소/최대/평균 급여 계산하기",
    "body": "department 기준으로 그룹화하여 직원 수, 최저 급여, 최고 급여, 평균 급여를 모두 계산하세요.",
    "schema": "name STRING, department STRING, salary DECIMAL",
    "sample_rows": [
      "Alice | Engineering | 80000",
      "Bob | Sales | 65000"
    ],
    "difficulty": "Lv3 중급",
    "kind": "python",
    "expected": [
      "groupBy",
      "agg",
      "F.count",
      "F.min",
      "F.max",
      "F.avg"
    ],
    "hint": "F.count('*').alias('count'), F.min('salary') 등 여러 함수를 agg() 안에 넣으세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_date_extract_year_month",
    "title": "주문 날짜에서 연도와 월 추출하기",
    "body": "order_date 컬럼에서 year과 month를 추출하여 새로운 컬럼으로 추가하세요.",
    "schema": "name STRING, order_date DATE",
    "sample_rows": [
      "Alice | 2024-03-15",
      "Bob | 2023-12-01"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "F.year",
      "F.month"
    ],
    "hint": "withColumn('year', F.year('order_date'))로 추출하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_date_days_ago_filter",
    "title": "현재 날짜와 주문 날짜 차이 계산 후 필터링하기",
    "body": "order_date와 현재 날짜의 차이를 days_ago 컬럼으로 추가하고, 30일 이상 지난 데이터만 필터링하세요.",
    "schema": "product STRING, order_date DATE",
    "sample_rows": [
      "Laptop | 2024-01-15",
      "Mouse | 2024-03-10"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "F.datediff",
      "F.current_date",
      "filter"
    ],
    "hint": "F.datediff(F.current_date(), 'order_date')로 날짜 차이 계산하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_coalesce_phone_display",
    "title": "전화번호 NULL 처리: N/A로 대체하기",
    "body": "phone 컬럼이 NULL인 경우 'N/A'로 대체한 phone_display 컬럼을 추가하세요.",
    "schema": "name STRING, phone STRING",
    "sample_rows": [
      "Alice | null",
      "Bob | 123-4567"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "F.coalesce",
      "F.lit"
    ],
    "hint": "F.coalesce('phone', F.lit('N/A'))로 처리하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_fillna_salary_null",
    "title": "NULL 값 채우기: salary는 0, department는 Unknown",
    "body": "salary가 NULL인 행은 0으로, department가 NULL인 행은 'Unknown'으로 fillna로 채우세요.",
    "schema": "name STRING, salary DOUBLE, department STRING",
    "sample_rows": [
      "Alice | null | Sales",
      "Bob | 60000 | null"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "fillna",
      "{'col': value}"
    ],
    "hint": "fillna({'salary': 0, 'department': 'Unknown'})로 처리하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_collect_high_score_students",
    "title": "80점 이상 학생을 collect하여 이름 리스트 만들기",
    "body": "score가 80 이상인 학생들을 collect()로 가져와, 이름만 담은 리스트를 만드세요.",
    "schema": "name STRING, score INT",
    "sample_rows": [
      "Alice | 95",
      "Bob | 75"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "filter",
      "collect()",
      "list comprehension"
    ],
    "hint": "for row in rows: name_list.append(row.name) 또는 리스트 컴프리헨션으로 처리하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_select_filter_null_check",
    "title": "가입 후 6개월 이내 신규 고객 조회: NULL 및 날짜 유효성 검사 포함",
    "body": "customers 테이블에서 가입일이 2024-01-01 이후이며, signup_date가 NULL인 행은 제외하고, 최근 가입일 순으로 정렬하세요. 실무에서는 잘못된 날짜 형식이나 NULL 값이 분석 결과에 영향을 줄 수 있으므로 주의해야 합니다.",
    "schema": "customers(id INT, name TEXT, signup_date DATE)",
    "sample_rows": [
      "1 | Alice | 2024-02-10",
      "2 | Bob | NULL",
      "3 | Casey | 2023-12-30"
    ],
    "difficulty": "Lv1 입문",
    "kind": "sql",
    "expected": [
      "SELECT",
      "WHERE",
      "IS NOT NULL",
      "ORDER BY"
    ],
    "hint": "signup_date IS NOT NULL 조건을 반드시 추가하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_aggregation_with_null_filter",
    "title": "월별 매출 합계: NULL 날짜 제외 및 포맷 정규화",
    "body": "sales 테이블에서 order_date가 NULL이 아닌 행만 대상으로 월별 총 매출액을 계산하세요. 결과는 'YYYY-MM' 형식의 month 컬럼과 total_sales로 출력하고, 월 기준 오름차순 정렬하세요.",
    "schema": "sales(order_date DATE, amount DECIMAL)",
    "sample_rows": [
      "2024-01-05 | 120000",
      "NULL | 98000",
      "2024-02-02 | 150000"
    ],
    "difficulty": "Lv2 초급",
    "kind": "sql",
    "expected": [
      "GROUP BY",
      "SUM",
      "DATE_TRUNC",
      "IS NOT NULL",
      "ORDER BY"
    ],
    "hint": "WHERE order_date IS NOT NULL로 데이터 정제 후 DATE_TRUNC('month', order_date)로 월 추출.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_join_with_null_handling",
    "title": "고객 주문 통계: 조인 시 NULL 값 제거 및 데이터 일관성 확보",
    "body": "customers(cust_id, name)와 orders(order_id, cust_id, amount) 테이블을 INNER JOIN하여 고객 이름과 총 주문 금액을 조회하세요. 단, orders.cust_id가 NULL인 행은 조인 대상에서 제외하고, 결과는 금액 내림차순으로 정렬하세요.",
    "schema": "customers(cust_id INT, name TEXT)\norders(order_id INT, cust_id INT, amount DECIMAL)",
    "sample_rows": [
      "customers: 1 | Alice",
      "customers: 2 | Bob",
      "orders: 10 | 1 | 50000",
      "orders: 11 | NULL | 30000",
      "orders: 12 | 2 | 45000"
    ],
    "difficulty": "Lv2 초급",
    "kind": "sql",
    "expected": [
      "JOIN",
      "ON",
      "IS NOT NULL",
      "GROUP BY",
      "SUM",
      "ORDER BY"
    ],
    "hint": "WHERE orders.cust_id IS NOT NULL로 조인 전 필터링이 필요합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_window_rank_with_ties",
    "title": "카테고리별 최고 상품 찾기: 동점 처리 및 복수 결과 반환",
    "body": "products(id, category, price) 테이블에서 카테고리별로 가장 비싼 제품을 찾아주세요. 그러나 가격이 같은 제품이 여러 개 있을 수 있으므로, **모든 동점 제품**을 포함하여 결과를 반환하세요 (RANK 사용). 결과는 id 오름차순으로 정렬.",
    "schema": "products(id INT, category TEXT, price DECIMAL)",
    "sample_rows": [
      "1 | electronics | 120000",
      "2 | electronics | 120000",
      "3 | books | 18000"
    ],
    "difficulty": "Lv3 중급",
    "kind": "sql",
    "expected": [
      "RANK() OVER(PARTITION BY category ORDER BY price DESC)",
      "WHERE rank = 1",
      "ORDER BY id"
    ],
    "hint": "ROW_NUMBER는 동점 시 하나만 반환하지만, RANK()는 모든 동점을 포함합니다.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_running_total_with_sorting",
    "title": "월별 누적 매출: 날짜 순서 오류 및 문자열 정렬 문제 해결",
    "body": "sales(month TEXT, amount DECIMAL)에서 월 기준으로 매출과 누적 매출 cum_sales를 계산하세요. 그러나 month 값이 '2024-03', '2024-01'처럼 문자열 순서로 정렬되지 않을 수 있으므로, 이를 **정확한 날짜 형식으로 변환하여 정렬**하고 누적합을 구하세요.",
    "schema": "sales(month TEXT, amount DECIMAL)",
    "sample_rows": [
      "2024-03 | 180000",
      "2024-01 | 200000",
      "2024-02 | 230000"
    ],
    "difficulty": "Lv3 중급",
    "kind": "sql",
    "expected": [
      "SUM",
      "OVER",
      "ORDER BY",
      "CAST AS DATE"
    ],
    "hint": "month를 DATE로 변환하여 정렬하고, SUM() OVER(ORDER BY)로 누적합 계산하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_pivot_sales_by_region",
    "title": "지역별 연도별 매출 피벗: 결측치를 0으로 채우기",
    "body": "sales(region, year, amount) 테이블에서 각 지역의 연도별 매출액을 피벗 형태로 출력하세요. 결과는 region과 year 오름차순 정렬하고, 결측치는 0으로 처리하세요.",
    "schema": "sales(region STRING, year INT, amount DECIMAL)",
    "sample_rows": [
      "North | 2023 | 10000",
      "South | 2024 | 15000"
    ],
    "difficulty": "Lv3 중급",
    "kind": "sql",
    "expected": [
      "PIVOT",
      "COALESCE",
      "ORDER BY"
    ],
    "hint": "GROUP BY region으로 집계 후 PIVOT(year)로 피벗, 결측치는 COALESCE(값, 0)로 처리.",
    "problem_type": "코딩"
  },
  {
    "pid": "sql_date_diff_with_condition",
    "title": "최근 90일 이내 활동한 고객 필터링: 날짜 차이 계산 및 조건 적용",
    "body": "customers 테이블에서 last_login이 현재 날짜로부터 90일 이내인 고객의 id와 name을 조회하세요. 단, last_login가 NULL인 경우는 제외하고, 결과는 최근 로그인 순으로 정렬하세요.",
    "schema": "customers(id INT, name TEXT, last_login DATE)",
    "sample_rows": [
      "1 | Alice | 2024-03-15",
      "2 | Bob | NULL",
      "3 | Casey | 2024-01-10"
    ],
    "difficulty": "Lv2 초급",
    "kind": "sql",
    "expected": [
      "WHERE",
      "DATEDIFF",
      "IS NOT NULL",
      "ORDER BY"
    ],
    "hint": "DATEDIFF(CURRENT_DATE(), last_login) <= 90 조건을 사용하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_filter_with_null",
    "title": "활성 사용자 필터링: NULL 값과 조건 복합 처리",
    "body": "users DataFrame에서 active가 True이고 last_login이 2024-01-01 이후인 사용자만 남기고 id와 last_login 컬럼을 선택하세요. 단, last_login가 NULL인 행은 제외하고, 결과는 최근 로그인 순으로 정렬하세요.",
    "schema": "users(id INT, active BOOLEAN, last_login TIMESTAMP)",
    "sample_rows": [
      "1 | true | 2024-02-03",
      "2 | false | 2023-12-10",
      "3 | true | null"
    ],
    "difficulty": "Lv1 입문",
    "kind": "python",
    "expected": [
      "filter",
      "select",
      "isNotNull()"
    ],
    "hint": "active == True와 last_login.isNotNull() 조건을 AND로 연결하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_join_with_filter",
    "title": "고객 주문 통계: 조인 후 조건 필터링 및 집계",
    "body": "customers와 orders DataFrame이 주어졌을 때, 고객별 주문 건수(order_count)와 총 금액(total_amount)을 계산하세요. 단, orders의 amount가 0 이하인 행은 제외하고, 결과는 total_amount 내림차순으로 정렬하세요.",
    "schema": "customers(cust_id INT, name STRING)\norders(order_id INT, cust_id INT, amount DOUBLE)",
    "sample_rows": [
      "customers: 1 | Alice",
      "customers: 2 | Bob",
      "orders: 10 | 1 | 50000.0",
      "orders: 11 | 1 | -10000.0",
      "orders: 12 | 2 | 10000.0"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "join",
      "filter",
      "groupBy",
      "agg",
      "count",
      "sum",
      "orderBy"
    ],
    "hint": "amount > 0 조건으로 필터링 후 집계하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_groupby_with_null_handling",
    "title": "부서별 평균 급여 계산: NULL 값 처리 및 잘못된 데이터 제거",
    "body": "employees DataFrame에서 salary가 NULL이 아닌 행만 대상으로 department 기준으로 평균 급여를 계산하고, 결과는 평균 내림차순 정렬하세요. 단, department가 NULL인 경우도 제외하세요.",
    "schema": "employees(department STRING, salary DOUBLE)",
    "sample_rows": [
      "Engineering | 80000",
      "Sales | null",
      "Marketing | 65000"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "filter",
      "groupBy",
      "agg",
      "F.avg",
      "orderBy"
    ],
    "hint": "df.filter(col('salary').isNotNull() & col('department').isNotNull())로 정제하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_window_rank_with_partition",
    "title": "부서별 급여 순위 매기기: 각 부서 내에서 최상위 직원 추출",
    "body": "employees DataFrame에서 department 기준으로 salary 내림차순으로 rank를 매기고, 각 부서의 1등만 추출하세요. 결과는 department와 salary 오름차순 정렬.",
    "schema": "employees(department STRING, salary INT)",
    "sample_rows": [
      "Engineering | 80000",
      "Sales | 75000",
      "Engineering | 70000"
    ],
    "difficulty": "Lv3 중급",
    "kind": "python",
    "expected": [
      "Window.partitionBy",
      "orderBy",
      "row_number()",
      "filter",
      "orderBy"
    ],
    "hint": "partitionBy('department')로 그룹화하고, rank == 1 조건으로 필터링하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_date_extract_and_filter",
    "title": "최근 30일 이내 주문 데이터 추출: 날짜 처리 및 조건 적용",
    "body": "orders DataFrame에서 order_date가 현재 날짜로부터 30일 이내인 행만 필터링하고, order_id와 order_date 컬럼을 선택하세요. 결과는 최근 주문 순으로 정렬.",
    "schema": "orders(order_id INT, order_date DATE)",
    "sample_rows": [
      "1 | 2024-03-15",
      "2 | 2024-02-10"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "filter",
      "datediff",
      "current_date",
      "orderBy"
    ],
    "hint": "F.datediff(F.current_date(), col('order_date')) <= 30 조건 사용.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_explode_with_null_check",
    "title": "구매 상품 배열 펼치기: NULL 및 빈 배열 처리 포함",
    "body": "orders DataFrame에서 products 컬럼(배열)을 explode하여 각 상품별 주문 기록을 생성하세요. 그러나 products가 NULL이거나 비어 있는 경우는 제외하고, 결과는 product 오름차순 정렬.",
    "schema": "orders(order_id INT, customer_id INT, products ARRAY<STRING>)",
    "sample_rows": [
      "1 | 101 | [Laptop, Mouse]",
      "2 | 102 | []",
      "3 | 103 | null"
    ],
    "difficulty": "Lv3 중급",
    "kind": "python",
    "expected": [
      "filter",
      "explode",
      "withColumn",
      "orderBy"
    ],
    "hint": "products가 NULL이 아니고 size(products) > 0 조건으로 필터링하세요.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_pivot_with_fillna",
    "title": "지역-카테고리별 판매량 피벗: 결측치를 0으로 채우기",
    "body": "sales(region STRING, category STRING, quantity INT) DataFrame에서 region과 category 기준으로 수량 합계를 피벗 테이블로 만들되, 결측치는 0으로 처리하세요. 결과는 region과 category 오름차순 정렬.",
    "schema": "sales(region STRING, category STRING, quantity INT)",
    "sample_rows": [
      "North | Electronics | 10",
      "South | Furniture | 5"
    ],
    "difficulty": "Lv3 중급",
    "kind": "python",
    "expected": [
      "groupBy",
      "pivot",
      "sum",
      "fillna"
    ],
    "hint": "pivot(category).agg(sum('quantity')) 후 fillna(0)로 결측치 처리.",
    "problem_type": "코딩"
  },
  {
    "pid": "pyspark_udf_null_safe_length",
    "title": "문자열 길이 UDF: NULL 값 안전 처리",
    "body": "name 컬럼의 문자열 길이를 계산하여 name_length 컬럼으로 추가하세요. 그러나 name이 NULL인 경우는 0을 반환하고, 함수는 효율적으로 작동해야 합니다.",
    "schema": "employees(name STRING, salary INT)",
    "sample_rows": [
      "Alice | 80000",
      "Bob | 75000",
      "null | 65000"
    ],
    "difficulty": "Lv2 초급",
    "kind": "python",
    "expected": [
      "udf",
      "StringType",
      "lambda s: len(s) if s is not None else 0"
    ],
    "hint": "UDF 내에서 NULL 체크 필수. 성능 고려할 것.",
    "problem_type": "코딩"
  },
  {
  "pid": "sql_null_cleaning_sales",
  "title": "매출 데이터의 NULL 정제 및 유효 범위 필터링",
  "body": "sales 테이블에서 매출 금액(amount)이 10,000원 이상이고, order_date가 2024년 1월 1일 이후인 행만 조회하세요. 그러나 amount가 NULL 또는 음수인 경우는 제외하고, 결과는 주문일 기준 오름차순으로 정렬해주세요.",
  "schema": "sales(order_id INT, product_name TEXT, order_date DATE, amount DECIMAL)",
  "sample_rows": [
    "1 | Laptop | 2024-01-05 | 150000",
    "2 | Mouse | 2023-12-28 | NULL",
    "3 | Keyboard | 2024-02-10 | -5000",
    "4 | Monitor | 2024-01-03 | 9000",
    "5 | Headset | 2024-03-01 | 28000"
  ],
  "difficulty": "Lv1 입문",
  "kind": "sql",
  "expected": [
    "SELECT",
    "WHERE",
    "IS NOT NULL",
    "BETWEEN",
    "AND",
    "ORDER BY"
  ],
  "hint": "amount가 NULL이거나 음수인 행은 WHERE 절에서 제거하세요. order_date는 '2024-01-01' 이후로 필터링해야 합니다.",
  "problem_type": "코딩"
},
{
  "pid": "sql_window_ranking_customer",
  "title": "고객별 주문 횟수 랭킹: 윈도우 함수를 활용한 순위 산정",
  "body": "orders 테이블에서 각 고객(customer_id)의 총 주문 횟수를 계산하고, 이를 기준으로 내림차순으로 랭크(순위)를 매기세요. 단, customer_id가 NULL인 행은 제외하고, 결과는 랭크 순서로 정렬하세요.",
  "schema": "orders(order_id INT, customer_id INT, order_date DATE)",
  "sample_rows": [
    "101 | 1 | 2024-03-05",
    "102 | 2 | 2024-03-06",
    "103 | NULL | 2024-03-07",
    "104 | 1 | 2024-03-08",
    "105 | 2 | 2024-03-09"
  ],
  "difficulty": "Lv2 초급",
  "kind": "sql",
  "expected": [
    "GROUP BY",
    "COUNT(*)",
    "RANK() OVER (ORDER BY)",
    "IS NOT NULL",
    "ORDER BY"
  ],
  "hint": "customer_id IS NOT NULL 조건을 추가하고, RANK() OVER를 사용해 순위를 계산하세요.",
  "problem_type": "코딩"
},
{
  "pid": "sql_pivot_daily_revenue",
  "title": "일별 매출 집계 및 피봇 테이블 변환: 실시간 분석용 데이터 준비",
  "body": "sales 테이블에서 각 날짜별로 매출 합계를 구하고, 이를 'YYYY-MM-DD' 형식으로 피봇하여 일자 별 매출을 열로 정리하세요. 단, 매출이 없는 날은 포함하지 않으며, 결과는 날짜 순으로 출력해주세요.",
  "schema": "sales(sale_id INT, sale_date DATE, revenue DECIMAL)",
  "sample_rows": [
    "1 | 2024-03-25 | 87000",
    "2 | 2024-03-25 | 65000",
    "3 | 2024-03-26 | 92000",
    "4 | 2024-03-27 | 105000"
  ],
  "difficulty": "Lv3 중급",
  "kind": "sql",
  "expected": [
    "GROUP BY",
    "SUM",
    "CASE WHEN",
    "PIVOT (aggregate)",
    "ORDER BY"
  ],
  "hint": "SQL에서 피봇은 CASE + SUM 또는 PIVOT 문법으로 구현됩니다. PostgreSQL에서는 crosstab, MySQL/Spark SQL에서는 CASE 문을 활용하세요.",
  "problem_type": "코딩"
},
{
  "pid": "pyspark_null_filtering_user",
  "title": "사용자 데이터의 NULL 제거 및 조건 기반 필터링",
  "body": "users_df DataFrame에서 name이 비어 있거나 NULL인 행은 제외하고, age가 18세 이상이며, city가 'Seoul' 또는 'Busan'인 사용자를 추출하세요. 결과는 나이 내림차순으로 정렬해주세요.",
  "schema": "users_df: user_id (INT), name (STRING), age (INT), city (STRING)",
  "sample_rows": [
    "1 | Alice | 25 | Seoul",
    "2 | Bob | NULL | Busan",
    "3 |   | 30 | Seoul",
    "4 | Charlie | 17 | Busan",
    "5 | Diana | 42 | Incheon"
  ],
  "difficulty": "Lv1 입문",
  "kind": "python",
  "expected": [
    "filter",
    "isNotNull()",
    "trim().eq(lit(''))",
    "isin",
    "gt",
    "orderBy"
  ],
  "hint": "name이 NULL 또는 빈 문자열인 경우는 trim(col('name')).eq(lit(''))로 검사하세요.",
  "problem_type": "코딩"
},
{
  "pid": "pyspark_window_cumulative",
  "title": "누적 매출 계산: 윈도우 함수를 활용한 실시간 데이터 분석",
  "body": "sales_df DataFrame에서 각 상품(product)별로 날짜 순서대로 누적 매출액(cumulative_revenue)을 계산하세요. 단, amount가 NULL이거나 음수인 경우는 제거하고, 결과는 product_name과 order_date 순으로 출력해주세요.",
  "schema": "sales_df: order_id (INT), product_name (STRING), order_date (DATE), amount (DECIMAL)",
  "sample_rows": [
    "101 | Laptop | 2024-03-25 | 80000",
    "102 | Mouse | 2024-03-26 | -5000",
    "103 | Keyboard | 2024-03-25 | 12000",
    "104 | Laptop | 2024-03-27 | 95000",
    "105 | Mouse | 2024-03-28 | 3000"
  ],
  "difficulty": "Lv2 초급",
  "kind": "python",
  "expected": [
    "filter",
    "isNotNull()",
    "gt",
    "window",
    "sum().over",
    "orderBy"
  ],
  "hint": "Window 함수를 사용해 partitionBy('product_name')하고, orderBy('order_date')로 순서 설정하세요.",
  "problem_type": "코딩"
},
{
  "pid": "pyspark_pivot_daily_sales",
  "title": "일별 매출 피봇 변환: 열 기반 데이터 재구성",
  "body": "sales_df DataFrame에서 각 날짜별로 총 매출액을 구하고, 이를 'YYYY-MM-DD' 형식의 날짜를 열로 정리하는 피봇 테이블을 생성하세요. 단, 매출이 없는 날은 포함하지 않으며, 결과는 날짜 순으로 출력해주세요.",
  "schema": "sales_df: sale_id (INT), sale_date (DATE), revenue (DECIMAL)",
  "sample_rows": [
    "1 | 2024-03-25 | 87000",
    "2 | 2024-03-25 | 65000",
    "3 | 2024-03-26 | 92000",
    "4 | 2024-03-27 | 105000"
  ],
  "difficulty": "Lv3 중급",
  "kind": "python",
  "expected": [
    "groupBy",
    "sum",
    "pivot",
    "agg",
    "orderBy"
  ],
  "hint": "PySpark에서는 pivot() 메서드를 사용해 특정 컬럼을 열로 변환할 수 있습니다. 예: pivot('sale_date')",
  "problem_type": "코딩"
},
{
  "pid": "sql_filter_active_users",
  "title": "활성 사용자 조회: 최근 로그인 기준 필터링 및 유효성 검증",
  "body": "users 테이블에서 최근 30일 이내에 최소 한 번 이상 로그인한 사용자를 조회하세요. 단, user_status가 'inactive'인 경우는 제외하고, 결과는 가장 최근 로그인일 기준 내림차순으로 정렬해주세요.",
  "schema": "users(user_id INT, name TEXT, user_status TEXT, last_login DATE)",
  "sample_rows": [
    "1 | Alice | active | 2024-03-25",
    "2 | Bob | inactive | 2024-03-10",
    "3 | Casey | active | NULL",
    "4 | David | active | 2024-02-28"
  ],
  "difficulty": "Lv1 입문",
  "kind": "sql",
  "expected": [
    "SELECT",
    "WHERE",
    "IS NOT NULL",
    "BETWEEN",
    "ORDER BY"
  ],
  "hint": "last_login이 NULL인 경우는 제외하고, 날짜 범위를 '2024-03-25' 기준으로 지난 30일로 설정하세요.",
  "problem_type": "코딩"
},
{
  "pid": "sql_aggregate_sales_by_region",
  "title": "지역별 매출 집계: NULL 값 제거 및 조건 그룹화",
  "body": "sales 테이블에서 region이 'North' 또는 'South'인 데이터만 대상으로, 각 지역의 총 매출액과 평균 매출을 계산하세요. 단, amount가 NULL이거나 0인 행은 제외하고, 결과는 매출 합계 내림차순으로 정렬해주세요.",
  "schema": "sales(sale_id INT, region TEXT, amount DECIMAL)",
  "sample_rows": [
    "1 | North | 50000",
    "2 | South | NULL",
    "3 | East | 80000",
    "4 | North | 75000",
    "5 | South | 0"
  ],
  "difficulty": "Lv2 초급",
  "kind": "sql",
  "expected": [
    "GROUP BY",
    "SUM",
    "AVG",
    "IS NOT NULL",
    "AND",
    "ORDER BY"
  ],
  "hint": "amount IS NOT NULL AND amount > 0 조건을 추가하고, region은 IN ('North', 'South')로 필터링하세요.",
  "problem_type": "코딩"
},
{
  "pid": "sql_join_customer_review",
  "title": "고객 리뷰 평균 점수 분석: 조인 및 NULL 처리 복합 문제",
  "body": "customers 테이블과 reviews 테이블을 조인하여, 고객 이름(name)과 해당 고객의 리뷰 평균 점수(avg_rating)를 조회하세요. 단, review_date가 NULL이거나 rating이 0점인 행은 제외하고, 결과는 평균 점수 내림차순으로 정렬해주세요.",
  "schema": "customers(cust_id INT, name TEXT)\nreviews(review_id INT, cust_id INT, rating DECIMAL, review_date DATE)",
  "sample_rows": [
    "customers: 101 | Alice",
    "customers: 102 | Bob",
    "reviews: 501 | 101 | 4.8 | 2024-03-20",
    "reviews: 502 | 101 | NULL | 2024-03-22", 
    "reviews: 503 | 102 | 0 | 2024-03-23"     
  ],
  "difficulty": "Lv3 중급",
  "kind": "sql",
  "expected": [
    "JOIN",
    "ON",
    "IS NOT NULL",
    "AND",
    "GROUP BY",
    "AVG",
    "ORDER BY"
  ],
  "hint": "WHERE 절에서 rating IS NOT NULL AND rating > 0 조건을 추가하세요.",
  "problem_type": "코딩"
},
{
  "pid": "sql_window_lead_lag_analysis",
  "title": "이전/다음 주문 간격 분석: 윈도우 함수 활용한 시간 기반 패턴 탐지",
  "body": "orders 테이블에서 각 고객별로 가장 최근 두 번의 주문 사이의 일수 차이를 계산하세요. 단, order_date가 NULL인 행은 제외하고, 결과는 고객 ID와 주문일 순으로 정렬해주세요.",
  "schema": "orders(order_id INT, customer_id INT, order_date DATE)",
  "sample_rows": [
    "101 | 201 | 2024-03-25",
    "102 | 201 | 2024-04-01",
    "103 | 202 | 2024-03-28",
    "104 | 202 | 2024-04-05"
  ],
  "difficulty": "Lv3 중급",
  "kind": "sql",
  "expected": [
    "OVER (PARTITION BY)",
    "ORDER BY",
    "LAG",
    "DATEDIFF",
    "IS NOT NULL",
    "ORDER BY"
  ],
  "hint": "window 함수를 사용해 LAG(order_date)로 이전 주문일을 가져온 후, DATEDIFF 계산하세요.",
  "problem_type": "코딩"
},
{
  "pid": "sql_pivot_category_sales",
  "title": "카테고리별 매출 피봇: 동적 컬럼 생성 및 통합 분석",
  "body": "sales 테이블에서 각 category의 총 매출액을 'YYYY-MM' 포맷으로 그룹화하여, 월별로 피봇 표 형식으로 출력하세요. 단, amount가 NULL인 행은 제외하고, 결과는 카테고리명 순서로 정렬해주세요.",
  "schema": "sales(sale_id INT, category TEXT, sale_date DATE, amount DECIMAL)",
  "sample_rows": [
    "1 | Electronics | 2024-03-05 | 80000",
    "2 | Clothing | 2024-03-12 | 60000",
    "3 | Electronics | 2024-04-01 | 95000",
    "4 | Furniture | 2024-04-08 | 75000"
  ],
  "difficulty": "Lv3 중급",
  "kind": "sql",
  "expected": [
    "GROUP BY",
    "SUM",
    "PIVOT",
    "DATE_TRUNC",
    "IS NOT NULL",
    "ORDER BY"
  ],
  "hint": "DATE_TRUNC('month', sale_date)로 월 기준 그룹화하고, PIVOT로 매출을 열로 전환하세요.",
  "problem_type": "코딩"
},
{
  "pid": "sql_conditional_aggregation",
  "title": "조건부 집계: 성별에 따른 구매 여부 분석",
  "body": "orders 테이블과 users 테이블을 조인하여, 남성(male)과 여성(female)의 총 주문 수를 각각 계산하세요. 단, user_status가 'suspended'인 경우는 제외하고, 결과는 성별 순서로 정렬해주세요.",
  "schema": "users(user_id INT, name TEXT, gender TEXT, user_status TEXT)\norders(order_id INT, user_id INT, order_date DATE)",
  "sample_rows": [
    "users: 1 | Alice | female | active",
    "users: 2 | Bob | male | suspended",
    "users: 3 | Carol | female | active",
    "orders: 101 | 1 | 2024-03-25",
    "orders: 102 | 3 | 2024-03-26"
  ],
  "difficulty": "Lv2 초급",
  "kind": "sql",
  "expected": [
    "JOIN",
    "ON",
    "WHERE",
    "CASE WHEN",
    "COUNT(*)",
    "GROUP BY",
    "ORDER BY"
  ],
  "hint": "gender가 'male' 또는 'female'인 경우에만 집계하고, CASE로 조건부 COUNT를 사용하세요.",
  "problem_type": "코딩"
},
{
  "pid": "pyspark_filter_invalid_records",
  "title": "잘못된 데이터 행 제거: 값 범위 및 형식 검사",
  "body": "transactions_df에서 amount가 음수이거나, currency가 'USD', 'KRW' 이외의 값인 행은 제거하세요. 결과는 transaction_date 기준 오름차순으로 정렬해주세요.",
  "schema": "transactions_df: txn_id (INT), amount (DECIMAL), currency (STRING), transaction_date (DATE)",
  "sample_rows": [
    "1 | 50000 | USD | 2024-03-25",
    "2 | -8000 | EUR | 2024-03-26",
    "3 | 120000 | JPY | 2024-03-27",
    "4 | 95000 | KRW | 2024-03-28"
  ],
  "difficulty": "Lv1 입문",
  "kind": "python",
  "expected": [
    "filter",
    "gt",
    "isin",
    "orderBy"
  ],
  "hint": "amount >= 0이고 currency가 ['USD', 'KRW'] 중 하나인 조건을 filter에 적용하세요.",
  "problem_type": "코딩"
},
{
  "pid": "pyspark_window_moving_avg",
  "title": "이동 평균 계산: 일별 매출의 7일 이동 평균 분석",
  "body": "sales_df에서 각 날짜별로 최근 7일간의 매출 합계를 기준으로 이동 평균을 계산하세요. 단, revenue가 NULL이거나 음수인 경우는 제거하고, 결과는 날짜 순서로 정렬해주세요.",
  "schema": "sales_df: sale_date DATE, revenue DECIMAL",
  "sample_rows": [
    "2024-03-25 | 87000",
    "2024-03-26 | 92000",
    "2024-03-27 | 105000",
    "2024-03-28 | -5000",
    "2024-03-29 | 110000"
  ],
  "difficulty": "Lv2 초급",
  "kind": "python",
  "expected": [
    "filter",
    "isNotNull()",
    "gt",
    "window",
    "avg().over",
    "orderBy"
  ],
  "hint": "Window 함수는 .partitionBy('sales_date')가 아닌, 일자 기준으로 정렬해 사용하세요.",
  "problem_type": "코딩"
},
{
  "pid": "pyspark_join_with_null_check",
  "title": "조인 시 NULL 값 처리: 외부 조인과 필터링 복합 문제",
  "body": "users_df와 profiles_df를 LEFT JOIN하여, 이름(name)과 생년월일(birth_date)을 조회하세요. 단, birth_date가 NULL이거나 비어 있는 경우는 결과에서 제거하고, 결과는 이름 기준 오름차순으로 정렬해주세요.",
  "schema": "users_df: user_id INT, name STRING\nprofiles_df: user_id INT, birth_date DATE",
  "sample_rows": [
    "1 | Alice",
    "2 | Bob",
    "3 | Charlie",
    "1 | 1990-05-12",
    "2 | NULL",
    "3 |   "
  ],
  "difficulty": "Lv3 중급",
  "kind": "python",
  "expected": [
    "join",
    "leftOuterJoin",
    "filter",
    "isNotNull()",
    "trim().eq(lit(''))",
    "orderBy"
  ],
  "hint": "birth_date가 NULL이거나 빈 문자열인 경우는 trim(col('birth_date')).eq(lit(''))로 검사하세요.",
  "problem_type": "코딩"
},
{
  "pid": "pyspark_pivot_sales_by_product",
  "title": "상품별 매출 피봇: 동적 열 생성 및 정렬",
  "body": "sales_df에서 각 상품(product_name)의 월별 총 매출액을 피봇하여, 'YYYY-MM' 형식으로 열로 전환하세요. 단, 매출이 없는 경우는 포함하지 않으며, 결과는 제품명 순서로 정렬해주세요.",
  "schema": "sales_df: product_name STRING, sale_date DATE, revenue DECIMAL",
  "sample_rows": [
    "Laptop | 2024-03-25 | 87000",
    "Mouse | 2024-03-26 | 92000",
    "Keyboard | 2024-04-01 | 105000"
  ],
  "difficulty": "Lv3 중급",
  "kind": "python",
  "expected": [
    "groupBy",
    "sum",
    "pivot",
    "agg",
    "orderBy"
  ],
  "hint": "pivot('sale_date')로 날짜를 열로 변환하고, agg(sum('revenue'))를 사용하세요.",
  "problem_type": "코딩"
},
{
  "pid": "pyspark_nested_filter_condition",
  "title": "복합 조건 필터링: 조건 중첩 처리 및 데이터 정제",
  "body": "events_df에서 event_type이 'click' 또는 'purchase'이고, user_id가 100 이상이며, session_duration이 30초 이상인 행만 추출하세요. 결과는 event_timestamp 기준 오름차순으로 정렬해주세요.",
  "schema": "events_df: event_id INT, user_id INT, event_type STRING, session_duration INT, event_timestamp TIMESTAMP",
  "sample_rows": [
    "1 | 99 | click | 25 | 2024-03-25 10:00:00",
    "2 | 101 | purchase | 45 | 2024-03-25 10:05:00",
    "3 | 102 | view | 60 | 2024-03-25 10:10:00"
  ],
  "difficulty": "Lv2 초급",
  "kind": "python",
  "expected": [
    "filter",
    "isin",
    "gt",
    "gte",
    "orderBy"
  ],
  "hint": "and 조건을 사용해 여러 필터를 연결하고, event_type은 in(['click', 'purchase'])로 처리하세요.",
  "problem_type": "코딩"
},
{
  "pid": "pyspark_string_cleaning",
  "title": "문자열 데이터 정제: 공백 제거 및 대소문자 통일",
  "body": "users_df에서 name 컬럼의 앞뒤 공백을 제거하고, 모든 글자를 소문자로 변환하여 새로운 'clean_name' 컬럼으로 생성하세요. 결과는 clean_name 기준 오름차순으로 정렬해주세요.",
  "schema": "users_df: user_id INT, name STRING",
  "sample_rows": [
    "1 |   Alice   ",
    "2 | Bob",
    "3 |   CHARLIE "
  ],
  "difficulty": "Lv1 입문",
  "kind": "python",
  "expected": [
    "withColumn",
    "trim",
    "lower",
    "orderBy"
  ],
  "hint": "trim(col('name'))으로 공백 제거하고, lower()로 소문자 변환하세요.",
  "problem_type": "코딩"
},
{
  "pid": "sql_filter_active_subscription",
  "title": "구독 상태별 활성 사용자 필터링: 조건 복합 처리",
  "body": "users 테이블에서 구독 상태(subscription_status)가 'active'이며, last_payment_date가 2024년 1월 1일 이후인 사용자를 조회하세요. 단, email이 NULL이거나 빈 문자열인 경우는 제외하고, 결과는 가입일 기준 오름차순으로 정렬해주세요.",
  "schema": "users(user_id INT, name TEXT, subscription_status TEXT, last_payment_date DATE, email TEXT)",
  "sample_rows": [
    "1 | Alice | active | 2024-03-25 | alice@domain.com",
    "2 | Bob | inactive | 2024-02-10 | NULL",
    "3 | Casey | active | 2023-12-31 |   ",
    "4 | David | active | 2024-01-05 | david@domain.com"
  ],
  "difficulty": "Lv1 입문",
  "kind": "sql",
  "expected": [
    "SELECT",
    "WHERE",
    "IS NOT NULL",
    "TRIM",
    "AND",
    "ORDER BY"
  ],
  "hint": "email이 NULL이거나 빈 문자열인 경우는 TRIM(email) = '' 로 확인하세요.",
  "problem_type": "코딩"
},
{
  "pid": "sql_groupby_with_having",
  "title": "고객별 주문 횟수 분석: 그룹화 후 조건 필터링",
  "body": "orders 테이블에서 customer_id가 NULL이 아닌 고객 중, 총 주문 횟수가 3회 이상인 고객의 이름(name)과 주문 수를 조회하세요. 단, order_date는 2024년 이전 데이터는 제외하고 결과는 주문 수 내림차순으로 정렬해주세요.",
  "schema": "orders(order_id INT, customer_id INT, order_date DATE)\nusers(user_id INT, name TEXT)",
  "sample_rows": [
    "101 | 501 | 2024-03-25",
    "102 | 501 | 2024-03-26",
    "103 | 501 | 2024-03-27",
    "104 | 502 | 2024-03-28",
    "105 | NULL | 2024-03-29"
  ],
  "difficulty": "Lv2 초급",
  "kind": "sql",
  "expected": [
    "JOIN",
    "ON",
    "GROUP BY",
    "HAVING",
    "COUNT(*)",
    "ORDER BY"
  ],
  "hint": "그룹화 후 조건을 걸려면 HAVING 절을 사용하세요. WHERE는 그룹 전 필터입니다.",
  "problem_type": "코딩"
},
{
  "pid": "sql_window_lag_with_condition",
  "title": "이전 주문 일자 비교: 조건부 윈도우 함수 적용",
  "body": "orders 테이블에서 각 고객의 최근 두 번의 주문 사이의 일수 차이를 계산하세요. 단, order_date가 NULL인 경우는 제외하고, 결과는 customer_id와 order_date 순으로 정렬해주세요.",
  "schema": "orders(order_id INT, customer_id INT, order_date DATE)",
  "sample_rows": [
    "1 | 201 | 2024-03-25",
    "2 | 201 | 2024-04-01",
    "3 | 202 | 2024-03-28",
    "4 | 202 | 2024-04-05"
  ],
  "difficulty": "Lv3 중급",
  "kind": "sql",
  "expected": [
    "OVER (PARTITION BY)",
    "ORDER BY",
    "LAG",
    "DATEDIFF",
    "IS NOT NULL",
    "ORDER BY"
  ],
  "hint": "window 함수를 사용해 LAG(order_date)로 이전 주문일을 가져온 후, DATEDIFF 계산하세요.",
  "problem_type": "코딩"
},
{
  "pid": "sql_pivot_sales_by_category_month",
  "title": "카테고리별 월간 매출 피봇: 동적 집계 및 시각화 준비",
  "body": "sales 테이블에서 category와 sale_date를 기준으로 각 카테고리의 월별 총 매출액을 피봇하여 출력하세요. 단, amount가 NULL인 행은 제외하고 결과는 카테고리명 순서로 정렬해주세요.",
  "schema": "sales(sale_id INT, category TEXT, sale_date DATE, amount DECIMAL)",
  "sample_rows": [
    "1 | Electronics | 2024-03-05 | 87000",
    "2 | Clothing | 2024-03-12 | 65000",
    "3 | Electronics | 2024-04-01 | 95000"
  ],
  "difficulty": "Lv3 중급",
  "kind": "sql",
  "expected": [
    "GROUP BY",
    "SUM",
    "PIVOT",
    "DATE_TRUNC",
    "IS NOT NULL",
    "ORDER BY"
  ],
  "hint": "DATE_TRUNC('month', sale_date)로 월 그룹화 후 PIVOT 사용하세요.",
  "problem_type": "코딩"
},
{
  "pid": "sql_conditional_case",
  "title": "성과 등급 분류: 조건부 컬럼 생성",
  "body": "employees 테이블에서 매출 금액(amount)에 따라 'High', 'Medium', 'Low'로 성과 등급을 부여하세요. 단, amount가 NULL인 경우는 'Unknown'으로 처리하고, 결과는 이름 순서로 정렬해주세요.",
  "schema": "employees(emp_id INT, name TEXT, department TEXT, amount DECIMAL)",
  "sample_rows": [
    "1 | Alice | Sales | 950000",
    "2 | Bob | Marketing | 480000",
    "3 | Casey | HR | NULL"
  ],
  "difficulty": "Lv2 초급",
  "kind": "sql",
  "expected": [
    "CASE WHEN",
    "IS NOT NULL",
    "THEN ELSE END",
    "ORDER BY"
  ],
  "hint": "amount > 700000 → 'High', 300000 ~ 699999 → 'Medium', 그 외 → 'Low'. NULL은 별도 처리.",
  "problem_type": "코딩"
},
{
  "pid": "sql_multiple_join_analysis",
  "title": "다중 조인을 통한 고객-주문-상품 분석",
  "body": "customers, orders, products 테이블을 조인하여 고객 이름(name), 주문 번호(order_id), 상품명(product_name)을 조회하세요. 단, order_date가 NULL이거나 amount가 0인 경우는 제외하고 결과는 주문일 내림차순으로 정렬해주세요.",
  "schema": "customers(cust_id INT, name TEXT)\norders(order_id INT, cust_id INT, product_id INT, order_date DATE, amount DECIMAL)\nproducts(product_id INT, product_name TEXT)",
  "sample_rows": [
    "1 | Alice",
    "2 | Bob",
    "101 | 1 | 501 | 2024-03-25 | 87000",
    "102 | 2 | 502 | NULL | 10000",
    "501 | Laptop"
  ],
  "difficulty": "Lv3 중급",
  "kind": "sql",
  "expected": [
    "JOIN",
    "ON",
    "WHERE",
    "IS NOT NULL",
    "AND",
    "ORDER BY"
  ],
  "hint": "여러 테이블을 연속 JOIN하고, WHERE 절에서 order_date IS NOT NULL AND amount > 0 조건 추가.",
  "problem_type": "코딩"
},
{
  "pid": "sql_subquery_with_exists",
  "title": "존재 여부 기반 필터링: 서브쿼리 활용",
  "body": "users 테이블에서 최근 30일 내에 로그인한 사용자만 조회하세요. 단, login_logs 테이블과 EXISTS를 사용하여 구현하고, 결과는 user_id 순으로 정렬해주세요.",
  "schema": "users(user_id INT, name TEXT)\nlogin_logs(user_id INT, login_date DATE)",
  "sample_rows": [
    "1 | Alice",
    "2 | Bob",
    "3 | Casey",
    "1 | 2024-03-25",
    "2 | 2024-03-26"
  ],
  "difficulty": "Lv2 초급",
  "kind": "sql",
  "expected": [
    "SELECT",
    "FROM",
    "WHERE EXISTS",
    "subquery",
    "ORDER BY"
  ],
  "hint": "EXISTS로 login_logs에서 해당 user_id의 최근 로그인 여부를 확인하세요.",
  "problem_type": "코딩"
},
{
  "pid": "sql_json_extract",
  "title": "JSON 컬럼 분석: JSON 데이터 추출 및 필터링",
  "body": "users 테이블에서 profile_info라는 JSON 형식의 컬럼을 사용하여, city가 'Seoul'인 사용자만 조회하세요. 단, profile_info가 NULL이거나 유효한 JSON이 아닌 경우는 제외하고 결과는 이름 순으로 정렬해주세요.",
  "schema": "users(user_id INT, name TEXT, profile_info JSON)",
  "sample_rows": [
    "1 | Alice | {\"city\": \"Seoul\", \"age\": 30}",
    "2 | Bob | {\"city\": \"Busan\", \"age\": 25}",
    "3 | Casey | NULL",
    "4 | David | \"{invalid json}\""
  ],
  "difficulty": "Lv3 중급",
  "kind": "sql",
  "expected": [
    "SELECT",
    "JSON_EXTRACT",
    "IS NOT NULL",
    "AND",
    "ORDER BY"
  ],
  "hint": "JSON_EXTRACT(profile_info, '$.city') = 'Seoul' 조건으로 필터링하세요.",
  "problem_type": "코딩"
},
{
  "pid": "sql_cte_with_ranking",
  "title": "CTE를 이용한 순위 산정 및 분석",
  "body": "sales 테이블에서 각 카테고리별 매출 합계를 구하고, 이를 기준으로 내림차순으로 랭크를 매기세요. 단, 총 매출액이 100만 원 미만인 카테고리는 제외하고 결과는 랭크 순서로 정렬해주세요.",
  "schema": "sales(sale_id INT, category TEXT, amount DECIMAL)",
  "sample_rows": [
    "1 | Electronics | 870000",
    "2 | Clothing | 560000",
    "3 | Furniture | 98000"
  ],
  "difficulty": "Lv3 중급",
  "kind": "sql",
  "expected": [
    "WITH CTE",
    "GROUP BY",
    "HAVING",
    "RANK() OVER (ORDER BY)",
    "ORDER BY"
  ],
  "hint": "CTE에서 GROUP BY 후 HAVING SUM(amount) >= 1000000 조건을 추가하세요.",
  "problem_type": "코딩"
},
{
  "pid": "sql_date_trunc_with_offset",
  "title": "시간 기반 그룹화: 특정 주기로 데이터 집계",
  "body": "events 테이블에서 event_timestamp를 기준으로 '15분 단위'로 이벤트 수를 집계하세요. 결과는 그룹 기준 시간 순서로 정렬해주세요.",
  "schema": "events(event_id INT, event_name TEXT, event_timestamp TIMESTAMP)",
  "sample_rows": [
    "1 | login | 2024-03-25 10:00:30",
    "2 | click | 2024-03-25 10:15:45",
    "3 | purchase | 2024-03-25 10:00:10"
  ],
  "difficulty": "Lv2 초급",
  "kind": "sql",
  "expected": [
    "DATE_TRUNC",
    "GROUP BY",
    "COUNT(*)",
    "ORDER BY"
  ],
  "hint": "DATE_TRUNC('minute', event_timestamp)를 사용하고, 분을 15로 나누어 그룹화하세요.",
  "problem_type": "코딩"
},
{
  "pid": "sql_cross_join_analysis",
  "title": "크로스 조인을 통한 조합 분석: 상품-지역 매칭",
  "body": "products 테이블과 regions 테이블의 크로스 조인을 통해 모든 상품과 지역의 조합을 생성하세요. 단, 결과는 product_name과 region 순으로 정렬해주세요.",
  "schema": "products(product_id INT, product_name TEXT)\nregions(region_id INT, name TEXT)",
  "sample_rows": [
    "1 | Laptop",
    "2 | Mouse",
    "101 | Seoul",
    "102 | Busan"
  ],
  "difficulty": "Lv3 중급",
  "kind": "sql",
  "expected": [
    "CROSS JOIN",
    "SELECT",
    "ORDER BY"
  ],
  "hint": "JOIN 없이 CROSS JOIN만으로 모든 조합을 생성하세요.",
  "problem_type": "코딩"
},
{
  "pid": "sql_recursive_cte",
  "title": "계층 구조 탐색: 직원 계층 구조 조회",
  "body": "employees 테이블에서 부서장(leader_id = NULL)으로부터 모든 하위 직원까지의 계층 관계를 재귀적으로 추출하세요. 결과는 employee_id 순서로 정렬해주세요.",
  "schema": "employees(emp_id INT, name TEXT, leader_id INT)",
  "sample_rows": [
    "1 | Alice | NULL",
    "2 | Bob | 1",
    "3 | Casey | 2",
    "4 | David | 1"
  ],
  "difficulty": "Lv3 중급",
  "kind": "sql",
  "expected": [
    "WITH RECURSIVE CTE",
    "UNION ALL",
    "ORDER BY"
  ],
  "hint": "RECURSIVE CTE를 사용해 root부터 하위까지 탐색하세요.",
  "problem_type": "코딩"
},
{
  "pid": "pyspark_drop_duplicates",
  "title": "중복 행 제거: 복수 컬럼 기준 중복 제거",
  "body": "transactions_df에서 user_id와 transaction_date가 동일한 행은 중복으로 간주하여 하나만 남기고 나머지는 제거하세요. 결과는 transaction_date 오름차순으로 정렬해주세요.",
  "schema": "transactions_df: txn_id INT, user_id INT, transaction_date DATE",
  "sample_rows": [
    "1 | 101 | 2024-03-25",
    "2 | 101 | 2024-03-25",
    "3 | 102 | 2024-03-26"
  ],
  "difficulty": "Lv1 입문",
  "kind": "python",
  "expected": [
    "dropDuplicates",
    "orderBy"
  ],
  "hint": "dropDuplicates(['user_id', 'transaction_date'])로 중복 제거하세요.",
  "problem_type": "코딩"
},
{
  "pid": "pyspark_cast_datetime",
  "title": "타입 변환: 문자열 날짜를 DATE 형식으로 변환",
  "body": "sales_df에서 sale_date가 STRING 타입인 경우, 이를 DATE 타입으로 변환하고 '2024-03' 이후의 데이터만 추출하세요. 결과는 sale_date 기준 오름차순 정렬해주세요.",
  "schema": "sales_df: sale_id INT, sale_date STRING, revenue DECIMAL",
  "sample_rows": [
    "1 | 2024-03-25 | 87000",
    "2 | 2024-02-28 | 65000"
  ],
  "difficulty": "Lv1 입문",
  "kind": "python",
  "expected": [
    "withColumn",
    "to_date",
    "cast",
    "filter",
    "orderBy"
  ],
  "hint": "to_date(col('sale_date'), 'yyyy-MM-dd')로 형식 지정 후 변환하세요.",
  "problem_type": "코딩"
},
{
  "pid": "pyspark_window_percentile",
  "title": "상위 10% 매출 고객 식별: 백분위수 기반 분석",
  "body": "sales_df에서 각 고객의 총 매출액을 계산하고, 상위 10%에 해당하는 고객만 추출하세요. 결과는 고객명과 총 매출 순으로 정렬해주세요.",
  "schema": "sales_df: customer_id INT, name STRING, revenue DECIMAL",
  "sample_rows": [
    "1 | Alice | 87000",
    "2 | Bob | 95000",
    "3 | Charlie | 65000"
  ],
  "difficulty": "Lv3 중급",
  "kind": "python",
  "expected": [
    "groupBy",
    "sum",
    "percent_rank().over",
    "filter",
    "orderBy"
  ],
  "hint": "percent_rank() over window을 사용해 상위 10%를 결정하세요.",
  "problem_type": "코딩"
},
{
  "pid": "pyspark_pivot_with_aggregate",
  "title": "피봇 후 집계: 여러 집계 함수 적용",
  "body": "sales_df에서 category별로 월별 총 매출액과 주문 수를 피봇하여 출력하세요. 결과는 category 순으로 정렬해주세요.",
  "schema": "sales_df: category STRING, sale_date DATE, revenue DECIMAL, order_id INT",
  "sample_rows": [
    "Electronics | 2024-03-25 | 87000 | 101",
    "Clothing | 2024-03-26 | 92000 | 102"
  ],
  "difficulty": "Lv3 중급",
  "kind": "python",
  "expected": [
    "pivot",
    "agg",
    "sum",
    "count",
    "orderBy"
  ],
  "hint": "agg(sum('revenue'), count('order_id'))로 여러 함수 동시 적용.",
  "problem_type": "코딩"
},
{
  "pid": "pyspark_string_split",
  "title": "문자열 분할 및 컬럼 생성: 주소에서 도시 추출",
  "body": "users_df에서 address 컬럼을 기준으로 도시(city)를 분리하여 새로운 'city' 컬럼을 생성하세요. 예: 'Seoul, Korea' → 'Seoul'. 결과는 city 기준 오름차순 정렬해주세요.",
  "schema": "users_df: user_id INT, name STRING, address STRING",
  "sample_rows": [
    "1 | Alice | Seoul, Korea",
    "2 | Bob | Busan, Korea"
  ],
  "difficulty": "Lv2 초급",
  "kind": "python",
  "expected": [
    "withColumn",
    "split",
    "getItem",
    "orderBy"
  ],
  "hint": "split(col('address'), ',')[0]으로 첫 번째 요소 추출.",
  "problem_type": "코딩"
},
{
  "pid": "pyspark_case_when",
  "title": "조건부 컬럼 생성: 연령대 분류",
  "body": "users_df에서 age 컬럼을 기준으로 'Teen', 'Adult', 'Senior'로 나이 그룹을 분류하세요. 단, age가 NULL인 경우는 'Unknown'으로 처리하고 결과는 이름 순으로 정렬해주세요.",
  "schema": "users_df: user_id INT, name STRING, age INT",
  "sample_rows": [
    "1 | Alice | 25",
    "2 | Bob | 67",
    "3 | Casey | NULL"
  ],
  "difficulty": "Lv2 초급",
  "kind": "python",
  "expected": [
    "when",
    ".otherwise",
    "orderBy"
  ],
  "hint": "when(col('age').between(13,19), 'Teen') ~ otherwise('Unknown').",
  "problem_type": "코딩"
},
{
  "pid": "pyspark_union_all_analysis",
  "title": "데이터 병합: 여러 데이터프레임 합치기",
  "body": "sales_q1_df와 sales_q2_df를 UNION ALL로 결합하여 총 매출 기록을 생성하세요. 결과는 sale_date 오름차순으로 정렬해주세요.",
  "schema": "sales_q1_df: sale_id INT, sale_date DATE, revenue DECIMAL\nsales_q2_df: same schema",
  "sample_rows": [
    "1 | 2024-01-05 | 87000",
    "2 | 2024-04-03 | 95000"
  ],
  "difficulty": "Lv1 입문",
  "kind": "python",
  "expected": [
    "unionAll",
    "orderBy"
  ],
  "hint": ".unionAll(df1, df2) 또는 .union(df1, df2)로 병합.",
  "problem_type": "코딩"
},
{
  "pid": "pyspark_regex_extract",
  "title": "정규 표현식을 통한 데이터 추출: 전화번호 형식 정제",
  "body": "users_df에서 phone_number 컬럼에서 숫자만 추출하여 'cleaned_phone' 컬럼으로 생성하세요. 예: '+82-10-1234-5678' → '821012345678'. 결과는 name 순으로 정렬해주세요.",
  "schema": "users_df: user_id INT, name STRING, phone_number STRING",
  "sample_rows": [
    "1 | Alice | +82-10-1234-5678",
    "2 | Bob | (02)1234-5678"
  ],
  "difficulty": "Lv2 초급",
  "kind": "python",
  "expected": [
    "regexp_extract",
    "withColumn",
    "orderBy"
  ],
  "hint": "regexp_extract(col('phone_number'), '\\d+', 0)로 숫자 추출.",
  "problem_type": "코딩"
},
{
  "pid": "pyspark_nested_struct",
  "title": "구조체 컬럼 분석: JSON 내부 값 접근",
  "body": "users_df에서 profile 컬럼이 Struct 타입이며, nested 필드 (city, age)를 추출하세요. 결과는 city 기준 오름차순 정렬해주세요.",
  "schema": "users_df: user_id INT, name STRING, profile STRUCT<city: STRING, age: INT>",
  "sample_rows": [
    "1 | Alice | {\"city\": \"Seoul\", \"age\": 30}",
    "2 | Bob | {\"city\": \"Busan\", \"age\": 25}"
  ],
  "difficulty": "Lv3 중급",
  "kind": "python",
  "expected": [
    "select",
    "col('profile.city')",
    "col('profile.age')",
    "orderBy"
  ],
  "hint": "col('profile.city')로 내부 필드 접근 가능.",
  "problem_type": "코딩"
},
{
  "pid": "pyspark_window_lead_lag",
  "title": "이전/다음 행 참조: 시계열 데이터 분석",
  "body": "stock_prices_df에서 각 날짜의 다음 거래일 가격과의 차이를 계산하세요. 결과는 날짜 순서로 정렬해주세요.",
  "schema": "stock_prices_df: date DATE, price DECIMAL",
  "sample_rows": [
    "2024-03-25 | 87000",
    "2024-03-26 | 92000"
  ],
  "difficulty": "Lv3 중급",
  "kind": "python",
  "expected": [
    "window",
    "lead().over",
    "withColumn",
    "orderBy"
  ],
  "hint": "Window.orderBy('date')로 정렬 후, lead(col('price')) 사용.",
  "problem_type": "코딩"
},
{
  "pid": "pyspark_coalesce_nulls",
  "title": "NULL 값 보완: 복수 컬럼에서 유효값 우선 추출",
  "body": "users_df에서 first_name, middle_name, last_name 중 최초로 비어 있지 않은 값을 'full_name'으로 생성하세요. 결과는 full_name 기준 오름차순 정렬해주세요.",
  "schema": "users_df: user_id INT, first_name STRING, middle_name STRING, last_name STRING",
  "sample_rows": [
    "1 | Alice | NULL | Smith",
    "2 | Bob | John | NULL"
  ],
  "difficulty": "Lv2 초급",
  "kind": "python",
  "expected": [
    "coalesce",
    "withColumn",
    "orderBy"
  ],
  "hint": "coalesce(col('first_name'), col('middle_name'), col('last_name')) 사용.",
  "problem_type": "코딩"
},
{
  "pid": "pyspark_session_window",
  "title": "세션 윈도우: 클릭 간격 기반 세션 분리",
  "body": "events_df에서 user_id별로 동일한 사용자인 경우, 이벤트 간격이 30분 이상 차이나면 새로운 세션으로 간주하세요. 결과는 user_id와 session_start_date 순으로 정렬해주세요.",
  "schema": "events_df: user_id INT, event_timestamp TIMESTAMP",
  "sample_rows": [
    "1 | 2024-03-25 10:00:00",
    "1 | 2024-03-25 10:25:00",
    "1 | 2024-03-25 11:10:00"
  ],
  "difficulty": "Lv3 중급",
  "kind": "python",
  "expected": [
    "window",
    "lag",
    "when",
    "sum().over",
    "orderBy"
  ],
  "hint": "session_id를 lag와 sum으로 계산하는 복잡한 윈도우 문제.",
  "problem_type": "코딩"
},
{
  "pid": "sql_select_all_columns",
  "title": "모든 컬럼 조회: 기본 SELECT 문법 연습",
  "body": "products 테이블에서 모든 행과 모든 컬럼을 조회하세요. 결과는 product_id 기준 오름차순으로 정렬해주세요.",
  "schema": "products(product_id INT, name TEXT, price DECIMAL)",
  "sample_rows": [
    "101 | Laptop | 950000",
    "102 | Mouse | 35000"
  ],
  "difficulty": "Lv1 입문",
  "kind": "sql",
  "expected": [
    "SELECT *",
    "FROM",
    "ORDER BY"
  ],
  "hint": "모든 컬럼은 '*' 기호로 지정할 수 있습니다.",
  "problem_type": "코딩"
},
{
  "pid": "sql_filter_by_value",
  "title": "값 기준 필터링: 특정 상품 조회",
  "body": "products 테이블에서 가격(price)이 50,000원 이상인 제품만 조회하세요. 결과는 가격 내림차순으로 정렬해주세요.",
  "schema": "products(product_id INT, name TEXT, price DECIMAL)",
  "sample_rows": [
    "101 | Laptop | 950000",
    "102 | Mouse | 35000",
    "103 | Keyboard | 78000"
  ],
  "difficulty": "Lv1 입문",
  "kind": "sql",
  "expected": [
    "SELECT",
    "WHERE",
    "ORDER BY"
  ],
  "hint": "price >= 50000 조건을 사용하세요.",
  "problem_type": "코딩"
},
{
  "pid": "sql_filter_null_values",
  "title": "NULL 값 제거: 필수 컬럼의 유효성 검사",
  "body": "orders 테이블에서 customer_id가 NULL이 아닌 주문만 조회하세요. 결과는 order_date 기준 오름차순으로 정렬해주세요.",
  "schema": "orders(order_id INT, customer_id INT, order_date DATE)",
  "sample_rows": [
    "101 | 501 | 2024-03-25",
    "102 | NULL | 2024-03-26",
    "103 | 502 | 2024-03-27"
  ],
  "difficulty": "Lv1 입문",
  "kind": "sql",
  "expected": [
    "SELECT",
    "WHERE",
    "IS NOT NULL",
    "ORDER BY"
  ],
  "hint": "customer_id IS NOT NULL 조건을 사용하세요.",
  "problem_type": "코딩"
},{
  "pid": "sql_multiple_conditions",
  "title": "복합 조건 필터링: 여러 조건 함께 적용",
  "body": "sales 테이블에서 판매일(sale_date)이 '2024-03-25'이고, 매출액(amount)이 100,000원 이상인 행만 조회하세요. 결과는 order_id 기준 오름차순으로 정렬해주세요.",
  "schema": "sales(sale_id INT, sale_date DATE, amount DECIMAL)",
  "sample_rows": [
    "1 | 2024-03-25 | 87000",
    "2 | 2024-03-25 | 95000",
    "3 | 2024-03-26 | 120000"
  ],
  "difficulty": "Lv2 초급",
  "kind": "sql",
  "expected": [
    "SELECT",
    "WHERE",
    "AND",
    "ORDER BY"
  ],
  "hint": "여러 조건은 AND로 연결하세요.",
  "problem_type": "코딩"
},{
  "pid": "sql_like_pattern_search",
  "title": "LIKE를 이용한 패턴 검색: 이름 부분 일치 조회",
  "body": "users 테이블에서 이름(name)에 'A'가 포함된 사용자만 조회하세요. 결과는 이름 기준 오름차순으로 정렬해주세요.",
  "schema": "users(user_id INT, name TEXT)",
  "sample_rows": [
    "1 | Alice",
    "2 | Bob",
    "3 | Anna"
  ],
  "difficulty": "Lv1 입문",
  "kind": "sql",
  "expected": [
    "SELECT",
    "WHERE",
    "LIKE",
    "ORDER BY"
  ],
  "hint": "%A%로 'A'를 포함하는 모든 문자열을 찾습니다.",
  "problem_type": "코딩"
},{
  "pid": "sql_in_clause_filter",
  "title": "IN 연산자 사용: 다수 값 필터링",
  "body": "products 테이블에서 제품명(name)이 'Laptop', 'Mouse', 또는 'Keyboard'인 제품만 조회하세요. 결과는 제품 ID 오름차순으로 정렬해주세요.",
  "schema": "products(product_id INT, name TEXT)",
  "sample_rows": [
    "101 | Laptop",
    "102 | Mouse",
    "103 | Chair"
  ],
  "difficulty": "Lv2 초급",
  "kind": "sql",
  "expected": [
    "SELECT",
    "WHERE",
    "IN",
    "ORDER BY"
  ],
  "hint": "name IN ('Laptop', 'Mouse', 'Keyboard')로 조건 설정.",
  "problem_type": "코딩"
},{
  "pid": "sql_count_rows",
  "title": "행 수 세기: 전체 행 개수 확인",
  "body": "orders 테이블의 총 주문 수를 계산하세요. 결과는 하나의 숫자 값으로 반환되며, 컬럼명은 'total_orders'로 지정하세요.",
  "schema": "orders(order_id INT, customer_id INT)",
  "sample_rows": [
    "1 | 501",
    "2 | 502"
  ],
  "difficulty": "Lv1 입문",
  "kind": "sql",
  "expected": [
    "SELECT",
    "COUNT(*)",
    "AS"
  ],
  "hint": "COUNT(*)는 모든 행 수를 세어줍니다.",
  "problem_type": "코딩"
},{
  "pid": "sql_sum_aggregation",
  "title": "합계 집계: 매출 총액 계산",
  "body": "sales 테이블에서 전체 매출 금액의 합계를 계산하세요. 결과 컬럼명은 'total_revenue'로 지정하고, 정수형으로 출력해주세요.",
  "schema": "sales(sale_id INT, amount DECIMAL)",
  "sample_rows": [
    "1 | 87000",
    "2 | 95000"
  ],
  "difficulty": "Lv1 입문",
  "kind": "sql",
  "expected": [
    "SELECT",
    "SUM",
    "AS"
  ],
  "hint": "SUM(amount)를 사용하고, 결과는 정수로 변환할 필요 없이 DECIMAL 그대로 출력 가능.",
  "problem_type": "코딩"
},{
  "pid": "sql_average_price",
  "title": "평균 계산: 제품 가격 평균 산출",
  "body": "products 테이블에서 모든 제품의 평균 가격을 계산하세요. 결과 컬럼명은 'avg_price'로 지정하고, 소수점 둘째자리까지 표시해주세요.",
  "schema": "products(product_id INT, name TEXT, price DECIMAL)",
  "sample_rows": [
    "101 | Laptop | 950000",
    "102 | Mouse | 35000"
  ],
  "difficulty": "Lv2 초급",
  "kind": "sql",
  "expected": [
    "SELECT",
    "AVG",
    "AS",
    "ROUND"
  ],
  "hint": "AVG(price)로 평균 계산하고, ROUND(..., 2)로 소수점 둘째자리까지 표시.",
  "problem_type": "코딩"
},{
  "pid": "sql_groupby_category",
  "title": "그룹화: 카테고리별 매출 합계",
  "body": "sales 테이블에서 category 컬럼을 기준으로 각 그룹의 총 매출액을 계산하세요. 결과는 매출 합계 내림차순으로 정렬해주세요.",
  "schema": "sales(sale_id INT, category TEXT, amount DECIMAL)",
  "sample_rows": [
    "1 | Electronics | 87000",
    "2 | Clothing | 65000"
  ],
  "difficulty": "Lv2 초급",
  "kind": "sql",
  "expected": [
    "SELECT",
    "GROUP BY",
    "SUM",
    "ORDER BY"
  ],
  "hint": "group by category 이후 sum(amount)로 집계.",
  "problem_type": "코딩"
},{
  "pid": "sql_case_when_ranking",
  "title": "조건부 컬럼 생성: 등급 분류",
  "body": "students 테이블에서 점수(score)에 따라 'A', 'B', 'C'로 등급을 부여하세요. 단, score가 90점 이상 → 'A', 80~89 → 'B', 나머지 → 'C'. 결과는 student_id 순으로 정렬해주세요.",
  "schema": "students(student_id INT, name TEXT, score DECIMAL)",
  "sample_rows": [
    "1 | Alice | 95",
    "2 | Bob | 85"
  ],
  "difficulty": "Lv2 초급",
  "kind": "sql",
  "expected": [
    "CASE WHEN",
    "THEN ELSE END",
    "ORDER BY"
  ],
  "hint": "WHEN score >= 90 THEN 'A'로 조건을 순서대로 나누세요.",
  "problem_type": "코딩"
},{
  "pid": "sql_distinct_values",
  "title": "중복 제거: 고유한 지역 목록 추출",
  "body": "customers 테이블에서 거주지(city)가 중복되지 않게 출력하세요. 결과는 도시명 오름차순으로 정렬해주세요.",
  "schema": "customers(customer_id INT, name TEXT, city TEXT)",
  "sample_rows": [
    "1 | Alice | Seoul",
    "2 | Bob | Busan",
    "3 | Casey | Seoul"
  ],
  "difficulty": "Lv1 입문",
  "kind": "sql",
  "expected": [
    "SELECT DISTINCT",
    "ORDER BY"
  ],
  "hint": "DISTINCT로 중복 제거하고, ORDER BY로 정렬.",
  "problem_type": "코딩"
},{
  "pid": "pyspark_select_all_columns",
  "title": "모든 컬럼 조회: 기본 DataFrame 조회",
  "body": "df에서 모든 컬럼과 행을 출력하세요. 결과는 'user_id' 기준 오름차순으로 정렬해주세요.",
  "schema": "df: user_id INT, name STRING, age INT",
  "sample_rows": [
    "1 | Alice | 25",
    "2 | Bob | 30"
  ],
  "difficulty": "Lv1 입문",
  "kind": "python",
  "expected": [
    "select('*')",
    "orderBy"
  ],
  "hint": "col('*') 대신 select('*')로 모든 컬럼 선택.",
  "problem_type": "코딩"
},{
  "pid": "pyspark_filter_by_value",
  "title": "값 기준 필터링: 나이 조건 적용",
  "body": "users_df에서 age가 25세 이상인 사용자만 추출하세요. 결과는 age 내림차순으로 정렬해주세요.",
  "schema": "users_df: user_id INT, name STRING, age INT",
  "sample_rows": [
    "1 | Alice | 25",
    "2 | Bob | 30"
  ],
  "difficulty": "Lv1 입문",
  "kind": "python",
  "expected": [
    "filter",
    "gt",
    "orderBy"
  ],
  "hint": "col('age') >= lit(25)로 조건 설정.",
  "problem_type": "코딩"
},{
  "pid": "pyspark_filter_nulls",
  "title": "NULL 값 제거: 필수 컬럼 유효성 검사",
  "body": "users_df에서 name이 NULL이 아닌 행만 추출하세요. 결과는 user_id 오름차순으로 정렬해주세요.",
  "schema": "users_df: user_id INT, name STRING, age INT",
  "sample_rows": [
    "1 | Alice | 25",
    "2 | NULL | 30"
  ],
  "difficulty": "Lv1 입문",
  "kind": "python",
  "expected": [
    "filter",
    "isNotNull()",
    "orderBy"
  ],
  "hint": ".filter(col('name').isNotNull())로 처리하세요.",
  "problem_type": "코딩"
},{
  "pid": "pyspark_multiple_conditions",
  "title": "복합 조건 필터링: 두 조건 모두 충족",
  "body": "sales_df에서 amount가 100,000 이상이고, category가 'Electronics'인 행만 추출하세요. 결과는 sale_id 오름차순으로 정렬해주세요.",
  "schema": "sales_df: sale_id INT, category STRING, amount DECIMAL",
  "sample_rows": [
    "1 | Electronics | 95000",
    "2 | Electronics | 120000"
  ],
  "difficulty": "Lv2 초급",
  "kind": "python",
  "expected": [
    "filter",
    "and",
    "orderBy"
  ],
  "hint": "col('amount') >= lit(100000) & (col('category') == lit('Electronics'))",
  "problem_type": "코딩"
},{
  "pid": "pyspark_like_pattern",
  "title": "LIKE 패턴 검색: 이름 부분 일치 추출",
  "body": "users_df에서 name 컬럼에 'A'가 포함된 사용자를 추출하세요. 결과는 name 오름차순으로 정렬해주세요.",
  "schema": "users_df: user_id INT, name STRING",
  "sample_rows": [
    "1 | Alice",
    "2 | Bob"
  ],
  "difficulty": "Lv1 입문",
  "kind": "python",
  "expected": [
    "filter",
    "rlike",
    "orderBy"
  ],
  "hint": "col('name').rlike('.*A.*')로 패턴 매칭.",
  "problem_type": "코딩"
},{
  "pid": "pyspark_isin_filter",
  "title": "IN 조건 사용: 다수 값 필터링",
  "body": "products_df에서 name이 'Laptop', 'Mouse', 또는 'Keyboard'인 제품만 추출하세요. 결과는 product_id 오름차순으로 정렬해주세요.",
  "schema": "products_df: product_id INT, name STRING",
  "sample_rows": [
    "101 | Laptop",
    "102 | Mouse"
  ],
  "difficulty": "Lv2 초급",
  "kind": "python",
  "expected": [
    "filter",
    "isin",
    "orderBy"
  ],
  "hint": "col('name').isin(['Laptop', 'Mouse'])로 조건 설정.",
  "problem_type": "코딩"
},{
  "pid": "pyspark_count_rows",
  "title": "행 수 세기: 총 행 개수 계산",
  "body": "users_df의 전체 행 수를 출력하세요. 결과는 하나의 숫자 값으로 반환되며, 컬럼명은 'total_users'로 지정해주세요.",
  "schema": "users_df: user_id INT, name STRING",
  "sample_rows": [
    "1 | Alice",
    "2 | Bob"
  ],
  "difficulty": "Lv1 입문",
  "kind": "python",
  "expected": [
    "count",
    "alias"
  ],
  "hint": "df.count()로 행 수 확인 후, withColumn 또는 select 사용.",
  "problem_type": "코딩"
},{
  "pid": "pyspark_sum_column",
  "title": "합계 계산: 매출 총액 산출",
  "body": "sales_df에서 amount 컬럼의 전체 합계를 출력하세요. 결과는 하나의 숫자 값으로 반환되며, 컬럼명은 'total_amount'로 지정해주세요.",
  "schema": "sales_df: sale_id INT, amount DECIMAL",
  "sample_rows": [
    "1 | 87000",
    "2 | 95000"
  ],
  "difficulty": "Lv1 입문",
  "kind": "python",
  "expected": [
    "sum",
    "alias"
  ],
  "hint": "df.agg(sum('amount').alias('total_amount')) 사용.",
  "problem_type": "코딩"
},{
  "pid": "pyspark_avg_column",
  "title": "평균 계산: 나이 평균 산출",
  "body": "users_df에서 age 컬럼의 평균 값을 출력하세요. 결과는 소수점 둘째자리까지 표시하고, 컬럼명은 'avg_age'로 지정해주세요.",
  "schema": "users_df: user_id INT, name STRING, age INT",
  "sample_rows": [
    "1 | Alice | 25",
    "2 | Bob | 30"
  ],
  "difficulty": "Lv2 초급",
  "kind": "python",
  "expected": [
    "avg",
    "alias",
    "round"
  ],
  "hint": "round(avg('age'), 2)로 소수점 둘째자리까지 반올림.",
  "problem_type": "코딩"
},{
  "pid": "pyspark_groupby_sum",
  "title": "그룹화 후 합계: 카테고리별 매출 집계",
  "body": "sales_df에서 category 컬럼을 기준으로 각 그룹의 amount 총합을 계산하세요. 결과는 매출 합계 내림차순으로 정렬해주세요.",
  "schema": "sales_df: category STRING, amount DECIMAL",
  "sample_rows": [
    "Electronics | 87000",
    "Clothing | 65000"
  ],
  "difficulty": "Lv2 초급",
  "kind": "python",
  "expected": [
    "groupBy",
    "sum",
    "orderBy"
  ],
  "hint": ".groupBy('category').sum('amount')로 집계.",
  "problem_type": "코딩"
},{
  "pid": "pyspark_withcolumn_case",
  "title": "조건부 컬럼 생성: 등급 분류",
  "body": "students_df에서 score 컬럼을 기준으로 'A', 'B', 'C'로 등급을 부여하세요. 점수가 90 이상 → 'A', 80~89 → 'B', 그 외 → 'C'. 결과는 student_id 오름차순 정렬해주세요.",
  "schema": "students_df: student_id INT, name STRING, score DECIMAL",
  "sample_rows": [
    "1 | Alice | 95",
    "2 | Bob | 85"
  ],
  "difficulty": "Lv2 초급",
  "kind": "python",
  "expected": [
    "when",
    ".otherwise",
    "withColumn",
    "orderBy"
  ],
  "hint": "when(col('score') >= 90, 'A').when(...).otherwise('C') 사용.",
  "problem_type": "코딩"
},
{
  "pid": "pyspark_distinct_values",
  "title": "고유 값 추출: 중복 제거된 도시 목록",
  "body": "customers_df에서 city 컬럼의 고유한 값을 추출하세요. 결과는 도시명 오름차순으로 정렬해주세요.",
  "schema": "customers_df: customer_id INT, name STRING, city TEXT",
  "sample_rows": [
    "1 | Alice | Seoul",
    "2 | Bob | Busan",
    "3 | Casey | Seoul"
  ],
  "difficulty": "Lv1 입문",
  "kind": "python",
  "expected": [
    "distinct",
    "orderBy"
  ],
  "hint": ".distinct()로 중복 제거 후, orderBy('city') 정렬.",
  "problem_type": "코딩"
}
]
